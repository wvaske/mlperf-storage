{
  "args": {
    "program": "checkpointing",
    "command": "datasize",
    "results_dir": "/root/mlpstorage_test_results",
    "loops": 1,
    "config_file": null,
    "closed": false,
    "debug": false,
    "verbose": false,
    "stream_log_level": "INFO",
    "allow_invalid_params": false,
    "what_if": false,
    "hosts": [
      "127.0.0.1",
      "127.0.0.1"
    ],
    "client_host_memory_in_gb": 256,
    "model": "llama3-405b",
    "num_checkpoints_read": 10,
    "num_checkpoints_write": 10,
    "num_processes": 8,
    "params": null,
    "checkpoint_folder": "/mnt/nvme/test_data",
    "dlio_bin_path": null,
    "num_client_hosts": 2
  },
  "debug": null,
  "run_datetime": "20250602_183300",
  "run_number": 0,
  "runtime": 0.0001347064971923828,
  "verification": "closed",
  "command_output_files": [],
  "run_result_output": "/root/mlpstorage_test_results/checkpointing/llama3-405b/20250602_183300",
  "metadata_filename": "checkpointing_20250602_183300_metadata.json",
  "metadata_file_path": "/root/mlpstorage_test_results/checkpointing/llama3-405b/20250602_183300/checkpointing_20250602_183300_metadata.json",
  "_config_name": "llama3_405b",
  "base_command": "dlio_benchmark",
  "base_path": "/usr/local/bin",
  "base_command_path": "/usr/local/bin/dlio_benchmark",
  "config_path": "/opt/mlcommons/storage/configs/dlio",
  "per_host_mem_kB": null,
  "total_mem_kB": null,
  "cluster_information": "<mlpstorage.rules.ClusterInformation object at 0x7095d6f7bb90>",
  "config_file": "llama3_405b.yaml",
  "params_dict": {
    "checkpoint.mode": "subset",
    "checkpoint.num_checkpoints_read": 10,
    "checkpoint.num_checkpoints_write": 10,
    "checkpoint.checkpoint_folder": "/mnt/nvme/test_data/llama3-405b"
  },
  "yaml_params": {
    "model": {
      "name": "llama_405b",
      "type": "transformer",
      "num_layers": 126,
      "model_datatype": "fp16",
      "optimizer_datatype": "fp32",
      "parallelism": {
        "tensor": 8,
        "pipeline": 32,
        "zero_stage": 1
      },
      "transformer": {
        "vocab_size": 128256,
        "hidden_size": 16384,
        "ffn_hidden_size": 53248,
        "num_attention_heads": 128,
        "num_kv_heads": 8
      }
    },
    "framework": "pytorch",
    "workflow": {
      "generate_data": false,
      "train": false,
      "checkpoint": true
    },
    "checkpoint": {
      "checkpoint_folder": "checkpoints/llama_405b",
      "time_between_checkpoints": 5,
      "num_checkpoints_write": 10,
      "num_checkpoints_read": 10,
      "fsync": true
    }
  },
  "combined_params": {
    "model": {
      "name": "llama_405b",
      "type": "transformer",
      "num_layers": 126,
      "model_datatype": "fp16",
      "optimizer_datatype": "fp32",
      "parallelism": {
        "tensor": 8,
        "pipeline": 32,
        "zero_stage": 1
      },
      "transformer": {
        "vocab_size": 128256,
        "hidden_size": 16384,
        "ffn_hidden_size": 53248,
        "num_attention_heads": 128,
        "num_kv_heads": 8
      }
    },
    "framework": "pytorch",
    "workflow": {
      "generate_data": false,
      "train": false,
      "checkpoint": true
    },
    "checkpoint": {
      "checkpoint_folder": "checkpoints/llama_405b",
      "time_between_checkpoints": 5,
      "num_checkpoints_write": 10,
      "num_checkpoints_read": 10,
      "fsync": true
    }
  },
  "benchmark_type": "checkpointing"
}