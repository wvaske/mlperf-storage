{
  "args": {
    "program": "checkpointing",
    "command": "datasize",
    "results_dir": "/root/mlpstorage_test_results",
    "loops": 1,
    "config_file": null,
    "closed": false,
    "debug": false,
    "verbose": false,
    "stream_log_level": "INFO",
    "allow_invalid_params": false,
    "what_if": false,
    "hosts": [
      "127.0.0.1",
      "127.0.0.1"
    ],
    "client_host_memory_in_gb": 256,
    "model": "llama3-70b",
    "num_checkpoints_read": 10,
    "num_checkpoints_write": 10,
    "num_processes": 8,
    "params": null,
    "checkpoint_folder": "/mnt/nvme/test_data",
    "dlio_bin_path": null,
    "num_client_hosts": 2
  },
  "debug": null,
  "run_datetime": "20250612_200611",
  "run_number": 0,
  "runtime": 0.0001380443572998047,
  "verification": "closed",
  "command_output_files": [],
  "run_result_output": "/root/mlpstorage_test_results/checkpointing/llama3-70b/20250612_200611",
  "metadata_filename": "checkpointing_20250612_200611_metadata.json",
  "metadata_file_path": "/root/mlpstorage_test_results/checkpointing/llama3-70b/20250612_200611/checkpointing_20250612_200611_metadata.json",
  "_config_name": "llama3_70b",
  "base_command": "dlio_benchmark",
  "base_path": "/usr/local/bin",
  "base_command_path": "/usr/local/bin/dlio_benchmark",
  "config_path": "/opt/mlcommons/storage/configs/dlio",
  "per_host_mem_kB": null,
  "total_mem_kB": null,
  "cluster_information": "<mlpstorage.rules.ClusterInformation object at 0x724aa047b830>",
  "config_file": "llama3_70b.yaml",
  "params_dict": {
    "checkpoint.mode": "subset",
    "model.parallelism.data": 8,
    "checkpoint.num_checkpoints_read": 10,
    "checkpoint.num_checkpoints_write": 10,
    "checkpoint.checkpoint_folder": "/mnt/nvme/test_data/llama3-70b"
  },
  "yaml_params": {
    "model": {
      "name": "llama_70b",
      "type": "transformer",
      "num_layers": 80,
      "model_datatype": "fp16",
      "optimizer_datatype": "fp32",
      "parallelism": {
        "tensor": 8,
        "pipeline": 1,
        "zero_stage": 3
      },
      "transformer": {
        "vocab_size": 128256,
        "hidden_size": 8192,
        "ffn_hidden_size": 28672,
        "num_attention_heads": 128,
        "num_kv_heads": 8
      }
    },
    "framework": "pytorch",
    "workflow": {
      "generate_data": false,
      "train": false,
      "checkpoint": true
    },
    "checkpoint": {
      "checkpoint_folder": "checkpoints/llama_70b",
      "time_between_checkpoints": 5,
      "num_checkpoints_write": 10,
      "num_checkpoints_read": 10,
      "fsync": true
    }
  },
  "combined_params": {
    "model": {
      "name": "llama_70b",
      "type": "transformer",
      "num_layers": 80,
      "model_datatype": "fp16",
      "optimizer_datatype": "fp32",
      "parallelism": {
        "tensor": 8,
        "pipeline": 1,
        "zero_stage": 3
      },
      "transformer": {
        "vocab_size": 128256,
        "hidden_size": 8192,
        "ffn_hidden_size": 28672,
        "num_attention_heads": 128,
        "num_kv_heads": 8
      }
    },
    "framework": "pytorch",
    "workflow": {
      "generate_data": false,
      "train": false,
      "checkpoint": true
    },
    "checkpoint": {
      "checkpoint_folder": "checkpoints/llama_70b",
      "time_between_checkpoints": 5,
      "num_checkpoints_write": 10,
      "num_checkpoints_read": 10,
      "fsync": true
    }
  },
  "benchmark_type": "checkpointing"
}