{
  "args": {
    "program": "checkpointing",
    "command": "run",
    "results_dir": "/root/mlpstorage_test_results",
    "loops": 1,
    "config_file": null,
    "closed": false,
    "debug": false,
    "verbose": false,
    "stream_log_level": "INFO",
    "allow_invalid_params": false,
    "what_if": false,
    "hosts": [
      "127.0.0.1"
    ],
    "client_host_memory_in_gb": 512,
    "model": "llama3-70b",
    "num_checkpoints_read": 1,
    "num_checkpoints_write": 1,
    "num_processes": 8,
    "params": null,
    "checkpoint_folder": "/mnt/nvme/test_data",
    "dlio_bin_path": null,
    "exec_type": "mpi",
    "mpi_bin": "mpirun",
    "oversubscribe": false,
    "allow_run_as_root": true,
    "num_client_hosts": 1
  },
  "debug": null,
  "run_datetime": "20250606_215128",
  "run_number": 0,
  "runtime": 89.03309965133667,
  "verification": "closed",
  "command_output_files": [
    {
      "command": "mpirun -n 8 -host 127.0.0.1:8 --allow-run-as-root /usr/local/bin/dlio_benchmark workload=llama3_70b ++hydra.run.dir=/root/mlpstorage_test_results/checkpointing/llama3-70b/20250606_215128 ++hydra.output_subdir=dlio_config ++workload.model.parallelism.data=1 ++workload.checkpoint.mode=subset ++workload.checkpoint.num_checkpoints_read=1 ++workload.checkpoint.num_checkpoints_write=1 ++workload.checkpoint.checkpoint_folder=/mnt/nvme/test_data/llama3-70b --config-dir=/opt/mlcommons/storage/configs/dlio",
      "stdout": "/root/mlpstorage_test_results/checkpointing/llama3-70b/20250606_215128/checkpointing_run.stdout.log",
      "stderr": "/root/mlpstorage_test_results/checkpointing/llama3-70b/20250606_215128/checkpointing_run.stderr.log"
    }
  ],
  "run_result_output": "/root/mlpstorage_test_results/checkpointing/llama3-70b/20250606_215128",
  "metadata_filename": "checkpointing_20250606_215128_metadata.json",
  "metadata_file_path": "/root/mlpstorage_test_results/checkpointing/llama3-70b/20250606_215128/checkpointing_20250606_215128_metadata.json",
  "_config_name": "llama3_70b",
  "base_command": "dlio_benchmark",
  "base_path": "/usr/local/bin",
  "base_command_path": "/usr/local/bin/dlio_benchmark",
  "config_path": "/opt/mlcommons/storage/configs/dlio",
  "per_host_mem_kB": null,
  "total_mem_kB": null,
  "cluster_information": "<mlpstorage.rules.ClusterInformation object at 0x712962249f40>",
  "config_file": "llama3_70b.yaml",
  "params_dict": {
    "model.parallelism.data": 1,
    "checkpoint.mode": "subset",
    "checkpoint.num_checkpoints_read": 1,
    "checkpoint.num_checkpoints_write": 1,
    "checkpoint.checkpoint_folder": "/mnt/nvme/test_data/llama3-70b"
  },
  "yaml_params": {
    "model": {
      "name": "llama_70b",
      "type": "transformer",
      "num_layers": 80,
      "model_datatype": "fp16",
      "optimizer_datatype": "fp32",
      "parallelism": {
        "tensor": 8,
        "pipeline": 1,
        "zero_stage": 3
      },
      "transformer": {
        "vocab_size": 128256,
        "hidden_size": 8192,
        "ffn_hidden_size": 28672,
        "num_attention_heads": 128,
        "num_kv_heads": 8
      }
    },
    "framework": "pytorch",
    "workflow": {
      "generate_data": false,
      "train": false,
      "checkpoint": true
    },
    "checkpoint": {
      "checkpoint_folder": "checkpoints/llama_70b",
      "time_between_checkpoints": 5,
      "num_checkpoints_write": 10,
      "num_checkpoints_read": 10,
      "fsync": true
    }
  },
  "combined_params": {
    "model": {
      "name": "llama_70b",
      "type": "transformer",
      "num_layers": 80,
      "model_datatype": "fp16",
      "optimizer_datatype": "fp32",
      "parallelism": {
        "tensor": 8,
        "pipeline": 1,
        "zero_stage": 3
      },
      "transformer": {
        "vocab_size": 128256,
        "hidden_size": 8192,
        "ffn_hidden_size": 28672,
        "num_attention_heads": 128,
        "num_kv_heads": 8
      }
    },
    "framework": "pytorch",
    "workflow": {
      "generate_data": false,
      "train": false,
      "checkpoint": true
    },
    "checkpoint": {
      "checkpoint_folder": "checkpoints/llama_70b",
      "time_between_checkpoints": 5,
      "num_checkpoints_write": 10,
      "num_checkpoints_read": 10,
      "fsync": true
    }
  },
  "executed_command": "mpirun -n 8 -host 127.0.0.1:8 --allow-run-as-root /usr/local/bin/dlio_benchmark workload=llama3_70b ++hydra.run.dir=/root/mlpstorage_test_results/checkpointing/llama3-70b/20250606_215128 ++hydra.output_subdir=dlio_config ++workload.model.parallelism.data=1 ++workload.checkpoint.mode=subset ++workload.checkpoint.num_checkpoints_read=1 ++workload.checkpoint.num_checkpoints_write=1 ++workload.checkpoint.checkpoint_folder=/mnt/nvme/test_data/llama3-70b --config-dir=/opt/mlcommons/storage/configs/dlio",
  "benchmark_type": "checkpointing"
}