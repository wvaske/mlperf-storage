---
phase: 08-new-training-models
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - mlpstorage/rules/run_checkers/training.py
  - tests/unit/test_rules_checkers.py
autonomous: true

must_haves:
  truths:
    - "Validation runs for dlrm, retinanet, and flux models"
    - "CLOSED/OPEN status determined correctly for new models"
    - "Unit tests verify validation behavior for new models"
  artifacts:
    - path: "mlpstorage/rules/run_checkers/training.py"
      provides: "Validation rules for new models"
      exports: ["TrainingRunRulesChecker"]
    - path: "tests/unit/test_rules_checkers.py"
      provides: "Unit tests for new model validation"
      min_lines: 10
  key_links:
    - from: "mlpstorage/rules/run_checkers/training.py"
      to: "mlpstorage/config.py"
      via: "Import DLRM, RETINANET, FLUX constants"
      pattern: "from mlpstorage.config import.*DLRM"
    - from: "tests/unit/test_rules_checkers.py"
      to: "mlpstorage/rules/run_checkers/training.py"
      via: "TrainingRunRulesChecker import and test"
---

<objective>
Add validation rules and unit tests for DLRM, RetinaNet, and Flux training models.

Purpose: Ensure new models integrate with the existing validation pipeline for CLOSED/OPEN/INVALID submission categorization, and that validation behavior is tested.

Output: Extended TrainingRunRulesChecker with model-specific validation and corresponding unit tests.
</objective>

<execution_context>
@./.claude/agents/gsd-planner.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-new-training-models/08-RESEARCH.md

# Prior work in this phase
@.planning/phases/08-new-training-models/08-01-SUMMARY.md

# Existing validation patterns
@mlpstorage/rules/run_checkers/training.py
@tests/unit/test_rules_checkers.py
@mlpstorage/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add model-specific validation rules</name>
  <files>mlpstorage/rules/run_checkers/training.py</files>
  <action>
Update TrainingRunRulesChecker to handle new models:

1. Add imports at top of file:
```python
from mlpstorage.config import BENCHMARK_TYPES, PARAM_VALIDATION, UNET, DLRM, RETINANET, FLUX
```

2. Add a new check method for model-specific parameters. The new models don't have special validation requirements like UNET's checkpoint requirement, but we should ensure they're recognized:

```python
def check_model_recognized(self) -> Optional[Issue]:
    """Verify the model is a recognized training model."""
    from mlpstorage.config import MODELS
    if self.benchmark_run.model not in MODELS:
        return Issue(
            validation=PARAM_VALIDATION.INVALID,
            message=f"Unrecognized model: {self.benchmark_run.model}",
            parameter="model",
            expected=f"One of: {', '.join(MODELS)}",
            actual=self.benchmark_run.model
        )
    return None
```

3. Update check_odirect_supported_model to explicitly list supported models (UNET only) rather than using negation, making it clearer:

```python
def check_odirect_supported_model(self) -> Optional[Issue]:
    """Check if reader.odirect is only used with supported models."""
    odirect = self.benchmark_run.parameters.get('reader', {}).get('odirect')
    # odirect is only supported for UNet3D
    odirect_supported_models = [UNET]
    if odirect and self.benchmark_run.model not in odirect_supported_models:
        return Issue(
            validation=PARAM_VALIDATION.INVALID,
            message=f"The reader.odirect option is only supported for {', '.join(odirect_supported_models)}",
            parameter="reader.odirect",
            expected="False",
            actual=odirect
        )
    return None
```

Note: The new models (DLRM, RetinaNet, Flux) work with the existing validation rules. They don't require checkpoint workflow like UNET, so check_workflow_parameters already handles them correctly (returns None).
  </action>
  <verify>
Run: `python -c "from mlpstorage.rules.run_checkers.training import TrainingRunRulesChecker; print('Import OK')"`
Run: `grep -n "DLRM\|RETINANET\|FLUX" mlpstorage/rules/run_checkers/training.py`
  </verify>
  <done>TrainingRunRulesChecker imports new model constants and has check_model_recognized method</done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for new model validation</name>
  <files>tests/unit/test_rules_checkers.py</files>
  <action>
Add test cases for the new models in the TrainingRunRulesChecker test section. Find the existing TrainingRunRulesChecker tests and add:

```python
class TestTrainingRunRulesCheckerNewModels:
    """Tests for TrainingRunRulesChecker with new models (DLRM, RetinaNet, Flux)."""

    @pytest.fixture
    def mock_logger(self):
        """Create a mock logger."""
        return MagicMock()

    def create_benchmark_run(self, model, parameters=None, override_parameters=None):
        """Helper to create a BenchmarkRun for testing."""
        default_params = {
            'dataset': {
                'num_files_train': 1000,
                'data_folder': f'data/{model}/'
            },
            'reader': {
                'batch_size': 16
            },
            'workflow': {}
        }
        if parameters:
            for key, value in parameters.items():
                if key in default_params and isinstance(value, dict):
                    default_params[key].update(value)
                else:
                    default_params[key] = value

        # Create minimal required objects
        host_info = HostInfo(
            hostname="test-host",
            memory=HostMemoryInfo(mem_total_gb=128)
        )
        cluster_info = ClusterInformation(hosts=[host_info])

        return BenchmarkRun(
            run_id=RunID("test_run"),
            model=model,
            benchmark_type=BENCHMARK_TYPES.training,
            command="run_benchmark",
            parameters=default_params,
            override_parameters=override_parameters or {},
            system_info=cluster_info,
            num_processes=8
        )

    @pytest.mark.parametrize("model", ["dlrm", "retinanet", "flux"])
    def test_new_model_recognized(self, mock_logger, model):
        """New models are recognized by the checker."""
        benchmark_run = self.create_benchmark_run(model)
        checker = TrainingRunRulesChecker(benchmark_run, logger=mock_logger)

        issue = checker.check_model_recognized()
        assert issue is None, f"Model {model} should be recognized"

    @pytest.mark.parametrize("model", ["dlrm", "retinanet", "flux"])
    def test_new_model_odirect_not_supported(self, mock_logger, model):
        """odirect is not supported for new models."""
        benchmark_run = self.create_benchmark_run(
            model,
            parameters={'reader': {'odirect': True}}
        )
        checker = TrainingRunRulesChecker(benchmark_run, logger=mock_logger)

        issue = checker.check_odirect_supported_model()
        assert issue is not None
        assert issue.validation == PARAM_VALIDATION.INVALID
        assert "odirect" in issue.message.lower()

    @pytest.mark.parametrize("model", ["dlrm", "retinanet", "flux"])
    def test_new_model_no_checkpoint_requirement(self, mock_logger, model):
        """New models don't require checkpoint workflow."""
        benchmark_run = self.create_benchmark_run(
            model,
            parameters={'workflow': {'checkpoint': False}}
        )
        checker = TrainingRunRulesChecker(benchmark_run, logger=mock_logger)

        # Should not return INVALID for missing checkpoint
        issue = checker.check_workflow_parameters()
        # Either None or not INVALID (UNET requires checkpoint, others don't)
        if issue is not None:
            assert issue.validation != PARAM_VALIDATION.INVALID

    def test_unrecognized_model_invalid(self, mock_logger):
        """Unrecognized model returns INVALID issue."""
        benchmark_run = self.create_benchmark_run("unknown_model")
        checker = TrainingRunRulesChecker(benchmark_run, logger=mock_logger)

        issue = checker.check_model_recognized()
        assert issue is not None
        assert issue.validation == PARAM_VALIDATION.INVALID
        assert "unknown_model" in issue.message
```

Note: This test class uses existing fixtures and patterns from the file. Add necessary imports if not already present:
- from mlpstorage.config import BENCHMARK_TYPES, PARAM_VALIDATION
- Ensure BenchmarkRun, RunID, ClusterInformation, HostInfo, HostMemoryInfo are imported
  </action>
  <verify>
Run: `pytest tests/unit/test_rules_checkers.py -v -k "TestTrainingRunRulesCheckerNewModels" --tb=short`
Expected: All 10 tests pass (3 models x 3 test methods + 1 unrecognized test)
  </verify>
  <done>Unit tests for new model validation pass</done>
</task>

</tasks>

<verification>
1. TrainingRunRulesChecker imports DLRM, RETINANET, FLUX
2. check_model_recognized validates model is in MODELS list
3. New models work with existing validation (no special requirements beyond existing rules)
4. All new unit tests pass
5. Existing tests still pass: `pytest tests/unit/test_rules_checkers.py -v`
</verification>

<success_criteria>
- `grep "DLRM" mlpstorage/rules/run_checkers/training.py` shows import
- `pytest tests/unit/test_rules_checkers.py -v -k "NewModels"` shows 10 passing tests
- `pytest tests/unit/test_rules_checkers.py -v` all tests pass (no regressions)
- Validation correctly categorizes new models as CLOSED/OPEN based on parameters
</success_criteria>

<output>
After completion, create `.planning/phases/08-new-training-models/08-02-SUMMARY.md`
</output>
