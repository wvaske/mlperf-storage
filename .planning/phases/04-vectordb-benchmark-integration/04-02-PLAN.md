---
phase: 04-vectordb-benchmark-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - mlpstorage/benchmarks/vectordbbench.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "VectorDB metadata includes 'model' field for history compatibility"
    - "VectorDB metadata includes benchmark_type, command, run_datetime, result_dir"
    - "VectorDB metadata includes vectordb-specific fields (host, port, config_name)"
  artifacts:
    - path: "mlpstorage/benchmarks/vectordbbench.py"
      provides: "Enhanced metadata property"
      contains: "def metadata"
      min_lines: 80
  key_links:
    - from: "mlpstorage/benchmarks/vectordbbench.py"
      to: "mlpstorage.history"
      via: "metadata['model'] field"
      pattern: "'model'"
---

<objective>
Enhance VectorDBBenchmark metadata property to include all fields required by history module and consistent with other benchmark types.

Purpose: VectorDB benchmark results appear correctly in `mlpstorage history list` output.
Output: Enhanced metadata property with model field and vectordb-specific parameters.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-vectordb-benchmark-integration/04-RESEARCH.md

# Reference implementation for metadata pattern
@mlpstorage/benchmarks/kvcache.py
@mlpstorage/benchmarks/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add metadata property to VectorDBBenchmark</name>
  <files>mlpstorage/benchmarks/vectordbbench.py</files>
  <action>
Add a `metadata` property that extends base class metadata with VectorDB-specific fields.

1. Add typing imports at top if not present:
   ```python
   from typing import Dict, Any
   ```

2. Add the metadata property after the existing methods:

```python
@property
def metadata(self) -> Dict[str, Any]:
    """Generate metadata for the VectorDB benchmark run.

    Returns:
        Dictionary containing benchmark metadata compatible with
        history module and reporting tools.
    """
    base_metadata = super().metadata

    # Use config_name as 'model' equivalent for history compatibility
    # VectorDB doesn't have ML models, but config_name serves same purpose
    base_metadata.update({
        'vectordb_config': self.config_name,
        'model': self.config_name,  # For history module compatibility
        'host': getattr(self.args, 'host', '127.0.0.1'),
        'port': getattr(self.args, 'port', 19530),
        'collection': getattr(self.args, 'collection', None),
    })

    # Add command-specific parameters
    if self.command == 'datagen':
        base_metadata.update({
            'dimension': getattr(self.args, 'dimension', None),
            'num_vectors': getattr(self.args, 'num_vectors', None),
            'num_shards': getattr(self.args, 'num_shards', None),
            'vector_dtype': getattr(self.args, 'vector_dtype', None),
            'distribution': getattr(self.args, 'distribution', None),
        })
    elif self.command == 'run':
        base_metadata.update({
            'num_query_processes': getattr(self.args, 'num_query_processes', None),
            'batch_size': getattr(self.args, 'batch_size', None),
            'runtime': getattr(self.args, 'runtime', None),
            'queries': getattr(self.args, 'queries', None),
        })

    return base_metadata
```

Note: The base class metadata already includes:
- benchmark_type (from BENCHMARK_TYPE)
- command (from args.command)
- run_datetime
- result_dir (from run_result_output)
- num_processes, accelerator, parameters, system_info, etc.
  </action>
  <verify>
  ```bash
  python -c "
from argparse import Namespace
from unittest.mock import patch, MagicMock
import tempfile
import os

# Create mock args
args = Namespace(
    debug=False, verbose=False, what_if=False, stream_log_level='INFO',
    results_dir=tempfile.gettempdir(),
    command='run', config='default', host='192.168.1.100', port=19530,
    num_query_processes=4, batch_size=10, runtime=60, queries=None,
    report_count=100, collection='test_collection', category=None,
)

# Mock the base class and verify metadata
with patch('mlpstorage.benchmarks.vectordbbench.Benchmark.__init__') as mock_init, \
     patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}):
    mock_init.return_value = None

    from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
    bm = VectorDBBenchmark.__new__(VectorDBBenchmark)
    bm.args = args
    bm.command = 'run'
    bm.config_name = 'default'
    bm.run_datetime = '20260124_120000'
    bm.run_result_output = '/tmp/results'
    bm.BENCHMARK_TYPE = MagicMock()
    bm.BENCHMARK_TYPE.name = 'vector_database'
    bm.runtime = 0
    bm.verification = None
    bm.command_output_files = []

    # Mock super().metadata
    with patch.object(VectorDBBenchmark, 'metadata', new_callable=lambda: property(lambda self: {
        'benchmark_type': 'vector_database',
        'command': 'run',
        'run_datetime': '20260124_120000',
        'result_dir': '/tmp/results',
    })):
        # Get metadata - need to call the actual property
        pass

print('Metadata structure verified')
"
  ```
  </verify>
  <done>VectorDBBenchmark.metadata includes model, host, port, collection, and command-specific fields</done>
</task>

<task type="auto">
  <name>Task 2: Add write_metadata call to execute_run</name>
  <files>mlpstorage/benchmarks/vectordbbench.py</files>
  <action>
Ensure metadata is written after benchmark execution. Update execute_run() method:

After the `self._execute_command(cmd, ...)` line, add:
```python
# Write metadata for history tracking
self.write_metadata()
```

The execute_run method should end with:
```python
self.logger.verbose(f'Execuging benchmark run.')  # Note: typo in original
self._execute_command(cmd, output_file_prefix=f"{self.BENCHMARK_TYPE.value}_{self.args.command}")
self.write_metadata()
```

Note: Consider also adding write_metadata() to execute_datagen() for consistency, though datagen typically doesn't need history tracking.
  </action>
  <verify>
  ```bash
  grep -n "write_metadata" mlpstorage/benchmarks/vectordbbench.py
  ```
  Should show write_metadata() call in execute_run method.
  </verify>
  <done>VectorDBBenchmark writes metadata JSON after benchmark execution</done>
</task>

</tasks>

<verification>
1. Metadata property exists and returns expected fields:
   ```bash
   python -c "
from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
import inspect
assert hasattr(VectorDBBenchmark, 'metadata')
sig = inspect.signature(VectorDBBenchmark.metadata.fget)
print('metadata property exists')
"
   ```

2. Import and basic instantiation works:
   ```bash
   python -c "from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark; print('Import OK')"
   ```

3. All existing tests still pass:
   ```bash
   pytest tests/unit -v -k "not integration" --tb=short
   ```
</verification>

<success_criteria>
- VectorDBBenchmark.metadata property exists and returns dict
- Metadata includes 'model' field (using config_name value)
- Metadata includes host, port, collection fields
- Metadata includes command-specific fields (datagen vs run)
- write_metadata() called after execute_run()
- All existing unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-vectordb-benchmark-integration/04-02-SUMMARY.md`
</output>
