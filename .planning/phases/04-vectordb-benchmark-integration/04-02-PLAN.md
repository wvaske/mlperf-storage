---
phase: 04-vectordb-benchmark-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - mlpstorage/benchmarks/vectordbbench.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "VectorDB metadata includes 'model' field for history compatibility"
    - "VectorDB metadata includes benchmark_type, command, run_datetime, result_dir"
    - "VectorDB metadata includes vectordb-specific fields (host, port, config_name)"
    - "Both run and datagen commands write metadata after execution"
  artifacts:
    - path: "mlpstorage/benchmarks/vectordbbench.py"
      provides: "Enhanced metadata property"
      contains: "def metadata"
      min_lines: 80
  key_links:
    - from: "mlpstorage/benchmarks/vectordbbench.py"
      to: "mlpstorage.history"
      via: "metadata['model'] field"
      pattern: "'model'"
    - from: "execute_run"
      to: "write_metadata"
      via: "method call after execution"
      pattern: "self.write_metadata()"
    - from: "execute_datagen"
      to: "write_metadata"
      via: "method call after execution"
      pattern: "self.write_metadata()"
---

<objective>
Enhance VectorDBBenchmark metadata property to include all fields required by history module and consistent with other benchmark types.

Purpose: VectorDB benchmark results appear correctly in `mlpstorage history list` output.
Output: Enhanced metadata property with model field and vectordb-specific parameters.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-vectordb-benchmark-integration/04-RESEARCH.md

# Reference implementation for metadata pattern
@mlpstorage/benchmarks/kvcache.py
@mlpstorage/benchmarks/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add metadata property to VectorDBBenchmark</name>
  <files>mlpstorage/benchmarks/vectordbbench.py</files>
  <action>
Add a `metadata` property that extends base class metadata with VectorDB-specific fields.

1. Add typing imports at top if not present:
   ```python
   from typing import Dict, Any
   ```

2. Add the metadata property after the existing methods:

```python
@property
def metadata(self) -> Dict[str, Any]:
    """Generate metadata for the VectorDB benchmark run.

    Returns:
        Dictionary containing benchmark metadata compatible with
        history module and reporting tools.
    """
    base_metadata = super().metadata

    # Use config_name as 'model' equivalent for history compatibility
    # VectorDB doesn't have ML models, but config_name serves same purpose
    base_metadata.update({
        'vectordb_config': self.config_name,
        'model': self.config_name,  # For history module compatibility
        'host': getattr(self.args, 'host', '127.0.0.1'),
        'port': getattr(self.args, 'port', 19530),
        'collection': getattr(self.args, 'collection', None),
    })

    # Add command-specific parameters
    if self.command == 'datagen':
        base_metadata.update({
            'dimension': getattr(self.args, 'dimension', None),
            'num_vectors': getattr(self.args, 'num_vectors', None),
            'num_shards': getattr(self.args, 'num_shards', None),
            'vector_dtype': getattr(self.args, 'vector_dtype', None),
            'distribution': getattr(self.args, 'distribution', None),
        })
    elif self.command == 'run':
        base_metadata.update({
            'num_query_processes': getattr(self.args, 'num_query_processes', None),
            'batch_size': getattr(self.args, 'batch_size', None),
            'runtime': getattr(self.args, 'runtime', None),
            'queries': getattr(self.args, 'queries', None),
        })

    return base_metadata
```

Note: The base class metadata already includes:
- benchmark_type (from BENCHMARK_TYPE)
- command (from args.command)
- run_datetime
- result_dir (from run_result_output)
- num_processes, accelerator, parameters, system_info, etc.
  </action>
  <verify>
  ```bash
  python -c "
from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
import inspect

# Verify property exists
assert hasattr(VectorDBBenchmark, 'metadata'), 'metadata property not found'
assert isinstance(inspect.getattr_static(VectorDBBenchmark, 'metadata'), property), 'metadata is not a property'

# Check the property source to verify it contains expected fields
source = inspect.getsource(VectorDBBenchmark.metadata.fget)
assert \"'model'\" in source, 'model field not in metadata property'
assert \"'vectordb_config'\" in source, 'vectordb_config field not in metadata property'
assert \"'host'\" in source, 'host field not in metadata property'
assert \"'port'\" in source, 'port field not in metadata property'

print('metadata property verified with required fields')
"
  ```
  </verify>
  <done>VectorDBBenchmark.metadata includes model, host, port, collection, and command-specific fields</done>
</task>

<task type="auto">
  <name>Task 2: Add write_metadata calls to execute_run and execute_datagen</name>
  <files>mlpstorage/benchmarks/vectordbbench.py</files>
  <action>
Ensure metadata is written after benchmark execution for BOTH commands (run AND datagen).

1. Update execute_run() method - after the `self._execute_command(cmd, ...)` line, add:
```python
# Write metadata for history tracking
self.write_metadata()
```

The execute_run method should end with:
```python
self.logger.verbose(f'Execuging benchmark run.')  # Note: typo in original
self._execute_command(cmd, output_file_prefix=f"{self.BENCHMARK_TYPE.value}_{self.args.command}")
self.write_metadata()
```

2. Update execute_datagen() method - after the `self._execute_command(cmd)` line, add:
```python
# Write metadata for history tracking
self.write_metadata()
```

The execute_datagen method should end with:
```python
self.logger.verbose(f'Executing data generation.')
self._execute_command(cmd)
self.write_metadata()
```

Both commands need write_metadata() for history tracking consistency. This follows the pattern from KVCacheBenchmark._execute_run() which calls write_metadata() after execution.
  </action>
  <verify>
  ```bash
  # Verify write_metadata appears in both methods
  python -c "
import inspect
from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark

# Get source of both methods
run_source = inspect.getsource(VectorDBBenchmark.execute_run)
datagen_source = inspect.getsource(VectorDBBenchmark.execute_datagen)

# Check both have write_metadata
assert 'write_metadata' in run_source, 'write_metadata not in execute_run'
assert 'write_metadata' in datagen_source, 'write_metadata not in execute_datagen'

print('write_metadata() found in both execute_run and execute_datagen')
"
  ```
  </verify>
  <done>VectorDBBenchmark writes metadata JSON after both run and datagen execution</done>
</task>

<task type="auto">
  <name>Task 3: Verify datagen command end-to-end functionality</name>
  <files>mlpstorage/benchmarks/vectordbbench.py</files>
  <action>
This task verifies the datagen command is functional after the metadata integration.

1. No code changes required - this is a verification task.

2. Verify the complete flow:
   - CLI parser accepts 'datagen' command (verified in 04-01)
   - VectorDBBenchmark.command_method_map routes 'datagen' to execute_datagen
   - execute_datagen builds command and calls _execute_command
   - write_metadata is called after execution (added in Task 2)
  </action>
  <verify>
  ```bash
  python -c "
from argparse import Namespace
from unittest.mock import patch, MagicMock
import tempfile
import os

# Create args that mimic datagen command
args = Namespace(
    debug=False,
    verbose=False,
    what_if=False,
    stream_log_level='INFO',
    results_dir=tempfile.gettempdir(),
    command='datagen',
    config='default',
    host='127.0.0.1',
    port=19530,
    collection=None,
    category=None,
    dimension=1536,
    num_vectors=1000,
    num_shards=1,
    vector_dtype='FLOAT_VECTOR',
    distribution='uniform',
    batch_size=1000,
    chunk_size=10000,
    force=False,
)

# Mock base class init and config loading
with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
     patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
     patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
    output_dir = tempfile.mkdtemp()
    mock_gen.return_value = output_dir

    from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
    bm = VectorDBBenchmark(args)

    # Verify command routing
    assert 'datagen' in bm.command_method_map, 'datagen not in command_method_map'
    assert bm.command == 'datagen', f'command should be datagen, got {bm.command}'

    # Verify metadata includes datagen-specific fields
    meta = bm.metadata
    assert 'dimension' in meta, 'dimension not in metadata for datagen'
    assert 'num_vectors' in meta, 'num_vectors not in metadata for datagen'
    assert 'model' in meta, 'model field missing (needed for history)'

    print('datagen command verified: routing, metadata fields all present')
"
  ```
  </verify>
  <done>Datagen command routes correctly, builds command, and writes metadata with datagen-specific fields</done>
</task>

</tasks>

<verification>
1. Metadata property exists and returns expected fields:
   ```bash
   python -c "
from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
import inspect
assert hasattr(VectorDBBenchmark, 'metadata')
sig = inspect.signature(VectorDBBenchmark.metadata.fget)
print('metadata property exists')
"
   ```

2. Import and basic instantiation works:
   ```bash
   python -c "from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark; print('Import OK')"
   ```

3. write_metadata in both methods:
   ```bash
   grep -n "write_metadata" mlpstorage/benchmarks/vectordbbench.py
   ```
   Should show write_metadata() call in BOTH execute_run AND execute_datagen.

4. All existing tests still pass:
   ```bash
   pytest tests/unit -v -k "not integration" --tb=short
   ```
</verification>

<success_criteria>
- VectorDBBenchmark.metadata property exists and returns dict
- Metadata includes 'model' field (using config_name value)
- Metadata includes host, port, collection fields
- Metadata includes command-specific fields (datagen vs run)
- write_metadata() called after BOTH execute_run() AND execute_datagen()
- Datagen command routes correctly and produces correct metadata
- All existing unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-vectordb-benchmark-integration/04-02-SUMMARY.md`
</output>
