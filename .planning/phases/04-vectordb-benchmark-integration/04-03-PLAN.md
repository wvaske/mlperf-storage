---
phase: 04-vectordb-benchmark-integration
plan: 03
type: execute
wave: 2
depends_on:
  - 04-01
  - 04-02
files_modified:
  - tests/unit/test_cli_vectordb.py
  - tests/unit/test_benchmarks_vectordb.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "VectorDB CLI arguments have comprehensive unit tests"
    - "VectorDB benchmark class has unit tests for metadata and command handling"
    - "All tests follow established patterns from KVCache tests"
  artifacts:
    - path: "tests/unit/test_cli_vectordb.py"
      provides: "CLI argument parsing tests"
      min_lines: 100
    - path: "tests/unit/test_benchmarks_vectordb.py"
      provides: "Benchmark class tests"
      min_lines: 80
  key_links:
    - from: "tests/unit/test_cli_vectordb.py"
      to: "mlpstorage/cli/vectordb_args.py"
      via: "add_vectordb_arguments import"
      pattern: "from mlpstorage.cli.vectordb_args import"
    - from: "tests/unit/test_benchmarks_vectordb.py"
      to: "mlpstorage/benchmarks/vectordbbench.py"
      via: "VectorDBBenchmark import"
      pattern: "from mlpstorage.benchmarks.vectordbbench import"
---

<objective>
Create comprehensive unit tests for VectorDB CLI arguments and benchmark class following established patterns from KVCache tests.

Purpose: Ensure VectorDB integration is well-tested and maintainable.
Output: Two test files with full coverage of CLI parsing and benchmark behavior.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-vectordb-benchmark-integration/04-RESEARCH.md

# Test patterns to follow
@tests/unit/test_cli_kvcache.py
@tests/unit/test_benchmarks_kvcache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test_cli_vectordb.py</name>
  <files>tests/unit/test_cli_vectordb.py</files>
  <action>
Create comprehensive CLI tests following the KVCache test pattern.

```python
"""
Tests for VectorDB benchmark CLI argument parsing.

Tests cover:
- VectorDB subcommand structure (run, datagen)
- Common arguments (host, port, collection, config)
- Datagen-specific arguments (dimension, num_vectors, distribution, etc.)
- Run-specific arguments (num_query_processes, batch_size, runtime, queries)
"""

import argparse
import pytest

from mlpstorage.cli.vectordb_args import add_vectordb_arguments
from mlpstorage.config import VECTOR_DTYPES, DISTRIBUTIONS


class TestVectorDBSubcommands:
    """Tests for VectorDB subcommand structure."""

    @pytest.fixture
    def parser(self):
        """Create a parser with vectordb subcommands."""
        parser = argparse.ArgumentParser()
        add_vectordb_arguments(parser)
        return parser

    def test_run_subcommand_exists(self, parser):
        """VectorDB should have run subcommand (not run-search)."""
        args = parser.parse_args(['run'])
        assert args.command == 'run'

    def test_datagen_subcommand_exists(self, parser):
        """VectorDB should have datagen subcommand."""
        args = parser.parse_args(['datagen'])
        assert args.command == 'datagen'

    def test_run_search_does_not_exist(self, parser):
        """run-search should NOT be a valid subcommand."""
        with pytest.raises(SystemExit):
            parser.parse_args(['run-search'])


class TestVectorDBCommonArguments:
    """Tests for arguments common to both run and datagen."""

    @pytest.fixture
    def parser(self):
        """Create a parser with vectordb subcommands."""
        parser = argparse.ArgumentParser()
        add_vectordb_arguments(parser)
        return parser

    def test_host_argument_default(self, parser):
        """Host should default to 127.0.0.1."""
        args = parser.parse_args(['run'])
        assert args.host == '127.0.0.1'

    def test_host_argument_custom(self, parser):
        """Should accept custom host."""
        args = parser.parse_args(['run', '--host', '192.168.1.100'])
        assert args.host == '192.168.1.100'

    def test_host_short_flag(self, parser):
        """Should accept -s shorthand for host."""
        args = parser.parse_args(['run', '-s', 'milvus.local'])
        assert args.host == 'milvus.local'

    def test_port_argument_default(self, parser):
        """Port should default to 19530."""
        args = parser.parse_args(['run'])
        assert args.port == 19530

    def test_port_argument_custom(self, parser):
        """Should accept custom port."""
        args = parser.parse_args(['run', '--port', '19531'])
        assert args.port == 19531

    def test_port_short_flag(self, parser):
        """Should accept -p shorthand for port."""
        args = parser.parse_args(['run', '-p', '8080'])
        assert args.port == 8080

    def test_config_argument(self, parser):
        """Should accept --config argument."""
        args = parser.parse_args(['run', '--config', '10m'])
        assert args.config == '10m'

    def test_collection_argument(self, parser):
        """Should accept --collection argument."""
        args = parser.parse_args(['run', '--collection', 'my_collection'])
        assert args.collection == 'my_collection'


class TestVectorDBDatagenArguments:
    """Tests for datagen-specific arguments."""

    @pytest.fixture
    def parser(self):
        """Create a parser with vectordb subcommands."""
        parser = argparse.ArgumentParser()
        add_vectordb_arguments(parser)
        return parser

    def test_dimension_argument_default(self, parser):
        """Dimension should default to 1536."""
        args = parser.parse_args(['datagen'])
        assert args.dimension == 1536

    def test_dimension_argument_custom(self, parser):
        """Should accept custom dimension."""
        args = parser.parse_args(['datagen', '--dimension', '768'])
        assert args.dimension == 768

    def test_num_shards_argument_default(self, parser):
        """num_shards should default to 1."""
        args = parser.parse_args(['datagen'])
        assert args.num_shards == 1

    def test_num_shards_argument_custom(self, parser):
        """Should accept custom num_shards."""
        args = parser.parse_args(['datagen', '--num-shards', '4'])
        assert args.num_shards == 4

    def test_vector_dtype_argument_default(self, parser):
        """vector_dtype should default to FLOAT_VECTOR."""
        args = parser.parse_args(['datagen'])
        assert args.vector_dtype == 'FLOAT_VECTOR'

    def test_vector_dtype_argument_choices(self, parser):
        """Should accept valid vector dtype choices."""
        for dtype in VECTOR_DTYPES:
            args = parser.parse_args(['datagen', '--vector-dtype', dtype])
            assert args.vector_dtype == dtype

    def test_num_vectors_argument_default(self, parser):
        """num_vectors should default to 1_000_000."""
        args = parser.parse_args(['datagen'])
        assert args.num_vectors == 1_000_000

    def test_num_vectors_argument_custom(self, parser):
        """Should accept custom num_vectors."""
        args = parser.parse_args(['datagen', '--num-vectors', '5000000'])
        assert args.num_vectors == 5000000

    def test_distribution_argument_default(self, parser):
        """distribution should default to uniform."""
        args = parser.parse_args(['datagen'])
        assert args.distribution == 'uniform'

    def test_distribution_argument_choices(self, parser):
        """Should accept valid distribution choices."""
        for dist in DISTRIBUTIONS:
            args = parser.parse_args(['datagen', '--distribution', dist])
            assert args.distribution == dist

    def test_batch_size_argument_default(self, parser):
        """batch_size should default to 1000."""
        args = parser.parse_args(['datagen'])
        assert args.batch_size == 1_000

    def test_chunk_size_argument_default(self, parser):
        """chunk_size should default to 10000."""
        args = parser.parse_args(['datagen'])
        assert args.chunk_size == 10_000

    def test_force_argument(self, parser):
        """Should accept --force flag."""
        args = parser.parse_args(['datagen', '--force'])
        assert args.force is True

    def test_force_argument_default(self, parser):
        """force should default to False."""
        args = parser.parse_args(['datagen'])
        assert args.force is False


class TestVectorDBRunArguments:
    """Tests for run-specific arguments."""

    @pytest.fixture
    def parser(self):
        """Create a parser with vectordb subcommands."""
        parser = argparse.ArgumentParser()
        add_vectordb_arguments(parser)
        return parser

    def test_num_query_processes_argument_default(self, parser):
        """num_query_processes should default to 1."""
        args = parser.parse_args(['run'])
        assert args.num_query_processes == 1

    def test_num_query_processes_argument_custom(self, parser):
        """Should accept custom num_query_processes."""
        args = parser.parse_args(['run', '--num-query-processes', '8'])
        assert args.num_query_processes == 8

    def test_batch_size_argument_default(self, parser):
        """batch_size should default to 1."""
        args = parser.parse_args(['run'])
        assert args.batch_size == 1

    def test_batch_size_argument_custom(self, parser):
        """Should accept custom batch_size."""
        args = parser.parse_args(['run', '--batch-size', '100'])
        assert args.batch_size == 100

    def test_report_count_argument_default(self, parser):
        """report_count should default to 100."""
        args = parser.parse_args(['run'])
        assert args.report_count == 100

    def test_report_count_argument_custom(self, parser):
        """Should accept custom report_count."""
        args = parser.parse_args(['run', '--report-count', '500'])
        assert args.report_count == 500

    def test_runtime_argument(self, parser):
        """Should accept --runtime argument."""
        args = parser.parse_args(['run', '--runtime', '120'])
        assert args.runtime == 120

    def test_queries_argument(self, parser):
        """Should accept --queries argument."""
        args = parser.parse_args(['run', '--queries', '10000'])
        assert args.queries == 10000

    def test_runtime_and_queries_mutually_exclusive(self, parser):
        """runtime and queries should be mutually exclusive."""
        with pytest.raises(SystemExit):
            parser.parse_args(['run', '--runtime', '60', '--queries', '1000'])


class TestVectorDBDatagenNoRunArgs:
    """Tests verifying datagen doesn't have run-specific args."""

    @pytest.fixture
    def parser(self):
        """Create a parser with vectordb subcommands."""
        parser = argparse.ArgumentParser()
        add_vectordb_arguments(parser)
        return parser

    def test_datagen_has_dimension(self, parser):
        """datagen should have dimension argument."""
        args = parser.parse_args(['datagen', '--dimension', '512'])
        assert args.dimension == 512

    def test_run_no_dimension(self, parser):
        """run should not have dimension argument."""
        args = parser.parse_args(['run'])
        assert not hasattr(args, 'dimension')

    def test_run_no_num_vectors(self, parser):
        """run should not have num_vectors argument."""
        args = parser.parse_args(['run'])
        assert not hasattr(args, 'num_vectors')

    def test_run_no_force(self, parser):
        """run should not have force argument."""
        args = parser.parse_args(['run'])
        assert not hasattr(args, 'force')
```
  </action>
  <verify>
  ```bash
  pytest tests/unit/test_cli_vectordb.py -v --tb=short
  ```
  All tests should pass.
  </verify>
  <done>test_cli_vectordb.py created with comprehensive CLI argument tests</done>
</task>

<task type="auto">
  <name>Task 2: Create test_benchmarks_vectordb.py</name>
  <files>tests/unit/test_benchmarks_vectordb.py</files>
  <action>
Create benchmark class tests following the KVCache test pattern.

```python
"""
Tests for VectorDBBenchmark class in mlpstorage.benchmarks.vectordbbench module.

Tests cover:
- Command method map structure
- Metadata generation for history integration
- Command-specific metadata fields
"""

import os
import pytest
from unittest.mock import MagicMock, patch
from argparse import Namespace


class TestVectorDBCommandMap:
    """Tests for VectorDBBenchmark command routing."""

    @pytest.fixture
    def basic_args(self, tmp_path):
        """Create basic args for VectorDB benchmark."""
        return Namespace(
            debug=False,
            verbose=False,
            what_if=False,
            stream_log_level='INFO',
            results_dir=str(tmp_path),
            command='run',
            config='default',
            host='127.0.0.1',
            port=19530,
            collection=None,
            category=None,
            num_query_processes=1,
            batch_size=1,
            runtime=60,
            queries=None,
            report_count=100,
        )

    def test_run_command_in_map(self, basic_args, tmp_path):
        """Command map should contain 'run' key."""
        with patch('mlpstorage.benchmarks.vectordbbench.Benchmark.__init__', return_value=None), \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark.__new__(VectorDBBenchmark)
            bm.args = basic_args
            bm.command = 'run'
            bm.logger = MagicMock()

            # Manually set what __init__ would set
            bm.command_method_map = {
                "datagen": bm.execute_datagen,
                "run": bm.execute_run,
            }

            assert 'run' in bm.command_method_map
            assert 'run-search' not in bm.command_method_map

    def test_datagen_command_in_map(self, basic_args, tmp_path):
        """Command map should contain 'datagen' key."""
        with patch('mlpstorage.benchmarks.vectordbbench.Benchmark.__init__', return_value=None), \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark.__new__(VectorDBBenchmark)
            bm.args = basic_args
            bm.command = 'datagen'
            bm.logger = MagicMock()

            bm.command_method_map = {
                "datagen": bm.execute_datagen,
                "run": bm.execute_run,
            }

            assert 'datagen' in bm.command_method_map


class TestVectorDBMetadata:
    """Test metadata structure for history integration."""

    @pytest.fixture
    def run_args(self, tmp_path):
        """Create args for VectorDB run command."""
        return Namespace(
            debug=False,
            verbose=False,
            what_if=False,
            stream_log_level='INFO',
            results_dir=str(tmp_path),
            command='run',
            config='10m',
            host='192.168.1.100',
            port=19531,
            collection='test_collection',
            category=None,
            num_query_processes=4,
            batch_size=10,
            runtime=120,
            queries=None,
            report_count=100,
        )

    @pytest.fixture
    def datagen_args(self, tmp_path):
        """Create args for VectorDB datagen command."""
        return Namespace(
            debug=False,
            verbose=False,
            what_if=False,
            stream_log_level='INFO',
            results_dir=str(tmp_path),
            command='datagen',
            config='default',
            host='127.0.0.1',
            port=19530,
            collection='gen_collection',
            category=None,
            dimension=768,
            num_vectors=5000000,
            num_shards=2,
            vector_dtype='FLOAT_VECTOR',
            distribution='gaussian',
            batch_size=1000,
            chunk_size=10000,
            force=True,
        )

    @pytest.fixture
    def mock_benchmark(self, run_args, tmp_path):
        """Create a mocked VectorDBBenchmark instance."""
        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            benchmark = VectorDBBenchmark(run_args)
            return benchmark

    def test_metadata_has_required_fields(self, run_args, tmp_path):
        """Verify metadata includes fields required by history module."""
        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark(run_args)
            meta = bm.metadata

        # Required by history module
        assert 'benchmark_type' in meta
        assert 'model' in meta  # Uses config_name
        assert 'command' in meta
        assert 'run_datetime' in meta
        assert 'result_dir' in meta

    def test_metadata_includes_vectordb_specific_fields(self, run_args, tmp_path):
        """Verify VectorDB specific metadata fields."""
        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark(run_args)
            meta = bm.metadata

        assert 'vectordb_config' in meta
        assert 'host' in meta
        assert 'port' in meta
        assert 'collection' in meta

    def test_metadata_model_uses_config_name(self, run_args, tmp_path):
        """Verify 'model' field uses config_name for history compatibility."""
        run_args.config = '10m'

        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark(run_args)
            meta = bm.metadata

        assert meta['model'] == '10m'
        assert meta['vectordb_config'] == '10m'

    def test_metadata_run_command_fields(self, run_args, tmp_path):
        """Verify run-specific metadata fields."""
        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark(run_args)
            meta = bm.metadata

        assert 'num_query_processes' in meta
        assert meta['num_query_processes'] == 4
        assert 'batch_size' in meta
        assert meta['batch_size'] == 10
        assert 'runtime' in meta
        assert meta['runtime'] == 120

    def test_metadata_datagen_command_fields(self, datagen_args, tmp_path):
        """Verify datagen-specific metadata fields."""
        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark(datagen_args)
            meta = bm.metadata

        assert 'dimension' in meta
        assert meta['dimension'] == 768
        assert 'num_vectors' in meta
        assert meta['num_vectors'] == 5000000
        assert 'num_shards' in meta
        assert meta['num_shards'] == 2
        assert 'vector_dtype' in meta
        assert meta['vector_dtype'] == 'FLOAT_VECTOR'
        assert 'distribution' in meta
        assert meta['distribution'] == 'gaussian'

    def test_metadata_connection_info(self, run_args, tmp_path):
        """Verify host/port connection info in metadata."""
        run_args.host = '10.0.0.50'
        run_args.port = 9999

        with patch('mlpstorage.benchmarks.base.generate_output_location') as mock_gen, \
             patch('mlpstorage.benchmarks.vectordbbench.read_config_from_file', return_value={}), \
             patch('mlpstorage.benchmarks.vectordbbench.VectorDBBenchmark.verify_benchmark'):
            output_dir = str(tmp_path / "output")
            mock_gen.return_value = output_dir
            os.makedirs(output_dir, exist_ok=True)

            from mlpstorage.benchmarks.vectordbbench import VectorDBBenchmark
            bm = VectorDBBenchmark(run_args)
            meta = bm.metadata

        assert meta['host'] == '10.0.0.50'
        assert meta['port'] == 9999
```
  </action>
  <verify>
  ```bash
  pytest tests/unit/test_benchmarks_vectordb.py -v --tb=short
  ```
  All tests should pass.
  </verify>
  <done>test_benchmarks_vectordb.py created with metadata and command handling tests</done>
</task>

</tasks>

<verification>
1. All new tests pass:
   ```bash
   pytest tests/unit/test_cli_vectordb.py tests/unit/test_benchmarks_vectordb.py -v
   ```

2. Test count is substantial:
   ```bash
   pytest tests/unit/test_cli_vectordb.py tests/unit/test_benchmarks_vectordb.py --collect-only | tail -5
   ```
   Should show 30+ tests collected.

3. All existing tests still pass:
   ```bash
   pytest tests/unit -v --tb=short
   ```

4. No import errors:
   ```bash
   python -c "import tests.unit.test_cli_vectordb; import tests.unit.test_benchmarks_vectordb; print('All imports OK')"
   ```
</verification>

<success_criteria>
- test_cli_vectordb.py exists with 25+ tests covering all CLI arguments
- test_benchmarks_vectordb.py exists with 8+ tests covering metadata and command handling
- All new tests pass
- Tests follow patterns established in KVCache test files
- All existing unit tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-vectordb-benchmark-integration/04-03-SUMMARY.md`
</output>
