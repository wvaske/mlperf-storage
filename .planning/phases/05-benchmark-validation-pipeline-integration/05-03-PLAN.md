---
phase: 05-benchmark-validation-pipeline-integration
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - tests/unit/test_rules_vectordb.py
autonomous: true

must_haves:
  truths:
    - "VectorDBRunRulesChecker tests verify all check methods"
    - "Tests confirm preview status returns OPEN validation"
    - "Tests confirm invalid benchmark_type returns INVALID"
    - "Tests confirm insufficient runtime returns INVALID"
  artifacts:
    - path: "tests/unit/test_rules_vectordb.py"
      provides: "Unit tests for VectorDBRunRulesChecker"
      min_lines: 80
  key_links:
    - from: "tests/unit/test_rules_vectordb.py"
      to: "mlpstorage/rules/run_checkers/vectordb.py"
      via: "import and test instantiation"
      pattern: "VectorDBRunRulesChecker"
---

<objective>
Create unit tests for VectorDBRunRulesChecker to verify validation behavior.

Purpose: Ensure VectorDB validation works correctly - invalid types rejected, runtime validated, and preview status always returns OPEN.

Output: Comprehensive test file following existing test patterns in test_rules_checkers.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Test pattern reference
@tests/unit/test_rules_checkers.py

# Implementation being tested
@mlpstorage/rules/run_checkers/vectordb.py
@mlpstorage/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create VectorDB rules checker tests</name>
  <files>tests/unit/test_rules_vectordb.py</files>
  <action>
Create `tests/unit/test_rules_vectordb.py` following the test_rules_checkers.py pattern:

1. File header with docstring explaining test coverage

2. Required imports:
   ```python
   import pytest
   from unittest.mock import MagicMock

   from mlpstorage.config import PARAM_VALIDATION, BENCHMARK_TYPES
   from mlpstorage.rules import (
       Issue,
       BenchmarkRun,
       BenchmarkRunData,
       VectorDBRunRulesChecker,
   )
   ```

3. Create test class `TestVectorDBRunRulesChecker` with:

   Fixtures:
   - `mock_logger`: Returns MagicMock()
   - `valid_vectordb_run(mock_logger)`: Creates BenchmarkRunData with:
     - benchmark_type=BENCHMARK_TYPES.vector_database
     - model='test-config'
     - command='run'
     - run_datetime='20260124_120000'
     - num_processes=1
     - parameters={'runtime': 60, 'host': 'localhost', 'port': 19530}
     - override_parameters={}
     Returns BenchmarkRun.from_data(data, mock_logger)

   Test methods:
   - `test_check_benchmark_type_valid`: Use valid_vectordb_run, assert returns None
   - `test_check_benchmark_type_invalid`: Create run with BENCHMARK_TYPES.training, assert returns INVALID Issue
   - `test_check_runtime_valid`: Use valid_vectordb_run (runtime=60), assert returns None
   - `test_check_runtime_insufficient`: Create run with runtime=10, assert returns INVALID Issue
   - `test_check_runtime_missing_uses_default`: Create run with no runtime in parameters, assert returns None (default 60 >= 30)
   - `test_check_preview_status_always_open`: Use valid_vectordb_run, assert returns OPEN Issue with "preview status" in message
   - `test_run_checks_collects_all_issues`: Use valid_vectordb_run, call checker.run_checks(), verify at least 1 issue (preview status)
   - `test_all_valid_run_returns_open_due_to_preview`: Use valid_vectordb_run, run all checks, verify no INVALID issues and at least one OPEN issue

Follow the exact assertion patterns from test_rules_checkers.py.
  </action>
  <verify>
Run: `pytest tests/unit/test_rules_vectordb.py -v`
All tests should pass.
  </verify>
  <done>All VectorDB rules checker tests pass</done>
</task>

<task type="auto">
  <name>Task 2: Verify full test suite passes</name>
  <files></files>
  <action>
Run the full rules test suite to ensure no regressions:

1. Run all rules-related tests:
   ```bash
   pytest tests/unit/test_rules*.py -v
   ```

2. Run the new VectorDB tests in isolation:
   ```bash
   pytest tests/unit/test_rules_vectordb.py -v --tb=short
   ```

3. If any failures, diagnose and fix the root cause in the implementation files.
  </action>
  <verify>
Run: `pytest tests/unit/test_rules*.py -v`
All tests should pass with exit code 0.
  </verify>
  <done>All rules tests pass including the new VectorDB tests</done>
</task>

</tasks>

<verification>
1. VectorDB tests exist and pass:
   ```bash
   pytest tests/unit/test_rules_vectordb.py -v
   ```

2. All rules tests pass (no regressions):
   ```bash
   pytest tests/unit/test_rules*.py -v
   ```

3. Test coverage includes key scenarios:
   ```bash
   pytest tests/unit/test_rules_vectordb.py -v --collect-only | grep "test_"
   ```
   Should show at least 8 test methods.
</verification>

<success_criteria>
- tests/unit/test_rules_vectordb.py exists with 8+ test methods
- All VectorDB tests pass
- All existing rules tests still pass
- Tests cover: valid type, invalid type, valid runtime, invalid runtime, preview status, run_checks integration
</success_criteria>

<output>
After completion, create `.planning/phases/05-benchmark-validation-pipeline-integration/05-03-SUMMARY.md`
</output>
