---
phase: 05-benchmark-validation-pipeline-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - mlpstorage/rules/verifier.py
  - mlpstorage/reporting/formatters.py
autonomous: true

must_haves:
  truths:
    - "BenchmarkVerifier routes kv_cache to KVCacheRunRulesChecker"
    - "BenchmarkVerifier routes vector_database to VectorDBRunRulesChecker"
    - "Multi-run verification uses base MultiRunRulesChecker for preview benchmarks"
    - "ClosedRequirementsFormatter includes VectorDB requirements"
  artifacts:
    - path: "mlpstorage/rules/verifier.py"
      provides: "BenchmarkVerifier with all 4 benchmark type routing"
      contains: "BENCHMARK_TYPES.kv_cache"
    - path: "mlpstorage/reporting/formatters.py"
      provides: "VECTORDB_REQUIREMENTS constant"
      contains: "VECTORDB_REQUIREMENTS"
  key_links:
    - from: "mlpstorage/rules/verifier.py"
      to: "mlpstorage/rules/run_checkers/kvcache.py"
      via: "import and instantiation"
      pattern: "KVCacheRunRulesChecker\\(benchmark_run"
    - from: "mlpstorage/rules/verifier.py"
      to: "mlpstorage/rules/run_checkers/vectordb.py"
      via: "import and instantiation"
      pattern: "VectorDBRunRulesChecker\\(benchmark_run"
---

<objective>
Update BenchmarkVerifier to route KV Cache and VectorDB benchmark types and add VECTORDB_REQUIREMENTS to formatters.

Purpose: Enable validation of all four benchmark types (training, checkpointing, kv_cache, vector_database) through the unified BenchmarkVerifier interface. Currently the verifier only handles training and checkpointing.

Output: BenchmarkVerifier routes all benchmark types; ClosedRequirementsFormatter includes VectorDB.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-benchmark-validation-pipeline-integration/05-RESEARCH.md

@mlpstorage/rules/verifier.py
@mlpstorage/reporting/formatters.py
@mlpstorage/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update BenchmarkVerifier to route all benchmark types</name>
  <files>mlpstorage/rules/verifier.py</files>
  <action>
Update BenchmarkVerifier to route kv_cache and vector_database types:

1. Update imports at top of file:
   - Add `KVCacheRunRulesChecker` to the import from run_checkers
   - Add `VectorDBRunRulesChecker` to the import from run_checkers
   - Add `MultiRunRulesChecker` import from submission_checkers (for preview benchmark multi-run)

2. Update `_create_rules_checker()` method:

   In the `if self.mode == "single"` block, add after checkpointing:
   ```python
   elif benchmark_run.benchmark_type == BENCHMARK_TYPES.kv_cache:
       self.rules_checker = KVCacheRunRulesChecker(benchmark_run, logger=self.logger)
   elif benchmark_run.benchmark_type == BENCHMARK_TYPES.vector_database:
       self.rules_checker = VectorDBRunRulesChecker(benchmark_run, logger=self.logger)
   ```

   In the `elif self.mode == "multi"` block, add after checkpointing:
   ```python
   elif benchmark_type == BENCHMARK_TYPES.kv_cache:
       # KV Cache preview - use base multi-run checker
       self.rules_checker = MultiRunRulesChecker(self.benchmark_runs, logger=self.logger)
   elif benchmark_type == BENCHMARK_TYPES.vector_database:
       # VectorDB preview - use base multi-run checker
       self.rules_checker = MultiRunRulesChecker(self.benchmark_runs, logger=self.logger)
   ```

Note: Import MultiRunRulesChecker from mlpstorage.rules.submission_checkers.base
  </action>
  <verify>
Run:
```bash
python -c "
from mlpstorage.rules.verifier import BenchmarkVerifier
from mlpstorage.config import BENCHMARK_TYPES
# Verify imports work
print('BenchmarkVerifier imports OK')
"
```
  </verify>
  <done>BenchmarkVerifier._create_rules_checker handles all four BENCHMARK_TYPES</done>
</task>

<task type="auto">
  <name>Task 2: Add VECTORDB_REQUIREMENTS to ClosedRequirementsFormatter</name>
  <files>mlpstorage/reporting/formatters.py</files>
  <action>
Add VectorDB requirements to ClosedRequirementsFormatter:

1. Add VECTORDB_REQUIREMENTS class constant after KVCACHE_REQUIREMENTS:
   ```python
   VECTORDB_REQUIREMENTS = {
       'title': 'VectorDB Benchmark Requirements (Preview)',
       'requirements': [
           'Minimum runtime of 30 seconds',
           'Valid collection configuration',
           'Database host and port accessible',
           'Note: VectorDB is in preview and not yet accepted for CLOSED submissions',
       ],
       'allowed_params': [],
   }
   ```

2. Update get_requirements() method to include vector_database:
   Add to requirements_map:
   `'vector_database': cls.VECTORDB_REQUIREMENTS`

Follow the existing pattern for KVCACHE_REQUIREMENTS.
  </action>
  <verify>
Run:
```bash
python -c "
from mlpstorage.reporting.formatters import ClosedRequirementsFormatter
reqs = ClosedRequirementsFormatter.get_requirements('vector_database')
assert reqs is not None
assert 'VectorDB' in reqs['title']
print('VECTORDB_REQUIREMENTS OK')
"
```
  </verify>
  <done>ClosedRequirementsFormatter.get_requirements('vector_database') returns VectorDB requirements</done>
</task>

</tasks>

<verification>
1. Verify BenchmarkVerifier routing works:
   ```bash
   python -c "
from mlpstorage.rules import BenchmarkVerifier, BenchmarkRun, BenchmarkRunData
from mlpstorage.config import BENCHMARK_TYPES
from unittest.mock import MagicMock

logger = MagicMock()

# Test KV Cache routing
kv_data = BenchmarkRunData(
    benchmark_type=BENCHMARK_TYPES.kv_cache,
    model='llama3.1-8b',
    command='run',
    run_datetime='20260124',
    num_processes=1,
    parameters={'duration': 60},
    override_parameters={}
)
kv_run = BenchmarkRun.from_data(kv_data, logger)
verifier = BenchmarkVerifier(kv_run, logger=logger)
print('KV Cache single-run routing OK')

# Test VectorDB routing
vdb_data = BenchmarkRunData(
    benchmark_type=BENCHMARK_TYPES.vector_database,
    model='test-config',
    command='run',
    run_datetime='20260124',
    num_processes=1,
    parameters={'runtime': 60},
    override_parameters={}
)
vdb_run = BenchmarkRun.from_data(vdb_data, logger)
verifier2 = BenchmarkVerifier(vdb_run, logger=logger)
print('VectorDB single-run routing OK')
"
   ```

2. Verify formatters work:
   ```bash
   python -c "
from mlpstorage.reporting.formatters import ClosedRequirementsFormatter

# All four types should return requirements
for btype in ['training', 'checkpointing', 'kv_cache', 'vector_database']:
    reqs = ClosedRequirementsFormatter.get_requirements(btype)
    assert reqs is not None, f'Missing requirements for {btype}'
    print(f'{btype}: {reqs[\"title\"][:40]}...')
print('All formatters OK')
"
   ```

3. Run existing tests:
   ```bash
   pytest tests/unit/test_rules_checkers.py -v
   ```
</verification>

<success_criteria>
- BenchmarkVerifier routes BENCHMARK_TYPES.kv_cache to KVCacheRunRulesChecker
- BenchmarkVerifier routes BENCHMARK_TYPES.vector_database to VectorDBRunRulesChecker
- Multi-run mode uses MultiRunRulesChecker for preview benchmarks
- ClosedRequirementsFormatter.get_requirements('vector_database') returns requirements dict
- Existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-benchmark-validation-pipeline-integration/05-02-SUMMARY.md`
</output>
