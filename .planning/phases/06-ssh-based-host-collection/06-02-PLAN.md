---
phase: 06-ssh-based-host-collection
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - mlpstorage/cluster_collector.py
  - tests/unit/test_cluster_collector.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "SSHClusterCollector implements ClusterCollectorInterface"
    - "SSHClusterCollector collects from remote hosts via SSH"
    - "SSHClusterCollector uses direct local collection for localhost"
    - "SSHClusterCollector collects hosts in parallel using ThreadPoolExecutor"
  artifacts:
    - path: "mlpstorage/cluster_collector.py"
      provides: "SSHClusterCollector class"
      contains: "class SSHClusterCollector"
    - path: "tests/unit/test_cluster_collector.py"
      provides: "Unit tests for SSHClusterCollector"
      contains: "TestSSHClusterCollector"
  key_links:
    - from: "SSHClusterCollector"
      to: "ClusterCollectorInterface"
      via: "inheritance"
      pattern: "class SSHClusterCollector.*ClusterCollectorInterface"
    - from: "SSHClusterCollector._collect_from_single_host"
      to: "subprocess.run"
      via: "SSH command execution"
      pattern: "subprocess\\.run.*ssh"
---

<objective>
Implement SSHClusterCollector class for collecting host information from remote hosts via SSH.

Purpose: Enable non-MPI benchmarks (KV Cache, VectorDB) to collect cluster information using SSH instead of MPI.

Output: SSHClusterCollector class implementing ClusterCollectorInterface with parallel SSH execution, localhost optimization, and unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-ssh-based-host-collection/06-RESEARCH.md
@.planning/phases/06-ssh-based-host-collection/06-01-SUMMARY.md

@mlpstorage/cluster_collector.py
@mlpstorage/interfaces/collector.py
@mlpstorage/environment/validators.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SSHClusterCollector class</name>
  <files>mlpstorage/cluster_collector.py</files>
  <action>
Add SSHClusterCollector class to cluster_collector.py after the MPIClusterCollector class. This class implements ClusterCollectorInterface and uses SSH to collect host information.

1. Add required imports at the top of the file:
```python
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from mlpstorage.interfaces.collector import ClusterCollectorInterface, CollectionResult
```

2. Add localhost detection constants and helper after the imports:
```python
LOCALHOST_IDENTIFIERS = ('localhost', '127.0.0.1', '::1')

def _is_localhost(hostname: str) -> bool:
    """Check if hostname refers to local machine."""
    hostname_lower = hostname.lower()
    if hostname_lower in LOCALHOST_IDENTIFIERS:
        return True
    try:
        local_hostname = socket.gethostname()
        if hostname_lower == local_hostname.lower():
            return True
        local_fqdn = socket.getfqdn()
        if hostname_lower == local_fqdn.lower():
            return True
    except Exception:
        pass
    return False
```

3. Add SSH collection script (similar to MPI script but simpler):
```python
SSH_COLLECTOR_SCRIPT = '''
import json
import socket
import time

def collect():
    result = {"hostname": socket.gethostname(), "errors": {}}

    files = [
        ("/proc/meminfo", "meminfo"),
        ("/proc/cpuinfo", "cpuinfo"),
        ("/proc/diskstats", "diskstats"),
        ("/proc/net/dev", "netdev"),
        ("/proc/version", "version"),
        ("/proc/loadavg", "loadavg"),
        ("/proc/uptime", "uptime"),
        ("/proc/vmstat", "vmstat"),
        ("/proc/mounts", "mounts"),
        ("/proc/cgroups", "cgroups"),
    ]

    for path, key in files:
        try:
            with open(path) as f:
                result[key] = f.read()
        except Exception as e:
            result["errors"][key] = str(e)
            result[key] = ""

    try:
        with open("/etc/os-release") as f:
            result["os_release_raw"] = f.read()
    except Exception as e:
        result["errors"]["os_release"] = str(e)

    result["collection_timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    print(json.dumps(result))

collect()
'''
```

4. Add SSHClusterCollector class:
```python
class SSHClusterCollector(ClusterCollectorInterface):
    """Collects system information from hosts using SSH.

    This collector uses SSH to gather system information from remote hosts.
    For localhost, it uses direct local collection to avoid SSH overhead
    and configuration requirements.

    Attributes:
        hosts: List of hostnames or IP addresses to collect from.
        logger: Logger instance for output.
        ssh_username: Optional SSH username (defaults to current user).
        timeout: Timeout in seconds for SSH connections.
        max_workers: Maximum number of parallel SSH connections.
    """

    def __init__(
        self,
        hosts: List[str],
        logger,
        ssh_username: Optional[str] = None,
        timeout_seconds: int = 60,
        max_workers: int = 10
    ):
        """Initialize the SSH cluster collector.

        Args:
            hosts: List of hostnames/IPs, optionally with slot counts (e.g., "host1:4").
            logger: Logger instance for messages.
            ssh_username: Optional SSH username. If not provided, uses current user.
            timeout_seconds: Maximum time to wait for SSH connections.
            max_workers: Maximum number of parallel SSH connections.
        """
        self.hosts = hosts
        self.logger = logger
        self.ssh_username = ssh_username
        self.timeout = timeout_seconds
        self.max_workers = max_workers

    def _get_unique_hosts(self) -> List[str]:
        """Extract unique hostnames from the hosts list (removing slot counts)."""
        unique = []
        seen = set()
        for host in self.hosts:
            hostname = host.split(':')[0].strip() if ':' in host else host.strip()
            if hostname and hostname not in seen:
                seen.add(hostname)
                unique.append(hostname)
        return unique

    def _build_ssh_command(self, hostname: str, remote_cmd: str) -> List[str]:
        """Build SSH command with proper options for automation."""
        cmd = [
            'ssh',
            '-o', 'BatchMode=yes',
            '-o', f'ConnectTimeout={self.timeout}',
            '-o', 'StrictHostKeyChecking=accept-new',
        ]
        if self.ssh_username:
            cmd.extend(['-l', self.ssh_username])
        cmd.extend([hostname, remote_cmd])
        return cmd

    def _parse_raw_collection(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse raw /proc file contents into structured data."""
        parsed = {
            'hostname': raw_data.get('hostname', 'unknown'),
            'collection_timestamp': raw_data.get('collection_timestamp'),
            'errors': raw_data.get('errors', {}),
        }

        # Parse meminfo
        if raw_data.get('meminfo'):
            parsed['meminfo'] = parse_proc_meminfo(raw_data['meminfo'])
        else:
            parsed['meminfo'] = {}

        # Parse cpuinfo
        if raw_data.get('cpuinfo'):
            parsed['cpuinfo'] = parse_proc_cpuinfo(raw_data['cpuinfo'])
        else:
            parsed['cpuinfo'] = []

        # Parse diskstats
        if raw_data.get('diskstats'):
            disks = parse_proc_diskstats(raw_data['diskstats'])
            parsed['diskstats'] = [d.to_dict() for d in disks]
        else:
            parsed['diskstats'] = []

        # Parse netdev
        if raw_data.get('netdev'):
            interfaces = parse_proc_net_dev(raw_data['netdev'])
            parsed['netdev'] = [n.to_dict() for n in interfaces]
        else:
            parsed['netdev'] = []

        # Parse version
        parsed['version'] = parse_proc_version(raw_data.get('version', ''))

        # Parse loadavg
        if raw_data.get('loadavg'):
            load_1, load_5, load_15, running, total = parse_proc_loadavg(raw_data['loadavg'])
            parsed['loadavg'] = {
                'load_1min': load_1,
                'load_5min': load_5,
                'load_15min': load_15,
                'running_processes': running,
                'total_processes': total
            }
        else:
            parsed['loadavg'] = {}

        # Parse uptime
        parsed['uptime_seconds'] = parse_proc_uptime(raw_data.get('uptime', ''))

        # Parse os_release
        if raw_data.get('os_release_raw'):
            parsed['os_release'] = parse_os_release(raw_data['os_release_raw'])
        else:
            parsed['os_release'] = {}

        # Parse vmstat
        if raw_data.get('vmstat'):
            parsed['vmstat'] = parse_proc_vmstat(raw_data['vmstat'])
        else:
            parsed['vmstat'] = {}

        # Parse mounts
        if raw_data.get('mounts'):
            mounts = parse_proc_mounts(raw_data['mounts'])
            parsed['mounts'] = [m.to_dict() for m in mounts]
        else:
            parsed['mounts'] = []

        # Parse cgroups
        if raw_data.get('cgroups'):
            cgroups = parse_proc_cgroups(raw_data['cgroups'])
            parsed['cgroups'] = [c.to_dict() for c in cgroups]
        else:
            parsed['cgroups'] = []

        if not parsed['errors']:
            del parsed['errors']

        return parsed

    def _collect_from_single_host(self, hostname: str) -> Dict[str, Any]:
        """Collect system information from a single host via SSH."""
        if _is_localhost(hostname):
            self.logger.debug(f'Collecting from {hostname} (localhost) via direct access')
            return collect_local_system_info()

        self.logger.debug(f'Collecting from {hostname} via SSH')

        # Build the remote command to run the collector script
        remote_cmd = f"python3 -c '{SSH_COLLECTOR_SCRIPT}'"
        cmd = self._build_ssh_command(hostname, remote_cmd)

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.timeout + 10  # Extra buffer for SSH overhead
            )

            if result.returncode != 0:
                error_msg = result.stderr.strip() or f'SSH failed with code {result.returncode}'
                self.logger.warning(f'SSH collection from {hostname} failed: {error_msg}')
                return {'hostname': hostname, 'error': error_msg}

            # Parse the JSON output
            try:
                raw_data = json.loads(result.stdout)
                return self._parse_raw_collection(raw_data)
            except json.JSONDecodeError as e:
                self.logger.warning(f'Failed to parse JSON from {hostname}: {e}')
                return {'hostname': hostname, 'error': f'JSON parse error: {e}'}

        except subprocess.TimeoutExpired:
            self.logger.warning(f'SSH to {hostname} timed out after {self.timeout}s')
            return {'hostname': hostname, 'error': f'Timeout after {self.timeout}s'}

        except Exception as e:
            self.logger.warning(f'SSH collection from {hostname} failed: {e}')
            return {'hostname': hostname, 'error': str(e)}

    def collect(self, hosts: List[str], timeout: int = 60) -> CollectionResult:
        """Collect information from all specified hosts in parallel.

        Args:
            hosts: List of hostnames or IP addresses to collect from.
            timeout: Maximum time in seconds to wait for collection.

        Returns:
            CollectionResult with data from all hosts.
        """
        unique_hosts = self._get_unique_hosts()
        self.logger.debug(f'Starting SSH cluster collection on {len(unique_hosts)} hosts')

        results = {}
        errors = []

        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(unique_hosts))) as executor:
            future_to_host = {
                executor.submit(self._collect_from_single_host, host): host
                for host in unique_hosts
            }

            for future in as_completed(future_to_host):
                host = future_to_host[future]
                try:
                    host_data = future.result()
                    if 'error' in host_data and len(host_data) <= 2:
                        # Collection failed for this host
                        errors.append(f"{host}: {host_data.get('error', 'Unknown error')}")
                    results[host] = host_data
                except Exception as e:
                    self.logger.warning(f'Exception collecting from {host}: {e}')
                    errors.append(f"{host}: {str(e)}")
                    results[host] = {'hostname': host, 'error': str(e)}

        success = len(errors) == 0 or len(results) > len(errors)

        return CollectionResult(
            success=success,
            data=results,
            errors=errors,
            collection_method='ssh',
            timestamp=time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
        )

    def collect_local(self) -> CollectionResult:
        """Collect information from local host only.

        Returns:
            CollectionResult with local host data.
        """
        local_info = collect_local_system_info()
        hostname = local_info.get('hostname', 'localhost')

        return CollectionResult(
            success=True,
            data={hostname: local_info},
            errors=[],
            collection_method='local',
            timestamp=time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
        )

    def is_available(self) -> bool:
        """Check if SSH is available for use.

        Returns:
            True if SSH command is available, False otherwise.
        """
        return shutil.which('ssh') is not None

    def get_collection_method(self) -> str:
        """Return the name of the collection method.

        Returns:
            String identifier 'ssh'.
        """
        return 'ssh'
```
  </action>
  <verify>
python -c "from mlpstorage.cluster_collector import SSHClusterCollector; from mlpstorage.interfaces.collector import ClusterCollectorInterface; assert issubclass(SSHClusterCollector, ClusterCollectorInterface); print('SSHClusterCollector implements ClusterCollectorInterface')"
  </verify>
  <done>SSHClusterCollector class is implemented and correctly inherits from ClusterCollectorInterface.</done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for SSHClusterCollector</name>
  <files>tests/unit/test_cluster_collector.py</files>
  <action>
Add tests for SSHClusterCollector to the existing tests/unit/test_cluster_collector.py file:

```python
from unittest.mock import MagicMock, patch, Mock
import json

from mlpstorage.cluster_collector import (
    SSHClusterCollector,
    _is_localhost,
    collect_local_system_info,
)
from mlpstorage.interfaces.collector import CollectionResult


class TestIsLocalhost:
    """Tests for _is_localhost helper function."""

    def test_localhost_string(self):
        """Test that 'localhost' is detected."""
        assert _is_localhost('localhost') is True

    def test_localhost_ipv4(self):
        """Test that 127.0.0.1 is detected."""
        assert _is_localhost('127.0.0.1') is True

    def test_localhost_ipv6(self):
        """Test that ::1 is detected."""
        assert _is_localhost('::1') is True

    def test_localhost_case_insensitive(self):
        """Test case insensitivity."""
        assert _is_localhost('LOCALHOST') is True
        assert _is_localhost('LocalHost') is True

    def test_remote_host(self):
        """Test that remote host is not localhost."""
        assert _is_localhost('node1.example.com') is False
        assert _is_localhost('192.168.1.100') is False


class TestSSHClusterCollector:
    """Tests for SSHClusterCollector class."""

    @pytest.fixture
    def mock_logger(self):
        """Create a mock logger."""
        return MagicMock()

    @pytest.fixture
    def collector(self, mock_logger):
        """Create a collector instance."""
        return SSHClusterCollector(
            hosts=['node1', 'node2:4', 'localhost'],
            logger=mock_logger
        )

    def test_get_unique_hosts(self, collector):
        """Test that unique hosts are extracted correctly."""
        unique = collector._get_unique_hosts()
        assert unique == ['node1', 'node2', 'localhost']

    def test_get_unique_hosts_removes_duplicates(self, mock_logger):
        """Test that duplicate hosts are removed."""
        collector = SSHClusterCollector(
            hosts=['node1', 'node1:4', 'node2'],
            logger=mock_logger
        )
        unique = collector._get_unique_hosts()
        assert unique == ['node1', 'node2']

    def test_build_ssh_command_basic(self, collector):
        """Test basic SSH command construction."""
        cmd = collector._build_ssh_command('node1', 'echo test')
        assert 'ssh' in cmd
        assert '-o' in cmd
        assert 'BatchMode=yes' in cmd
        assert 'node1' in cmd
        assert 'echo test' in cmd

    def test_build_ssh_command_with_username(self, mock_logger):
        """Test SSH command with username."""
        collector = SSHClusterCollector(
            hosts=['node1'],
            logger=mock_logger,
            ssh_username='testuser'
        )
        cmd = collector._build_ssh_command('node1', 'echo test')
        assert '-l' in cmd
        assert 'testuser' in cmd

    def test_is_available_with_ssh(self, collector):
        """Test is_available when SSH exists."""
        with patch('shutil.which', return_value='/usr/bin/ssh'):
            assert collector.is_available() is True

    def test_is_available_without_ssh(self, collector):
        """Test is_available when SSH is missing."""
        with patch('shutil.which', return_value=None):
            assert collector.is_available() is False

    def test_get_collection_method(self, collector):
        """Test get_collection_method returns 'ssh'."""
        assert collector.get_collection_method() == 'ssh'

    def test_collect_local(self, collector):
        """Test collect_local returns local system info."""
        result = collector.collect_local()
        assert isinstance(result, CollectionResult)
        assert result.success is True
        assert result.collection_method == 'local'
        assert len(result.data) == 1

    @patch('mlpstorage.cluster_collector.collect_local_system_info')
    def test_collect_from_localhost_uses_direct_collection(self, mock_local, collector):
        """Test that localhost uses direct collection, not SSH."""
        mock_local.return_value = {'hostname': 'localhost', 'meminfo': {}}
        result = collector._collect_from_single_host('localhost')
        mock_local.assert_called_once()
        assert result['hostname'] == 'localhost'

    @patch('subprocess.run')
    def test_collect_from_remote_host(self, mock_run, collector):
        """Test collecting from a remote host via SSH."""
        mock_run.return_value = Mock(
            returncode=0,
            stdout=json.dumps({
                'hostname': 'node1',
                'meminfo': 'MemTotal: 16384000 kB\n',
                'cpuinfo': '',
                'diskstats': '',
                'netdev': '',
                'version': 'Linux version 5.4.0',
                'loadavg': '0.1 0.2 0.3 1/100 12345',
                'uptime': '12345.67',
                'vmstat': 'nr_free_pages 12345\n',
                'mounts': '/dev/sda1 / ext4 rw 0 1\n',
                'cgroups': '#subsys_name\tcpu\t0\t1\t1\n',
                'os_release_raw': 'NAME="Ubuntu"\n',
                'collection_timestamp': '2026-01-24T12:00:00Z'
            }),
            stderr=''
        )

        result = collector._collect_from_single_host('node1')
        mock_run.assert_called_once()
        assert result['hostname'] == 'node1'
        assert 'meminfo' in result

    @patch('subprocess.run')
    def test_collect_handles_ssh_failure(self, mock_run, collector):
        """Test handling of SSH connection failure."""
        mock_run.return_value = Mock(
            returncode=255,
            stdout='',
            stderr='Connection refused'
        )

        result = collector._collect_from_single_host('node1')
        assert 'error' in result
        assert 'Connection refused' in result['error']

    @patch('subprocess.run')
    def test_collect_handles_timeout(self, mock_run, collector):
        """Test handling of SSH timeout."""
        import subprocess
        mock_run.side_effect = subprocess.TimeoutExpired('ssh', 60)

        result = collector._collect_from_single_host('node1')
        assert 'error' in result
        assert 'Timeout' in result['error']
```
  </action>
  <verify>pytest tests/unit/test_cluster_collector.py -v -k "SSHClusterCollector or Localhost"</verify>
  <done>All unit tests for SSHClusterCollector pass, covering localhost detection, SSH command building, parallel collection, and error handling.</done>
</task>

</tasks>

<verification>
1. SSHClusterCollector imports correctly: `python -c "from mlpstorage.cluster_collector import SSHClusterCollector"`
2. SSHClusterCollector implements ClusterCollectorInterface
3. All unit tests pass: `pytest tests/unit/test_cluster_collector.py -v`
4. Localhost detection works correctly
5. SSH command uses BatchMode for non-interactive execution
</verification>

<success_criteria>
1. SSHClusterCollector class exists and implements ClusterCollectorInterface
2. _is_localhost helper correctly identifies localhost variants
3. collect() uses ThreadPoolExecutor for parallel SSH
4. collect_from_single_host uses direct collection for localhost
5. SSH commands use BatchMode=yes and ConnectTimeout
6. All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-ssh-based-host-collection/06-02-SUMMARY.md`
</output>
