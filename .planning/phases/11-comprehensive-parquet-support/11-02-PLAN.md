---
phase: 11-comprehensive-parquet-support
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py
  - dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py
autonomous: true

must_haves:
  truths:
    - "ParquetReader reads columns specified in config, not entire table"
    - "ParquetReader validates file schema matches config on open"
    - "ParquetReader supports row_group iteration mode for large files"
    - "ParquetGenerator creates files with config-driven schema and multiple dtypes"
    - "ParquetGenerator supports None, Snappy, GZIP, ZSTD, LZ4 compression"
    - "ParquetGenerator supports configurable row_group_size"
  artifacts:
    - path: "dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py"
      provides: "Memory-efficient parquet reader with column filtering and schema validation"
      min_lines: 60
    - path: "dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py"
      provides: "Schema-driven parquet generator with compression and partitioning"
      min_lines: 80
  key_links:
    - from: "dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py"
      to: "dlio_parquet_fork/dlio_benchmark/utils/config.py"
      via: "ConfigArguments.parquet_columns, parquet_read_mode"
      pattern: "self\\._args\\.parquet_"
    - from: "dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py"
      to: "dlio_parquet_fork/dlio_benchmark/common/enumerations.py"
      via: "Compression enum mapping including LZ4, ZSTD"
      pattern: "Compression\\.(LZ4|ZSTD)"
---

<objective>
Rewrite ParquetReader for memory-efficient column-filtered reads with schema validation, and rewrite ParquetGenerator for config-driven schema with full compression support.

Purpose: Production-ready parquet I/O replacing naive Phase 9 implementation.
Output: Rewritten parquet_reader.py and parquet_generator.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-comprehensive-parquet-support/11-CONTEXT.md
@.planning/phases/11-comprehensive-parquet-support/11-RESEARCH.md
@.planning/phases/11-comprehensive-parquet-support/11-01-SUMMARY.md
@dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py
@dlio_parquet_fork/dlio_benchmark/reader/reader_handler.py
@dlio_parquet_fork/dlio_benchmark/reader/hdf5_reader.py
@dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py
@dlio_parquet_fork/dlio_benchmark/data_generator/data_generator.py
@dlio_parquet_fork/dlio_benchmark/data_generator/hdf5_generator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite ParquetReader with memory-efficient I/O</name>
  <files>dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py</files>
  <action>
Rewrite parquet_reader.py completely. Keep the Apache 2.0 license header and class structure matching FormatReader interface. Key changes:

1. **__init__**: Extract column names from `self._args.parquet_columns`. If empty (no config), fall back to reading all columns (backward compat with Phase 9 single-column files). Store `self.read_mode = self._args.parquet_read_mode`.

2. **open(filename)**:
   - Call `super().open(filename)`
   - **Schema validation**: Use `pq.read_schema(filename)` to get file schema. If `self.column_names` is set, verify all requested columns exist in file schema. Raise `ValueError` with clear message listing missing columns if mismatch.
   - **Default mode** (`read_mode == "default"`): Use `pq.read_table(filename, columns=self.column_names if self.column_names else None, memory_map=True)`. Convert to numpy: `table.to_pandas().to_numpy()` -- this matches the existing DLIO pattern where `open_file_map[filename]` stores a numpy array indexed by sample_index. The key change is column filtering and memory_map.
   - **Row-group mode** (`read_mode == "row_group"`): Use `pq.ParquetFile(filename, memory_map=True)`. Store the ParquetFile handle. In this mode, `get_sample` will need to load the relevant row group. For simplicity, on first access read all row groups into a concatenated numpy array (lazy load), since DLIO's batch iteration pattern accesses samples by index.
   - Return the data (numpy array for default, ParquetFile for row_group).

3. **get_sample(filename, sample_index)**:
   - Call `super().get_sample(filename, sample_index)`
   - For default mode: `image = self.open_file_map[filename][sample_index]` (same as current)
   - For row_group mode: If `self.open_file_map[filename]` is a ParquetFile, read batches into numpy array, cache it, then index. Use a dict `self._cached_arrays` to store converted arrays.
   - Call `dlp.update(image_size=image.nbytes)`

4. **close, next, read_index, finalize, is_index_based, is_iterator_based**: Keep identical to current implementation (delegate to super).

IMPORTANT: The open() method must return data compatible with `self.open_file_map[filename][sample_index]` pattern used by the base class `next()`. The base class `next()` calls `self.get_sample()` which accesses `self.open_file_map[filename]` -- so the return value of open() gets stored there. For both modes, ensure a numpy array is ultimately what's indexed.

For row_group mode, a practical approach: in open(), read via ParquetFile and iter_batches, concatenate all batches into a single table, then convert to numpy. This gives memory-efficient reading (decompresses row-group by row-group) but still produces the numpy array DLIO expects. The memory savings come from column filtering, not from keeping data on disk.

```python
if self.read_mode == 'row_group':
    pf = pq.ParquetFile(filename, memory_map=True)
    batches = []
    for batch in pf.iter_batches(columns=self.column_names if self.column_names else None):
        batches.append(batch)
    table = pa.Table.from_batches(batches)
    return table.to_pandas().to_numpy()
```
  </action>
  <verify>Run `python -c "from dlio_benchmark.reader.parquet_reader import ParquetReader; print('OK')"` from dlio_parquet_fork directory to verify import works.</verify>
  <done>ParquetReader reads only configured columns, validates schema on open, supports default and row_group read modes, converts Arrow to numpy via column filtering</done>
</task>

<task type="auto">
  <name>Task 2: Rewrite ParquetGenerator with schema-driven generation</name>
  <files>dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py</files>
  <action>
Rewrite parquet_generator.py completely. Keep Apache 2.0 license header. Key changes:

1. **__init__**: Call `super().__init__()`. Extract config:
   - `self.parquet_columns = self._args.parquet_columns`
   - `self.row_group_size = self._args.parquet_row_group_size`
   - `self.partition_by = self._args.parquet_partition_by`

2. **Compression mapping**: Build a dict mapping Compression enum to PyArrow compression strings:
```python
from dlio_benchmark.common.enumerations import Compression

COMPRESSION_MAP = {
    Compression.NONE: None,
    Compression.SNAPPY: 'snappy',
    Compression.GZIP: 'gzip',
    Compression.LZ4: 'lz4',
    Compression.ZSTD: 'zstd',
}
```

3. **generate()**:
   - Call `super().generate()`, seed with `np.random.seed(10)`
   - For each file (same MPI rank distribution as current):
     - If `self.parquet_columns` is non-empty, use config-driven schema:
       - For each column spec, generate data based on dtype:
         - `float32`/`float64`: `np.random.rand(self.num_samples, col_spec.get('size', 1024)).astype(dtype)` -> store as `pa.list_(pa.float32())` Arrow type, convert rows with `.tolist()`
         - `string`: `[f"text_{j}" for j in range(self.num_samples)]` -> `pa.string()`
         - `binary`: `[np.random.bytes(col_spec.get('size', 256)) for _ in range(self.num_samples)]` -> `pa.binary()`
         - `bool`: `np.random.choice([True, False], self.num_samples).tolist()` -> `pa.bool_()`
         - `list`: treat like float32 with size
       - Build `pa.schema(fields)` and `pa.table(data_dict, schema=schema)`
     - If `self.parquet_columns` is empty (backward compat), use current behavior:
       - Generate single 'data' column with random uint8 array (same as Phase 9)
       - `table = pa.table({'data': [rec.tolist() for rec in records]})`
     - Get compression string from COMPRESSION_MAP (default None if not found)
     - If `self.partition_by`: use `pq.write_to_dataset(table, root_path=os.path.dirname(out_path_spec), partition_cols=[self.partition_by], compression=compression, row_group_size=self.row_group_size)`
     - Else: `pq.write_table(table, out_path_spec, compression=compression, row_group_size=self.row_group_size)`
   - Reset seed: `np.random.seed()`

Keep the `progress()` call for each file iteration.

Import `os` at top level. Import `pyarrow as pa` and `pyarrow.parquet as pq`.
  </action>
  <verify>Run `python -c "from dlio_benchmark.data_generator.parquet_generator import ParquetGenerator; print('OK')"` from dlio_parquet_fork directory to verify import works.</verify>
  <done>ParquetGenerator creates parquet files with config-driven multi-dtype schema, supports 5 compression options (None/Snappy/GZIP/ZSTD/LZ4), configurable row_group_size, optional Hive partitioning, backward compatible with empty config</done>
</task>

</tasks>

<verification>
- Both modules import without error
- ParquetReader references `self._args.parquet_columns` and `self._args.parquet_read_mode`
- ParquetReader has schema validation logic (pq.read_schema)
- ParquetGenerator maps all 5 Compression values to PyArrow strings
- ParquetGenerator handles float32, float64, string, binary, bool dtypes
- ParquetGenerator uses `pq.write_table` with `row_group_size` parameter
- Backward compatibility: empty parquet_columns falls back to Phase 9 behavior
</verification>

<success_criteria>
- ParquetReader does column-filtered reads with memory_map=True
- ParquetReader validates schema on open, raises ValueError on mismatch
- ParquetReader supports default and row_group read modes
- ParquetGenerator supports 5 dtypes from config
- ParquetGenerator supports 5 compression options including LZ4 and ZSTD
- ParquetGenerator supports row_group_size and Hive partitioning
- Both are backward compatible when parquet config section is omitted
</success_criteria>

<output>
After completion, create `.planning/phases/11-comprehensive-parquet-support/11-02-SUMMARY.md`
</output>
