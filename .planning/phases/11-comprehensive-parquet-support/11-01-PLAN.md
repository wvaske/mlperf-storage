---
phase: 11-comprehensive-parquet-support
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - dlio_parquet_fork/dlio_benchmark/common/enumerations.py
  - dlio_parquet_fork/dlio_benchmark/utils/config.py
autonomous: true

must_haves:
  truths:
    - "ConfigArguments has parquet_columns, parquet_row_group_size, parquet_read_mode, parquet_partition_by fields"
    - "Compression enum includes LZ4 and ZSTD values"
    - "LoadConfig parses dataset.parquet nested YAML section into flat ConfigArguments fields"
  artifacts:
    - path: "dlio_parquet_fork/dlio_benchmark/common/enumerations.py"
      provides: "LZ4 and ZSTD compression enum values"
      contains: "LZ4|ZSTD"
    - path: "dlio_parquet_fork/dlio_benchmark/utils/config.py"
      provides: "Parquet config fields and LoadConfig parsing"
      contains: "parquet_columns"
  key_links:
    - from: "dlio_parquet_fork/dlio_benchmark/utils/config.py"
      to: "dlio_parquet_fork/dlio_benchmark/common/enumerations.py"
      via: "Compression enum import"
      pattern: "Compression\\.LZ4|Compression\\.ZSTD"
---

<objective>
Add parquet-specific configuration fields to DLIO's ConfigArguments and extend the Compression enum with LZ4 and ZSTD.

Purpose: Foundation for memory-efficient parquet reader and schema-driven generator.
Output: Extended ConfigArguments dataclass and Compression enum.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-comprehensive-parquet-support/11-CONTEXT.md
@.planning/phases/11-comprehensive-parquet-support/11-RESEARCH.md
@dlio_parquet_fork/dlio_benchmark/common/enumerations.py
@dlio_parquet_fork/dlio_benchmark/utils/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LZ4 and ZSTD to Compression enum</name>
  <files>dlio_parquet_fork/dlio_benchmark/common/enumerations.py</files>
  <action>
Add two new values to the Compression enum class (around line 280, before SNAPPY):

```python
LZ4 = 'lz4'
ZSTD = 'zstd'
```

Keep existing values (NONE, GZIP, LZF, BZIP2, ZIP, XZ, SNAPPY) unchanged.
  </action>
  <verify>Run `python -c "from dlio_benchmark.common.enumerations import Compression; print(Compression.LZ4, Compression.ZSTD)"` from dlio_parquet_fork directory. Should print `Compression.LZ4 Compression.ZSTD`.</verify>
  <done>Compression enum has LZ4='lz4' and ZSTD='zstd' members</done>
</task>

<task type="auto">
  <name>Task 2: Add parquet config fields to ConfigArguments and LoadConfig</name>
  <files>dlio_parquet_fork/dlio_benchmark/utils/config.py</files>
  <action>
1. Add these fields to the ConfigArguments dataclass (after the `odirect` field, around line 148):

```python
# Parquet-specific configuration
parquet_columns: ClassVar[List[Dict[str, Any]]] = []
parquet_row_group_size: int = 1000000
parquet_read_mode: str = "default"  # "default" or "row_group"
parquet_partition_by: str = None
```

Note: `parquet_columns` must be ClassVar because it's a mutable list (same pattern as `computation_time`, `optimization_groups`).

2. In the LoadConfig function, after the existing `dataset` block (after the `keep_files` check around line 755), add parsing for nested parquet config:

```python
        if 'parquet' in config['dataset']:
            parquet_cfg = config['dataset']['parquet']
            if 'columns' in parquet_cfg:
                if isinstance(parquet_cfg['columns'], (list, DictConfig)):
                    args.parquet_columns = OmegaConf.to_container(parquet_cfg['columns']) if isinstance(parquet_cfg['columns'], DictConfig) else parquet_cfg['columns']
                else:
                    args.parquet_columns = parquet_cfg['columns']
            if 'row_group_size' in parquet_cfg:
                args.parquet_row_group_size = parquet_cfg['row_group_size']
            if 'read_mode' in parquet_cfg:
                args.parquet_read_mode = parquet_cfg['read_mode']
            if 'partition_by' in parquet_cfg:
                args.parquet_partition_by = parquet_cfg['partition_by']
```

This must be inside the existing `if 'dataset' in config:` block, indented accordingly. Use OmegaConf.to_container() for the columns list since Hydra returns DictConfig objects.
  </action>
  <verify>Run `python -c "from dlio_benchmark.utils.config import ConfigArguments; print(hasattr(ConfigArguments, 'parquet_columns'), hasattr(ConfigArguments, 'parquet_row_group_size'))"` from dlio_parquet_fork directory. Should print `True True`.</verify>
  <done>ConfigArguments has 4 parquet fields, LoadConfig parses dataset.parquet nested config</done>
</task>

</tasks>

<verification>
- `python -c "from dlio_benchmark.common.enumerations import Compression; assert Compression.LZ4.value == 'lz4'; assert Compression.ZSTD.value == 'zstd'"` passes
- `python -c "from dlio_benchmark.utils.config import ConfigArguments; c = ConfigArguments.__dataclass_fields__; assert 'parquet_row_group_size' in c; assert 'parquet_read_mode' in c"` passes
- Existing DLIO imports still work: `python -c "from dlio_benchmark.utils.config import LoadConfig"` succeeds
</verification>

<success_criteria>
- Compression enum has LZ4 and ZSTD values
- ConfigArguments has parquet_columns, parquet_row_group_size, parquet_read_mode, parquet_partition_by
- LoadConfig handles dataset.parquet nested YAML section
- No existing functionality broken
</success_criteria>

<output>
After completion, create `.planning/phases/11-comprehensive-parquet-support/11-01-SUMMARY.md`
</output>
