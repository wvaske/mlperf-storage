---
phase: 10-progress-indication
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - mlpstorage/benchmarks/base.py
  - tests/unit/test_benchmarks_base.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User sees stage indicator during benchmark.run() execution"
    - "User sees elapsed time during 'Running benchmark...' stage"
    - "User sees spinner during cluster info collection"
    - "Stage transitions are visible: validating -> collecting -> running -> processing"
    - "Non-interactive terminals receive status log messages instead of animations"
    - "DLIO benchmark output is NOT wrapped in progress (flows through directly)"
  artifacts:
    - path: "mlpstorage/benchmarks/base.py"
      provides: "Stage indicators in benchmark lifecycle"
      contains: "from mlpstorage.progress import"
    - path: "tests/unit/test_benchmarks_base.py"
      provides: "Tests for progress integration"
      contains: "test_run_shows_stage"
  key_links:
    - from: "mlpstorage/benchmarks/base.py"
      to: "mlpstorage/progress.py"
      via: "import in run()"
      pattern: "from mlpstorage\\.progress import"
---

<objective>
Integrate progress indicators into Benchmark base class for visible stage feedback.

Purpose: Users see clear stage transitions and elapsed time during benchmark execution without interfering with DLIO's own progress output. The stage progress remains visible during the "Running benchmark..." stage, showing elapsed time. Cluster collection gets spinners for indeterminate operations.

Output: Updated base.py with progress integration in run() method, spinners in cluster collection methods, tests verifying progress behavior.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-progress-indication/10-RESEARCH.md
@.planning/phases/10-progress-indication/10-01-SUMMARY.md
@mlpstorage/benchmarks/base.py
@mlpstorage/progress.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add stage indicators to benchmark run() method</name>
  <files>mlpstorage/benchmarks/base.py</files>
  <action>
Modify the Benchmark base class run() method to wrap operations in stage progress.

1. Add import at top of file:
   ```python
   from mlpstorage.progress import create_stage_progress
   ```

2. Modify run() method to wrap operations in stage progress:
   ```python
   def run(self) -> int:
       """Execute the benchmark and track runtime.

       Wraps _run() with timing measurement, cluster collection, and
       time-series collection. Shows stage indicators during execution.
       """
       stages = [
           "Validating environment...",
           "Collecting cluster info...",
           "Running benchmark...",
           "Processing results...",
       ]

       with create_stage_progress(stages, logger=self.logger) as advance_stage:
           # Stage 1: Validation
           self._validate_environment()
           advance_stage()

           # Stage 2: Cluster collection
           self._collect_cluster_start()
           self._start_timeseries_collection()
           advance_stage()

           # Stage 3: Benchmark execution
           # Note: Stage progress remains visible showing elapsed time
           # during this phase. DLIO output flows through directly.
           start_time = time.time()
           try:
               result = self._run()
           finally:
               self.runtime = time.time() - start_time
               advance_stage()

               # Stage 4: Cleanup/Processing
               self._stop_timeseries_collection()
               self._collect_cluster_end()
               self.write_timeseries_data()
               advance_stage()

       return result
   ```

IMPORTANT: The stage progress shows TimeElapsedColumn which displays elapsed time during the "Running benchmark..." stage, satisfying the requirement that users see elapsed time during benchmark execution. Do NOT wrap the _run() call with additional progress - DLIO has its own progress output. The stage indicator remains visible alongside DLIO output.
  </action>
  <verify>python -c "from mlpstorage.benchmarks.base import Benchmark; print('Import OK')"</verify>
  <done>run() shows stage indicators with 4 stages, elapsed time visible during benchmark execution stage</done>
</task>

<task type="auto">
  <name>Task 2: Add spinners to cluster collection methods</name>
  <files>mlpstorage/benchmarks/base.py</files>
  <action>
Add spinners to _collect_cluster_start() and _collect_cluster_end() for visual feedback during indeterminate operations.

1. Add progress_context to the import statement from Task 1:
   ```python
   from mlpstorage.progress import create_stage_progress, progress_context
   ```

2. Add spinner to _collect_cluster_start():
   ```python
   def _collect_cluster_start(self) -> None:
       """Collect cluster information at benchmark start."""
       if not self._should_collect_cluster_info() and not self._should_use_ssh_collection():
           self.logger.debug('Skipping start cluster collection (conditions not met)')
           return

       hosts = self.args.hosts if hasattr(self.args, 'hosts') else []
       host_count = len(hosts) if hosts else 1

       with progress_context(
           f"Collecting cluster info ({host_count} host{'s' if host_count != 1 else ''})...",
           total=None,  # Indeterminate - spinner
           logger=self.logger
       ) as (update, set_desc):
           if self._should_use_ssh_collection():
               set_desc("Collecting via SSH...")
               self._cluster_info_start = self._collect_via_ssh()
               self._collection_method = 'ssh'
           else:
               set_desc("Collecting via MPI...")
               self._cluster_info_start = self._collect_cluster_information()
               self._collection_method = 'mpi'

       if self._cluster_info_start:
           self.logger.debug(f'Collected start cluster info via {self._collection_method}')
   ```

3. Add spinner to _collect_cluster_end():
   ```python
   def _collect_cluster_end(self) -> None:
       """Collect cluster information at benchmark end."""
       if not self._cluster_info_start:
           self.logger.debug('Skipping end cluster collection (no start collection)')
           return

       with progress_context(
           "Collecting end cluster info...",
           total=None,  # Indeterminate - spinner
           logger=self.logger
       ) as (update, set_desc):
           if self._collection_method == 'ssh':
               set_desc("Collecting via SSH...")
               self._cluster_info_end = self._collect_via_ssh()
           else:
               set_desc("Collecting via MPI...")
               self._cluster_info_end = self._collect_cluster_information()

       if self._cluster_info_end:
           self.logger.debug(f'Collected end cluster info via {self._collection_method}')
   ```

4. DO NOT add progress to _run() or _execute_command() - those methods execute DLIO which has its own progress output.
  </action>
  <verify>python -c "from mlpstorage.benchmarks.base import Benchmark; print('Import OK')" && grep -n "progress_context" mlpstorage/benchmarks/base.py</verify>
  <done>Cluster collection methods (_collect_cluster_start, _collect_cluster_end) show spinners with host count and collection method</done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for progress integration in base.py</name>
  <files>tests/unit/test_benchmarks_base.py</files>
  <action>
Add unit tests for progress integration to the existing test file.

Add new test class TestBenchmarkProgress:

1. test_run_shows_stage_progress:
   - Mock create_stage_progress
   - Call benchmark.run()
   - Verify create_stage_progress called with expected stages list
   - Verify advance_stage called 4 times (one per stage)

2. test_run_non_interactive_logs_stages:
   - Mock is_interactive_terminal() = False
   - Call benchmark.run()
   - Verify logger.status() called for each stage

3. test_cluster_collection_shows_spinner:
   - Mock progress_context
   - Set up args with hosts=['host1', 'host2']
   - Mock SSH collection to succeed
   - Call benchmark._collect_cluster_start()
   - Verify progress_context called with total=None (spinner)

4. test_cluster_collection_updates_description:
   - Mock progress_context to return mock set_desc
   - Call _collect_cluster_start() with SSH collection
   - Verify set_desc called with "Collecting via SSH..."

5. test_run_progress_cleanup_on_exception:
   - Make _run() raise exception
   - Verify stage progress context properly exits (no resource leak)

Use fixtures from existing tests:
- basic_args fixture
- ConcreteBenchmark class
- tmp_path for output directories
  </action>
  <verify>pytest tests/unit/test_benchmarks_base.py -v -k "progress or stage"</verify>
  <done>5+ new tests covering stage indicators, spinners, description updates, exception cleanup</done>
</task>

</tasks>

<verification>
1. Import still works: `python -c "from mlpstorage.benchmarks.base import Benchmark"`
2. Progress import present: `grep "from mlpstorage.progress import" mlpstorage/benchmarks/base.py`
3. Stage tests pass: `pytest tests/unit/test_benchmarks_base.py -v -k "progress or stage"`
4. All base tests pass: `pytest tests/unit/test_benchmarks_base.py -v`
5. No regressions: `pytest tests/unit -v --ignore=tests/unit/test_rules_calculations.py --ignore=tests/unit/test_reporting.py`
</verification>

<success_criteria>
1. run() method wraps operations in create_stage_progress with 4 stages
2. Elapsed time is visible during "Running benchmark..." stage via TimeElapsedColumn
3. Cluster collection methods use progress_context with spinner (total=None)
4. _run() and _execute_command() do NOT have progress wrapping (DLIO owns that output)
5. Non-interactive mode falls back to logger.status() calls
6. All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-progress-indication/10-02-SUMMARY.md`
</output>
