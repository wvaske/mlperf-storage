---
phase: 10-progress-indication
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - mlpstorage/benchmarks/base.py
  - tests/unit/test_benchmarks_base.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User sees stage indicator during benchmark.run() execution"
    - "User sees spinner during cluster info collection"
    - "Stage transitions are visible: validating -> collecting -> running -> processing"
    - "Non-interactive terminals receive status log messages instead of animations"
    - "DLIO benchmark output is NOT wrapped in progress (flows through directly)"
  artifacts:
    - path: "mlpstorage/benchmarks/base.py"
      provides: "Stage indicators in benchmark lifecycle"
      contains: "from mlpstorage.progress import"
    - path: "tests/unit/test_benchmarks_base.py"
      provides: "Tests for progress integration"
      contains: "test_run_shows_stage"
  key_links:
    - from: "mlpstorage/benchmarks/base.py"
      to: "mlpstorage/progress.py"
      via: "import in run()"
      pattern: "from mlpstorage\\.progress import"
---

<objective>
Integrate progress indicators into Benchmark base class for visible stage feedback.

Purpose: Users see clear stage transitions during benchmark execution without interfering with DLIO's own progress output. Cluster collection gets spinners for indeterminate operations.

Output: Updated base.py with progress integration in run() method, spinners in cluster collection methods, tests verifying progress behavior.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-progress-indication/10-RESEARCH.md
@.planning/phases/10-progress-indication/10-01-SUMMARY.md
@mlpstorage/benchmarks/base.py
@mlpstorage/progress.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add stage indicators to benchmark run() method</name>
  <files>mlpstorage/benchmarks/base.py</files>
  <action>
Modify the Benchmark base class to show stage progress during run() execution.

1. Add import at top of file:
   ```python
   from mlpstorage.progress import create_stage_progress, progress_context
   ```

2. Modify run() method to wrap operations in stage progress:
   ```python
   def run(self) -> int:
       """Execute the benchmark and track runtime.

       Wraps _run() with timing measurement, cluster collection, and
       time-series collection. Shows stage indicators during execution.
       """
       stages = [
           "Validating environment...",
           "Collecting cluster info...",
           "Running benchmark...",
           "Processing results...",
       ]

       with create_stage_progress(stages, logger=self.logger) as advance_stage:
           # Stage 1: Validation
           self._validate_environment()
           advance_stage()

           # Stage 2: Cluster collection
           self._collect_cluster_start()
           self._start_timeseries_collection()
           advance_stage()

           # Stage 3: Benchmark execution
           start_time = time.time()
           try:
               result = self._run()
           finally:
               self.runtime = time.time() - start_time
               advance_stage()

               # Stage 4: Cleanup/Processing
               self._stop_timeseries_collection()
               self._collect_cluster_end()
               self.write_timeseries_data()
               advance_stage()

       return result
   ```

IMPORTANT: Do NOT wrap the _run() call with progress - DLIO has its own progress output. Only show stage transitions BETWEEN operations.

3. Add spinner to _collect_cluster_start():
   ```python
   def _collect_cluster_start(self) -> None:
       """Collect cluster information at benchmark start."""
       if not self._should_collect_cluster_info() and not self._should_use_ssh_collection():
           self.logger.debug('Skipping start cluster collection (conditions not met)')
           return

       hosts = self.args.hosts if hasattr(self.args, 'hosts') else []
       host_count = len(hosts) if hosts else 1

       with progress_context(
           f"Collecting cluster info ({host_count} host{'s' if host_count != 1 else ''})...",
           total=None,  # Indeterminate - spinner
           logger=self.logger
       ) as (update, set_desc):
           if self._should_use_ssh_collection():
               set_desc("Collecting via SSH...")
               self._cluster_info_start = self._collect_via_ssh()
               self._collection_method = 'ssh'
           else:
               set_desc("Collecting via MPI...")
               self._cluster_info_start = self._collect_cluster_information()
               self._collection_method = 'mpi'

       if self._cluster_info_start:
           self.logger.debug(f'Collected start cluster info via {self._collection_method}')
   ```

4. Similar spinner for _collect_cluster_end():
   - Wrap collection in progress_context with spinner
   - Only if _cluster_info_start exists

5. DO NOT add progress to _run() or _execute_command() - those methods execute DLIO which has its own progress output.
  </action>
  <verify>python -c "from mlpstorage.benchmarks.base import Benchmark; print('Import OK')"</verify>
  <done>run() shows stage indicators, cluster collection shows spinners, no progress wrapping around DLIO execution</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for progress integration in base.py</name>
  <files>tests/unit/test_benchmarks_base.py</files>
  <action>
Add unit tests for progress integration to the existing test file.

Add new test class TestBenchmarkProgress:

1. test_run_shows_stage_progress:
   - Mock create_stage_progress
   - Call benchmark.run()
   - Verify create_stage_progress called with expected stages list
   - Verify advance_stage called 4 times (one per stage)

2. test_run_non_interactive_logs_stages:
   - Mock is_interactive_terminal() = False
   - Call benchmark.run()
   - Verify logger.status() called for each stage

3. test_cluster_collection_shows_spinner:
   - Mock progress_context
   - Set up args with hosts=['host1', 'host2']
   - Mock SSH collection to succeed
   - Call benchmark._collect_cluster_start()
   - Verify progress_context called with total=None (spinner)

4. test_cluster_collection_updates_description:
   - Mock progress_context to return mock set_desc
   - Call _collect_cluster_start() with SSH collection
   - Verify set_desc called with "Collecting via SSH..."

5. test_run_progress_cleanup_on_exception:
   - Make _run() raise exception
   - Verify stage progress context properly exits (no resource leak)

Use fixtures from existing tests:
- basic_args fixture
- ConcreteBenchmark class
- tmp_path for output directories
  </action>
  <verify>pytest tests/unit/test_benchmarks_base.py -v -k "progress or stage"</verify>
  <done>5+ new tests covering stage indicators, spinners, description updates, exception cleanup</done>
</task>

</tasks>

<verification>
1. Import still works: `python -c "from mlpstorage.benchmarks.base import Benchmark"`
2. Progress import present: `grep "from mlpstorage.progress import" mlpstorage/benchmarks/base.py`
3. Stage tests pass: `pytest tests/unit/test_benchmarks_base.py -v -k "progress or stage"`
4. All base tests pass: `pytest tests/unit/test_benchmarks_base.py -v`
5. No regressions: `pytest tests/unit -v --ignore=tests/unit/test_rules_calculations.py --ignore=tests/unit/test_reporting.py`
</verification>

<success_criteria>
1. run() method wraps operations in create_stage_progress with 4 stages
2. Cluster collection methods use progress_context with spinner (total=None)
3. _run() and _execute_command() do NOT have progress wrapping (DLIO owns that output)
4. Non-interactive mode falls back to logger.status() calls
5. All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-progress-indication/10-02-SUMMARY.md`
</output>
