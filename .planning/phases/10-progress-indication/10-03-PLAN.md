---
phase: 10-progress-indication
plan: 03
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - mlpstorage/main.py
  - tests/unit/test_progress.py
autonomous: false
user_setup: []

must_haves:
  truths:
    - "Environment validation shows spinner during checks"
    - "User sees clear indication when validation is running"
    - "Lockfile validation shows spinner while checking packages"
    - "Progress indication works in interactive terminal"
    - "Non-interactive mode logs status messages cleanly"
  artifacts:
    - path: "mlpstorage/main.py"
      provides: "Progress during environment validation"
      contains: "from mlpstorage.progress import"
  key_links:
    - from: "mlpstorage/main.py"
      to: "mlpstorage/progress.py"
      via: "import"
      pattern: "from mlpstorage\\.progress import"
---

<objective>
Add progress indication to main.py for environment validation and verify complete progress integration.

Purpose: Users see clear feedback during environment validation and lockfile checking, completing the UX-04 requirement for progress indication during all long-running operations.

Output: Updated main.py with progress spinners, human verification of visual progress output.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-progress-indication/10-RESEARCH.md
@.planning/phases/10-progress-indication/10-01-SUMMARY.md
@mlpstorage/main.py
@mlpstorage/progress.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add progress to main.py validation and lockfile operations</name>
  <files>mlpstorage/main.py</files>
  <action>
Add progress spinners to environment validation and lockfile operations in main.py.

1. Add import at top of file:
   ```python
   from mlpstorage.progress import progress_context
   ```

2. Update run_benchmark() to show progress during lockfile validation:
   ```python
   # Validate lockfile if requested
   if hasattr(args, 'verify_lockfile') and args.verify_lockfile:
       with progress_context(
           "Validating packages against lockfile...",
           total=None,
           logger=logger
       ) as (update, set_desc):
           try:
               result = validate_lockfile(args.verify_lockfile, fail_on_missing=False)
               # ... existing error handling ...
           except FileNotFoundError:
               # ... existing handling ...
   ```

3. Update run_benchmark() to show progress during environment validation:
   ```python
   # Fail-fast environment validation
   skip_validation = getattr(args, 'skip_validation', False)
   if not skip_validation:
       with progress_context(
           "Validating environment...",
           total=None,
           logger=logger
       ) as (update, set_desc):
           validate_benchmark_environment(args, logger=logger)
   else:
       logger.warning("Skipping environment validation (--skip-validation flag)")
   ```

4. Update handle_lockfile_command() to show progress during generate:
   ```python
   if args.lockfile_command == "generate":
       try:
           with progress_context(
               "Generating lockfile...",
               total=None,
               logger=logger
           ) as (update, set_desc):
               if args.generate_all:
                   # ... existing generation ...
                   set_desc("Generating base lockfile...")
                   # ... etc
               else:
                   # ... existing single generation ...
   ```

5. Update handle_lockfile_command() verify operation:
   ```python
   elif args.lockfile_command == "verify":
       with progress_context(
           "Verifying lockfile...",
           total=None,
           logger=logger
       ) as (update, set_desc):
           try:
               # ... existing verification ...
   ```

Keep error handling intact - progress context wraps the operation but errors should still propagate correctly.
  </action>
  <verify>python -c "from mlpstorage.main import main, run_benchmark; print('Import OK')"</verify>
  <done>main.py shows progress spinners during validation and lockfile operations</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Progress indication throughout MLPerf Storage CLI</what-built>
  <how-to-verify>
Test progress indication visually in an interactive terminal:

1. Test environment validation progress:
   ```bash
   cd /home/wvaske/Projects/mlperf-storage
   # This should show a spinner during environment validation
   mlpstorage training run --model unet3d --num-accelerators 1 --accelerator-type h100 --data-dir /tmp/test --results-dir /tmp/results --what-if
   ```
   Expected: See "Validating environment..." spinner, then stage indicators during benchmark lifecycle

2. Test lockfile generation progress (if uv available):
   ```bash
   mlpstorage lockfile generate --output /tmp/test-lockfile.txt --pyproject pyproject.toml
   ```
   Expected: See "Generating lockfile..." spinner

3. Test lockfile verify progress:
   ```bash
   # First generate a lockfile, then verify
   mlpstorage lockfile verify /tmp/test-lockfile.txt --allow-missing
   ```
   Expected: See "Verifying lockfile..." spinner

4. Test non-interactive mode (redirected output):
   ```bash
   mlpstorage training run --model unet3d --num-accelerators 1 --accelerator-type h100 --data-dir /tmp/test --results-dir /tmp/results --what-if 2>&1 | head -20
   ```
   Expected: See status log messages instead of spinners (no garbled output)

5. Verify no interference with DLIO output:
   - If DLIO is installed, run actual benchmark briefly
   - DLIO's own progress should display cleanly without interference
  </how-to-verify>
  <resume-signal>Type "approved" if progress indicators work correctly in interactive and non-interactive modes, or describe any issues observed</resume-signal>
</task>

</tasks>

<verification>
1. Import works: `python -c "from mlpstorage.main import main"`
2. Progress import present: `grep "from mlpstorage.progress import" mlpstorage/main.py`
3. Unit tests still pass: `pytest tests/unit -v --ignore=tests/unit/test_rules_calculations.py --ignore=tests/unit/test_reporting.py`
4. Human verified progress indicators work visually
</verification>

<success_criteria>
1. Environment validation shows spinner in interactive mode
2. Lockfile generate/verify shows spinners in interactive mode
3. Non-interactive mode (redirected output) shows clean log messages
4. No interference with DLIO's own progress output
5. User confirms visual progress indicators are working
</success_criteria>

<output>
After completion, create `.planning/phases/10-progress-indication/10-03-SUMMARY.md`
</output>
