---
phase: 07-time-series-host-data-collection
plan: 03
type: execute
wave: 3
depends_on: ["07-01", "07-02"]
files_modified:
  - mlpstorage/cli/common_args.py
  - mlpstorage/cli/training_args.py
  - mlpstorage/cli/checkpointing_args.py
  - mlpstorage/cli/vectordb_args.py
  - mlpstorage/cli/kvcache_args.py
  - mlpstorage/benchmarks/base.py
  - tests/unit/test_benchmark_base.py
autonomous: true

must_haves:
  truths:
    - "User can specify --timeseries-interval to control collection interval"
    - "User can specify --skip-timeseries to disable time-series collection"
    - "User can specify --max-timeseries-samples to limit memory usage"
    - "Time-series collection starts before benchmark _run() and stops after"
    - "Time-series data file is written to results directory"
    - "Metadata includes reference to time-series data"
    - "Time-series file follows naming convention {benchmark_type}_{datetime}_timeseries.json"
  artifacts:
    - path: "mlpstorage/cli/common_args.py"
      provides: "Time-series CLI arguments"
      contains: "--timeseries-interval"
    - path: "mlpstorage/cli/training_args.py"
      provides: "Time-series arguments wired to training run command"
      contains: "add_timeseries_arguments"
    - path: "mlpstorage/cli/checkpointing_args.py"
      provides: "Time-series arguments wired to checkpointing run command"
      contains: "add_timeseries_arguments"
    - path: "mlpstorage/cli/vectordb_args.py"
      provides: "Time-series arguments wired to vectordb run command"
      contains: "add_timeseries_arguments"
    - path: "mlpstorage/cli/kvcache_args.py"
      provides: "Time-series arguments wired to kvcache run command"
      contains: "add_timeseries_arguments"
    - path: "mlpstorage/benchmarks/base.py"
      provides: "Time-series integration in Benchmark.run()"
      contains: "_start_timeseries_collection"
    - path: "tests/unit/test_benchmark_base.py"
      provides: "Unit tests for time-series benchmark integration"
      contains: "test_timeseries"
  key_links:
    - from: "mlpstorage/benchmarks/base.py"
      to: "MultiHostTimeSeriesCollector"
      via: "_start_timeseries_collection method"
      pattern: "MultiHostTimeSeriesCollector"
    - from: "mlpstorage/benchmarks/base.py"
      to: "TimeSeriesData"
      via: "time-series data construction"
      pattern: "TimeSeriesData"
    - from: "mlpstorage/benchmarks/base.py"
      to: "write_timeseries_data method"
      via: "output file writing"
      pattern: "timeseries\\.json"
    - from: "mlpstorage/cli/training_args.py"
      to: "add_timeseries_arguments"
      via: "function call for run parser"
      pattern: "add_timeseries_arguments\\(run"
    - from: "mlpstorage/cli/checkpointing_args.py"
      to: "add_timeseries_arguments"
      via: "function call for run parser"
      pattern: "add_timeseries_arguments\\(run"
    - from: "mlpstorage/cli/vectordb_args.py"
      to: "add_timeseries_arguments"
      via: "function call for run parser"
      pattern: "add_timeseries_arguments\\(run"
    - from: "mlpstorage/cli/kvcache_args.py"
      to: "add_timeseries_arguments"
      via: "function call for run parser"
      pattern: "add_timeseries_arguments\\(run"
---

<objective>
Integrate time-series collection into benchmark execution with CLI arguments and file output.

Purpose: Completes HOST-04 and HOST-05 requirements by integrating time-series collection into the benchmark execution flow, adding user-controllable CLI arguments, and writing time-series data to result files.

Output:
- CLI arguments: --timeseries-interval, --skip-timeseries, --max-timeseries-samples
- CLI arguments wired into all benchmark run parsers (training, checkpointing, vectordb, kvcache)
- Benchmark.run() integration that starts/stops time-series collection
- Time-series data written to {benchmark_type}_{datetime}_timeseries.json
- Metadata includes timeseries_data reference with file path
- Unit tests for the integration
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-time-series-host-data-collection/07-RESEARCH.md
@.planning/phases/07-time-series-host-data-collection/07-01-SUMMARY.md
@.planning/phases/07-time-series-host-data-collection/07-02-SUMMARY.md

# Key files to modify
@mlpstorage/cli/common_args.py
@mlpstorage/cli/training_args.py
@mlpstorage/cli/checkpointing_args.py
@mlpstorage/cli/vectordb_args.py
@mlpstorage/cli/kvcache_args.py
@mlpstorage/benchmarks/base.py
@tests/unit/test_benchmark_base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add time-series CLI arguments and wire into all benchmark parsers</name>
  <files>mlpstorage/cli/common_args.py, mlpstorage/cli/training_args.py, mlpstorage/cli/checkpointing_args.py, mlpstorage/cli/vectordb_args.py, mlpstorage/cli/kvcache_args.py</files>
  <action>
Add time-series related CLI arguments to common_args.py AND wire them into all benchmark CLI parsers.

**Step 1: Update common_args.py**

1. Add help messages to HELP_MESSAGES dict (after existing entries):
```python
    # Time-series collection help messages
    'timeseries_interval': (
        "Interval in seconds between time-series data collection samples during benchmark execution. "
        "Lower values provide more granular data but increase collection overhead. Default: 10 seconds."
    ),
    'skip_timeseries': (
        "Disable time-series host data collection during benchmark execution. "
        "Useful for debugging or when collection overhead is a concern."
    ),
    'max_timeseries_samples': (
        "Maximum number of time-series samples to keep per host. Prevents memory issues "
        "in long-running benchmarks. Default: 3600 (10 hours at 10-second intervals)."
    ),
```

2. Add new function for time-series arguments (after add_dlio_arguments or at end of file):
```python
def add_timeseries_arguments(parser):
    """Add time-series collection arguments.

    Args:
        parser: Argparse parser to add arguments to.
    """
    timeseries_group = parser.add_argument_group("Time-Series Collection")
    timeseries_group.add_argument(
        '--timeseries-interval',
        type=float,
        default=10.0,
        help=HELP_MESSAGES['timeseries_interval']
    )
    timeseries_group.add_argument(
        '--skip-timeseries',
        action='store_true',
        help=HELP_MESSAGES['skip_timeseries']
    )
    timeseries_group.add_argument(
        '--max-timeseries-samples',
        type=int,
        default=3600,
        help=HELP_MESSAGES['max_timeseries_samples']
    )
```

**Step 2: Wire into training_args.py**

1. Add import at top:
```python
from mlpstorage.cli.common_args import (
    HELP_MESSAGES,
    add_universal_arguments,
    add_mpi_arguments,
    add_host_arguments,
    add_dlio_arguments,
    add_timeseries_arguments,  # Add this
)
```

2. Add call after the run_benchmark parser is configured (after other argument additions for run_benchmark):
```python
    # Add time-series arguments to run command only
    add_timeseries_arguments(run_benchmark)
```

**Step 3: Wire into checkpointing_args.py**

1. Add import at top:
```python
from mlpstorage.cli.common_args import (
    # ... existing imports ...
    add_timeseries_arguments,  # Add this
)
```

2. Add call after the run_benchmark parser is configured:
```python
    # Add time-series arguments to run command only
    add_timeseries_arguments(run_benchmark)
```

**Step 4: Wire into vectordb_args.py**

1. Add import at top:
```python
from mlpstorage.cli.common_args import (
    # ... existing imports ...
    add_timeseries_arguments,  # Add this
)
```

2. Add call after the run parser is configured:
```python
    # Add time-series arguments to run command only
    add_timeseries_arguments(run_parser)
```

**Step 5: Wire into kvcache_args.py**

1. Add import at top:
```python
from mlpstorage.cli.common_args import (
    # ... existing imports ...
    add_timeseries_arguments,  # Add this
)
```

2. Add call after the run parser is configured:
```python
    # Add time-series arguments to run command only
    add_timeseries_arguments(run_parser)
```
  </action>
  <verify>
Run all these checks:
1. `python -c "from mlpstorage.cli.common_args import add_timeseries_arguments; print('common_args OK')"`
2. `python -c "from mlpstorage.cli.training_args import add_training_arguments; print('training_args OK')"`
3. `python -c "from mlpstorage.cli.checkpointing_args import add_checkpointing_arguments; print('checkpointing_args OK')"`
4. `python -c "from mlpstorage.cli.vectordb_args import add_vectordb_arguments; print('vectordb_args OK')"`
5. `python -c "from mlpstorage.cli.kvcache_args import add_kvcache_arguments; print('kvcache_args OK')"`
6. `grep -l "add_timeseries_arguments" mlpstorage/cli/*_args.py | wc -l` - should show 5 files
  </verify>
  <done>Time-series CLI arguments defined and wired into all four benchmark CLI parsers (training, checkpointing, vectordb, kvcache)</done>
</task>

<task type="auto">
  <name>Task 2: Integrate time-series collection into Benchmark base class</name>
  <files>mlpstorage/benchmarks/base.py</files>
  <action>
Add time-series collection integration to the Benchmark base class.

1. Add imports at top of file (with existing imports):
```python
from mlpstorage.cluster_collector import (
    collect_cluster_info,
    SSHClusterCollector,
    TimeSeriesCollector,
    MultiHostTimeSeriesCollector,
)
from mlpstorage.rules.models import ClusterSnapshots, TimeSeriesData, TimeSeriesSample
```

2. Add instance variables in __init__ after existing variables (around line 130):
```python
        # Time-series collection
        self._timeseries_collector = None
        self._timeseries_data = None
        self.timeseries_filename = f"{self.BENCHMARK_TYPE.value}_{self.run_datetime}_timeseries.json"
        self.timeseries_file_path = os.path.join(self.run_result_output, self.timeseries_filename)
```

3. Add time-series methods after _collect_cluster_end() method:
```python
    def _should_collect_timeseries(self) -> bool:
        """Determine if time-series collection should be performed.

        Returns:
            True if time-series collection should be performed.
        """
        # Check if user explicitly disabled
        if hasattr(self.args, 'skip_timeseries') and self.args.skip_timeseries:
            return False

        # Only collect for 'run' command
        if hasattr(self.args, 'command') and self.args.command not in ('run',):
            return False

        # Skip in what-if mode
        if hasattr(self.args, 'what_if') and self.args.what_if:
            return False

        return True

    def _start_timeseries_collection(self) -> None:
        """Start time-series collection in background.

        Uses MultiHostTimeSeriesCollector if hosts specified,
        otherwise uses single-host TimeSeriesCollector.

        Collection runs in a background thread to minimize performance impact
        on benchmark execution (HOST-05 requirement).
        """
        if not self._should_collect_timeseries():
            self.logger.debug('Skipping time-series collection (disabled or not applicable)')
            return

        interval = getattr(self.args, 'timeseries_interval', 10.0)
        max_samples = getattr(self.args, 'max_timeseries_samples', 3600)

        try:
            if hasattr(self.args, 'hosts') and self.args.hosts:
                # Multi-host collection
                ssh_username = getattr(self.args, 'ssh_username', None)
                ssh_timeout = getattr(self.args, 'cluster_collection_timeout', 30)

                self._timeseries_collector = MultiHostTimeSeriesCollector(
                    hosts=self.args.hosts,
                    interval_seconds=interval,
                    max_samples=max_samples,
                    ssh_username=ssh_username,
                    ssh_timeout=ssh_timeout,
                    logger=self.logger
                )
                self.logger.debug(
                    f'Starting multi-host time-series collection ({len(self.args.hosts)} hosts, '
                    f'interval={interval}s)'
                )
            else:
                # Single-host collection (localhost only)
                self._timeseries_collector = TimeSeriesCollector(
                    interval_seconds=interval,
                    max_samples=max_samples,
                    logger=self.logger
                )
                self.logger.debug(
                    f'Starting single-host time-series collection (interval={interval}s)'
                )

            self._timeseries_collector.start()

        except Exception as e:
            self.logger.warning(f'Failed to start time-series collection: {e}')
            self._timeseries_collector = None

    def _stop_timeseries_collection(self) -> None:
        """Stop time-series collection and store results."""
        if self._timeseries_collector is None:
            return

        try:
            if isinstance(self._timeseries_collector, MultiHostTimeSeriesCollector):
                samples_by_host = self._timeseries_collector.stop()
                hosts_collected = self._timeseries_collector.get_hosts_with_data()

                # Convert to TimeSeriesSample dataclasses
                samples_by_host_typed = {}
                total_samples = 0
                for host, samples in samples_by_host.items():
                    samples_by_host_typed[host] = [
                        TimeSeriesSample.from_dict(s) for s in samples
                    ]
                    total_samples += len(samples)

                self._timeseries_data = TimeSeriesData(
                    collection_interval_seconds=self._timeseries_collector.interval_seconds,
                    start_time=self._timeseries_collector.start_time or '',
                    end_time=self._timeseries_collector.end_time or '',
                    num_samples=total_samples,
                    samples_by_host=samples_by_host_typed,
                    collection_method='ssh' if len(hosts_collected) > 1 else 'local',
                    hosts_requested=list(self._timeseries_collector.hosts),
                    hosts_collected=hosts_collected,
                )

            else:
                # Single-host TimeSeriesCollector
                samples = self._timeseries_collector.stop()
                hostname = samples[0]['hostname'] if samples else 'localhost'

                samples_typed = [TimeSeriesSample.from_dict(s) for s in samples]

                self._timeseries_data = TimeSeriesData(
                    collection_interval_seconds=self._timeseries_collector.interval_seconds,
                    start_time=self._timeseries_collector.start_time or '',
                    end_time=self._timeseries_collector.end_time or '',
                    num_samples=len(samples),
                    samples_by_host={hostname: samples_typed},
                    collection_method='local',
                    hosts_requested=[hostname],
                    hosts_collected=[hostname] if samples else [],
                )

            self.logger.debug(
                f'Time-series collection complete ({self._timeseries_data.num_samples} samples)'
            )

        except Exception as e:
            self.logger.warning(f'Failed to stop time-series collection: {e}')
            self._timeseries_data = None

    def write_timeseries_data(self) -> None:
        """Write time-series data to JSON file.

        Output file follows naming convention: {benchmark_type}_{datetime}_timeseries.json
        This ensures the file is discoverable alongside other benchmark output files
        (HOST-04 requirement).
        """
        if self._timeseries_data is None:
            return

        try:
            with open(self.timeseries_file_path, 'w') as f:
                json.dump(self._timeseries_data.to_dict(), f, indent=2, cls=MLPSJsonEncoder)
            self.logger.verbose(f'Time-series data saved to: {self.timeseries_filename}')
        except Exception as e:
            self.logger.warning(f'Failed to write time-series data: {e}')
```

4. Update the metadata property to include time-series reference (in the metadata property, add after cluster_snapshots):
```python
        # Include time-series data reference if available (HOST-04)
        if hasattr(self, '_timeseries_data') and self._timeseries_data:
            metadata['timeseries_data'] = {
                'file': self.timeseries_filename,
                'num_samples': self._timeseries_data.num_samples,
                'interval_seconds': self._timeseries_data.collection_interval_seconds,
                'hosts_collected': self._timeseries_data.hosts_collected,
            }
```

5. Update run() method to include time-series collection (replace existing run method):
```python
    def run(self) -> int:
        """Execute the benchmark and track runtime.

        Wraps _run() with timing measurement, cluster collection, and
        time-series collection.

        Collects cluster information at start and end of benchmark
        (HOST-03 requirement).

        Collects time-series data during benchmark execution using a
        background thread to minimize performance impact (HOST-04, HOST-05).

        Returns:
            Exit code from _run().
        """
        self._validate_environment()

        # Collect cluster info at start (HOST-03)
        self._collect_cluster_start()

        # Start time-series collection (HOST-04, HOST-05)
        # Uses background thread for minimal performance impact
        self._start_timeseries_collection()

        start_time = time.time()
        try:
            result = self._run()
        finally:
            self.runtime = time.time() - start_time

            # Stop time-series collection
            self._stop_timeseries_collection()

            # Collect cluster info at end (HOST-03)
            self._collect_cluster_end()

            # Write time-series data to file
            self.write_timeseries_data()

        return result
```
  </action>
  <verify>
Run all these checks:
1. `python -c "from mlpstorage.benchmarks.base import Benchmark; print(hasattr(Benchmark, '_start_timeseries_collection'))"`
2. `python -c "from mlpstorage.benchmarks.base import Benchmark; print(hasattr(Benchmark, '_stop_timeseries_collection'))"`
3. `python -c "from mlpstorage.benchmarks.base import Benchmark; print(hasattr(Benchmark, 'write_timeseries_data'))"`
4. `python -c "from mlpstorage.benchmarks.base import Benchmark; print('OK')"`
  </verify>
  <done>Time-series collection integrated into Benchmark.run() with _start_timeseries_collection, _stop_timeseries_collection, and write_timeseries_data methods</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for time-series benchmark integration</name>
  <files>tests/unit/test_benchmark_base.py</files>
  <action>
Add unit tests for time-series integration in benchmark base class at the end of the test file.

```python
# =============================================================================
# Time-Series Collection Integration Tests
# =============================================================================

class TestTimeSeriesCollectionIntegration:
    """Tests for time-series collection integration in Benchmark base."""

    def test_should_collect_timeseries_default_true(self, mock_benchmark):
        """Time-series collection should be enabled by default for run command."""
        mock_benchmark.args.command = 'run'

        assert mock_benchmark._should_collect_timeseries() is True

    def test_should_collect_timeseries_skip_flag(self, mock_benchmark):
        """Time-series collection should be disabled when skip flag is set."""
        mock_benchmark.args.skip_timeseries = True

        assert mock_benchmark._should_collect_timeseries() is False

    def test_should_collect_timeseries_datagen_disabled(self, mock_benchmark):
        """Time-series collection should be disabled for datagen command."""
        mock_benchmark.args.command = 'datagen'

        assert mock_benchmark._should_collect_timeseries() is False

    def test_should_collect_timeseries_whatif_disabled(self, mock_benchmark):
        """Time-series collection should be disabled in what-if mode."""
        mock_benchmark.args.what_if = True
        mock_benchmark.args.command = 'run'

        assert mock_benchmark._should_collect_timeseries() is False

    def test_start_timeseries_creates_collector(self, mock_benchmark):
        """_start_timeseries_collection should create a collector."""
        mock_benchmark.args.command = 'run'
        mock_benchmark.args.timeseries_interval = 0.1
        mock_benchmark.args.max_timeseries_samples = 10

        mock_benchmark._start_timeseries_collection()

        assert mock_benchmark._timeseries_collector is not None
        assert mock_benchmark._timeseries_collector.is_running

        # Cleanup
        mock_benchmark._timeseries_collector.stop()

    def test_start_timeseries_multihost_with_hosts(self, mock_benchmark):
        """Should use MultiHostTimeSeriesCollector when hosts are provided."""
        from mlpstorage.cluster_collector import MultiHostTimeSeriesCollector

        mock_benchmark.args.command = 'run'
        mock_benchmark.args.hosts = ['localhost']
        mock_benchmark.args.timeseries_interval = 0.1

        mock_benchmark._start_timeseries_collection()

        assert isinstance(mock_benchmark._timeseries_collector, MultiHostTimeSeriesCollector)

        # Cleanup
        mock_benchmark._timeseries_collector.stop()

    def test_start_timeseries_singlehost_without_hosts(self, mock_benchmark):
        """Should use TimeSeriesCollector when no hosts provided."""
        from mlpstorage.cluster_collector import TimeSeriesCollector

        mock_benchmark.args.command = 'run'
        mock_benchmark.args.hosts = None
        mock_benchmark.args.timeseries_interval = 0.1

        mock_benchmark._start_timeseries_collection()

        assert isinstance(mock_benchmark._timeseries_collector, TimeSeriesCollector)

        # Cleanup
        mock_benchmark._timeseries_collector.stop()

    def test_stop_timeseries_creates_data(self, mock_benchmark):
        """_stop_timeseries_collection should create TimeSeriesData."""
        mock_benchmark.args.command = 'run'
        mock_benchmark.args.hosts = None
        mock_benchmark.args.timeseries_interval = 0.1

        mock_benchmark._start_timeseries_collection()
        time.sleep(0.25)
        mock_benchmark._stop_timeseries_collection()

        assert mock_benchmark._timeseries_data is not None
        assert mock_benchmark._timeseries_data.num_samples >= 1

    def test_stop_timeseries_multihost_creates_data(self, mock_benchmark):
        """_stop_timeseries_collection should create TimeSeriesData for multi-host."""
        mock_benchmark.args.command = 'run'
        mock_benchmark.args.hosts = ['localhost']
        mock_benchmark.args.timeseries_interval = 0.1

        mock_benchmark._start_timeseries_collection()
        time.sleep(0.25)
        mock_benchmark._stop_timeseries_collection()

        assert mock_benchmark._timeseries_data is not None
        assert 'localhost' in mock_benchmark._timeseries_data.hosts_collected

    def test_write_timeseries_creates_file(self, mock_benchmark, tmp_path):
        """write_timeseries_data should create JSON file."""
        from mlpstorage.rules.models import TimeSeriesData, TimeSeriesSample

        # Setup output path
        mock_benchmark.run_result_output = str(tmp_path)
        mock_benchmark.timeseries_file_path = str(tmp_path / 'test_timeseries.json')

        # Create test data
        sample = TimeSeriesSample(
            timestamp='2026-01-24T12:00:00Z',
            hostname='testhost',
            vmstat={'key': 123}
        )
        mock_benchmark._timeseries_data = TimeSeriesData(
            collection_interval_seconds=10.0,
            start_time='2026-01-24T12:00:00Z',
            end_time='2026-01-24T12:01:00Z',
            num_samples=1,
            samples_by_host={'testhost': [sample]},
            collection_method='local',
            hosts_requested=['testhost'],
            hosts_collected=['testhost']
        )

        mock_benchmark.write_timeseries_data()

        assert os.path.exists(mock_benchmark.timeseries_file_path)

        # Verify content
        with open(mock_benchmark.timeseries_file_path) as f:
            data = json.load(f)
        assert data['num_samples'] == 1
        assert 'testhost' in data['samples_by_host']

    def test_timeseries_file_follows_naming_convention(self, mock_benchmark, tmp_path):
        """Time-series file should follow {benchmark_type}_{datetime}_timeseries.json pattern (HOST-04)."""
        mock_benchmark.run_result_output = str(tmp_path)

        # Check filename follows pattern
        assert mock_benchmark.timeseries_filename.endswith('_timeseries.json')
        assert mock_benchmark.BENCHMARK_TYPE.value in mock_benchmark.timeseries_filename
        assert mock_benchmark.run_datetime in mock_benchmark.timeseries_filename

    def test_metadata_includes_timeseries_reference(self, mock_benchmark, tmp_path):
        """metadata property should include time-series data reference (HOST-04)."""
        from mlpstorage.rules.models import TimeSeriesData, TimeSeriesSample

        mock_benchmark.run_result_output = str(tmp_path)

        sample = TimeSeriesSample(
            timestamp='2026-01-24T12:00:00Z',
            hostname='testhost'
        )
        mock_benchmark._timeseries_data = TimeSeriesData(
            collection_interval_seconds=10.0,
            start_time='2026-01-24T12:00:00Z',
            end_time='2026-01-24T12:01:00Z',
            num_samples=5,
            samples_by_host={'testhost': [sample]},
            collection_method='local',
            hosts_requested=['testhost'],
            hosts_collected=['testhost']
        )

        metadata = mock_benchmark.metadata

        assert 'timeseries_data' in metadata
        assert metadata['timeseries_data']['num_samples'] == 5
        assert metadata['timeseries_data']['interval_seconds'] == 10.0
        assert metadata['timeseries_data']['file'] == mock_benchmark.timeseries_filename

    def test_run_integrates_timeseries_collection(self, mock_benchmark):
        """run() should start and stop time-series collection."""
        mock_benchmark.args.command = 'run'
        mock_benchmark.args.hosts = None
        mock_benchmark.args.timeseries_interval = 0.1
        mock_benchmark.args.skip_timeseries = False

        # Make _run return quickly
        mock_benchmark._run_result = 0

        def quick_run():
            time.sleep(0.2)
            return mock_benchmark._run_result

        mock_benchmark._run = quick_run

        result = mock_benchmark.run()

        assert result == 0
        # Time-series collection should have been performed
        assert mock_benchmark._timeseries_data is not None

    def test_timeseries_uses_background_thread(self, mock_benchmark):
        """Time-series collection should use background thread (HOST-05 architecture)."""
        mock_benchmark.args.command = 'run'
        mock_benchmark.args.hosts = None
        mock_benchmark.args.timeseries_interval = 0.1

        mock_benchmark._start_timeseries_collection()

        # Verify collector uses threading
        assert hasattr(mock_benchmark._timeseries_collector, '_thread')
        assert mock_benchmark._timeseries_collector._thread.name in ('TimeSeriesCollector', 'MultiHostTimeSeriesCollector')

        # Cleanup
        mock_benchmark._timeseries_collector.stop()
```

Add required import at top of test file:
```python
import time
import json
import os
```

Note: The mock_benchmark fixture may need adjustment. If it doesn't exist or doesn't have the needed attributes, add this fixture:

```python
@pytest.fixture
def mock_benchmark(tmp_path):
    """Create a mock benchmark instance for testing."""
    from argparse import Namespace
    from tests.fixtures.mock_logger import MockLogger

    # Minimal args for benchmark instantiation
    args = Namespace(
        debug=False,
        verbose=False,
        stream_log_level='INFO',
        what_if=False,
        results_dir=str(tmp_path),
        command='run',
        hosts=None,
        skip_timeseries=False,
        timeseries_interval=10.0,
        max_timeseries_samples=100,
        closed=False,
        allow_invalid_params=False,
    )

    # Create a concrete benchmark subclass for testing
    class TestBenchmark(Benchmark):
        BENCHMARK_TYPE = BENCHMARK_TYPES.training

        def _run(self):
            return 0

    benchmark = TestBenchmark(
        args=args,
        logger=MockLogger(),
        run_datetime='20260124_120000'
    )

    return benchmark
```
  </action>
  <verify>Run `pytest tests/unit/test_benchmark_base.py -v -k "timeseries or TimeSeriesCollection" --tb=short` - all tests should pass</verify>
  <done>All time-series benchmark integration unit tests pass, including HOST-04 naming convention and HOST-05 background thread architecture tests</done>
</task>

</tasks>

<verification>
1. CLI args check: `python -c "from mlpstorage.cli.common_args import add_timeseries_arguments, HELP_MESSAGES; print('timeseries_interval' in HELP_MESSAGES)"`
2. CLI wiring check: `grep -l "add_timeseries_arguments" mlpstorage/cli/*_args.py | wc -l` - should show 5
3. Benchmark integration check: `python -c "from mlpstorage.benchmarks.base import Benchmark; print(hasattr(Benchmark, '_start_timeseries_collection') and hasattr(Benchmark, '_stop_timeseries_collection') and hasattr(Benchmark, 'write_timeseries_data'))"`
4. Unit tests: `pytest tests/unit/test_benchmark_base.py -v -k "timeseries or TimeSeriesCollection" --tb=short`
5. Full test suite: `pytest tests/unit/test_cluster_collector.py tests/unit/test_benchmark_base.py -v --tb=short`
</verification>

<success_criteria>
1. CLI arguments --timeseries-interval, --skip-timeseries, --max-timeseries-samples are defined in common_args.py
2. CLI arguments are wired into training_args.py, checkpointing_args.py, vectordb_args.py, kvcache_args.py
3. Benchmark._should_collect_timeseries() correctly determines when to collect
4. Benchmark._start_timeseries_collection() creates appropriate collector type
5. Benchmark._stop_timeseries_collection() creates TimeSeriesData
6. Benchmark.write_timeseries_data() writes JSON file to results directory
7. Time-series file follows naming convention {benchmark_type}_{datetime}_timeseries.json (HOST-04)
8. Benchmark.metadata includes time-series data reference with file path (HOST-04)
9. Time-series collection uses background thread for minimal performance impact (HOST-05)
10. Benchmark.run() integrates time-series collection around _run()
11. All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-time-series-host-data-collection/07-03-SUMMARY.md`
</output>
