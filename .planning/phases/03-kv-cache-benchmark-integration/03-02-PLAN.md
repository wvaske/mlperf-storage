---
phase: 03-kv-cache-benchmark-integration
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - mlpstorage/benchmarks/kvcache.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "KV cache benchmark can execute via MPI across multiple hosts"
    - "MPI prefix is correctly generated when --hosts is provided"
    - "Local execution still works when --exec-type is not MPI"
    - "Cluster information is collected for distributed runs"
  artifacts:
    - path: "mlpstorage/benchmarks/kvcache.py"
      provides: "MPI-enabled KV cache benchmark execution"
      contains: "generate_mpi_prefix_cmd"
  key_links:
    - from: "mlpstorage/benchmarks/kvcache.py"
      to: "mlpstorage/utils.py"
      via: "import generate_mpi_prefix_cmd"
      pattern: "from mlpstorage.utils import.*generate_mpi_prefix_cmd"
    - from: "mlpstorage/benchmarks/kvcache.py._build_kvcache_command"
      to: "generate_mpi_prefix_cmd"
      via: "MPI command wrapping"
      pattern: "mpi_prefix = generate_mpi_prefix_cmd"
---

<objective>
Add MPI execution support to KVCacheBenchmark class.

Purpose: Enable the KV cache benchmark to run across multiple hosts using MPI, following the pattern established by DLIOBenchmark. This allows users to simulate distributed KV cache workloads.

Output: Updated KVCacheBenchmark class that wraps kv-cache.py commands with MPI prefix when distributed execution is requested.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-kv-cache-benchmark-integration/03-RESEARCH.md
@.planning/phases/03-kv-cache-benchmark-integration/03-01-SUMMARY.md

# Reference for MPI patterns:
@mlpstorage/benchmarks/dlio.py
@mlpstorage/utils.py
@mlpstorage/benchmarks/kvcache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add MPI command wrapping to KVCacheBenchmark</name>
  <files>mlpstorage/benchmarks/kvcache.py</files>
  <action>
  Modify mlpstorage/benchmarks/kvcache.py to add MPI execution support:

  1. Add imports at top:
     ```python
     from mlpstorage.config import EXEC_TYPE
     from mlpstorage.utils import generate_mpi_prefix_cmd
     ```

  2. In __init__, store num_processes:
     ```python
     self.num_processes = getattr(args, 'num_processes', None)
     ```

  3. Modify `_build_kvcache_command()` to wrap with MPI prefix when distributed execution is requested:

     At the END of the method, before `return " ".join(cmd_parts)`, add:
     ```python
     # Build the base command
     cmd = " ".join(cmd_parts)

     # Add MPI wrapper if distributed execution requested
     exec_type = getattr(self.args, 'exec_type', None)
     if exec_type == EXEC_TYPE.MPI:
         hosts = getattr(self.args, 'hosts', None)
         if hosts and len(hosts) > 0:
             # Default num_processes to number of hosts if not specified
             num_procs = self.num_processes or len(hosts)
             mpi_prefix = generate_mpi_prefix_cmd(
                 mpi_cmd=getattr(self.args, 'mpi_bin', 'mpirun'),
                 hosts=hosts,
                 num_processes=num_procs,
                 oversubscribe=getattr(self.args, 'oversubscribe', False),
                 allow_run_as_root=getattr(self.args, 'allow_run_as_root', False),
                 params=getattr(self.args, 'mpi_params', None),
                 logger=self.logger
             )
             cmd = f"{mpi_prefix} {cmd}"

     return cmd
     ```

  4. Add cluster info collection for distributed runs. In __init__, after calling super().__init__():
     ```python
     # Collect cluster information for distributed runs
     if getattr(args, 'command', '') == 'run':
         self.cluster_information = self._collect_cluster_information()
     ```

  The pattern follows DLIOBenchmark.generate_dlio_command() lines 171-178.
  </action>
  <verify>
  Run Python to verify the command generation:
  ```python
  from argparse import Namespace
  from mlpstorage.benchmarks.kvcache import KVCacheBenchmark
  from mlpstorage.config import EXEC_TYPE
  import logging

  logger = logging.getLogger()
  args = Namespace(
      command='run',
      model='llama3.1-8b',
      num_users=100,
      duration=60,
      gpu_mem_gb=16.0,
      cpu_mem_gb=32.0,
      cache_dir=None,
      generation_mode='realistic',
      performance_profile='latency',
      exec_type=EXEC_TYPE.MPI,
      hosts=['host1', 'host2'],
      num_processes=2,
      mpi_bin='mpirun',
      oversubscribe=False,
      allow_run_as_root=False,
      mpi_params=None,
      kvcache_bin_path=None,
      disable_multi_turn=False,
      disable_prefix_caching=False,
      enable_rag=False,
      enable_autoscaling=False,
      seed=None,
      results_dir='/tmp/results',
      closed=False,
      what_if=True,
      debug=False,
      verbose=False,
      stream_log_level='INFO',
  )
  bm = KVCacheBenchmark(args, logger=logger)
  cmd = bm._build_kvcache_command()
  print("Command:", cmd)
  assert 'mpirun' in cmd
  assert 'host1' in cmd
  print("OK: MPI prefix present")
  ```
  </verify>
  <done>
  - _build_kvcache_command() returns MPI-wrapped command when exec_type=MPI and hosts specified
  - Command includes correct host list and process count
  - Local execution still works when exec_type is not MPI
  - num_processes defaults to len(hosts) if not specified
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for MPI execution in KVCacheBenchmark</name>
  <files>tests/unit/test_benchmarks_kvcache.py</files>
  <action>
  Create or update tests/unit/test_benchmarks_kvcache.py with tests for MPI command generation:

  ```python
  """Tests for KVCacheBenchmark MPI execution."""
  import pytest
  from argparse import Namespace
  from unittest.mock import MagicMock, patch
  from mlpstorage.benchmarks.kvcache import KVCacheBenchmark
  from mlpstorage.config import EXEC_TYPE


  @pytest.fixture
  def base_args():
      """Base arguments for KVCacheBenchmark."""
      return Namespace(
          command='run',
          model='llama3.1-8b',
          num_users=100,
          duration=60,
          gpu_mem_gb=16.0,
          cpu_mem_gb=32.0,
          cache_dir='/tmp/cache',
          generation_mode='realistic',
          performance_profile='latency',
          kvcache_bin_path=None,
          disable_multi_turn=False,
          disable_prefix_caching=False,
          enable_rag=False,
          enable_autoscaling=False,
          seed=None,
          results_dir='/tmp/results',
          closed=False,
          what_if=True,
          debug=False,
          verbose=False,
          stream_log_level='INFO',
      )


  @pytest.fixture
  def mock_logger():
      """Create a mock logger."""
      logger = MagicMock()
      logger.status = MagicMock()
      logger.info = MagicMock()
      logger.debug = MagicMock()
      logger.warning = MagicMock()
      logger.verboser = MagicMock()
      return logger


  class TestKVCacheMPIExecution:
      """Test MPI execution in KVCacheBenchmark."""

      def test_local_execution_no_mpi_prefix(self, base_args, mock_logger):
          """Verify local execution doesn't add MPI prefix."""
          base_args.exec_type = None
          base_args.hosts = None
          base_args.num_processes = None
          base_args.mpi_bin = 'mpirun'
          base_args.oversubscribe = False
          base_args.allow_run_as_root = False
          base_args.mpi_params = None

          with patch.object(KVCacheBenchmark, '_collect_cluster_information', return_value=None):
              bm = KVCacheBenchmark(base_args, logger=mock_logger)
              cmd = bm._build_kvcache_command()

          assert 'mpirun' not in cmd
          assert 'python' in cmd.lower() or 'kv-cache.py' in cmd

      def test_mpi_execution_adds_prefix(self, base_args, mock_logger):
          """Verify MPI execution adds mpirun prefix."""
          base_args.exec_type = EXEC_TYPE.MPI
          base_args.hosts = ['host1', 'host2']
          base_args.num_processes = 2
          base_args.mpi_bin = 'mpirun'
          base_args.oversubscribe = False
          base_args.allow_run_as_root = False
          base_args.mpi_params = None

          with patch.object(KVCacheBenchmark, '_collect_cluster_information', return_value=None):
              bm = KVCacheBenchmark(base_args, logger=mock_logger)
              cmd = bm._build_kvcache_command()

          assert 'mpirun' in cmd
          assert 'host1' in cmd
          assert 'host2' in cmd
          assert '-n 2' in cmd

      def test_mpi_with_mpiexec(self, base_args, mock_logger):
          """Verify mpiexec is used when specified."""
          base_args.exec_type = EXEC_TYPE.MPI
          base_args.hosts = ['host1']
          base_args.num_processes = 1
          base_args.mpi_bin = 'mpiexec'
          base_args.oversubscribe = False
          base_args.allow_run_as_root = False
          base_args.mpi_params = None

          with patch.object(KVCacheBenchmark, '_collect_cluster_information', return_value=None):
              bm = KVCacheBenchmark(base_args, logger=mock_logger)
              cmd = bm._build_kvcache_command()

          assert 'mpiexec' in cmd

      def test_num_processes_defaults_to_host_count(self, base_args, mock_logger):
          """Verify num_processes defaults to number of hosts."""
          base_args.exec_type = EXEC_TYPE.MPI
          base_args.hosts = ['host1', 'host2', 'host3']
          base_args.num_processes = None  # Not specified
          base_args.mpi_bin = 'mpirun'
          base_args.oversubscribe = False
          base_args.allow_run_as_root = False
          base_args.mpi_params = None

          with patch.object(KVCacheBenchmark, '_collect_cluster_information', return_value=None):
              bm = KVCacheBenchmark(base_args, logger=mock_logger)
              cmd = bm._build_kvcache_command()

          assert '-n 3' in cmd  # Should default to 3 hosts

      def test_mpi_flags_passed(self, base_args, mock_logger):
          """Verify MPI flags are included in command."""
          base_args.exec_type = EXEC_TYPE.MPI
          base_args.hosts = ['host1']
          base_args.num_processes = 4
          base_args.mpi_bin = 'mpirun'
          base_args.oversubscribe = True
          base_args.allow_run_as_root = True
          base_args.mpi_params = None

          with patch.object(KVCacheBenchmark, '_collect_cluster_information', return_value=None):
              bm = KVCacheBenchmark(base_args, logger=mock_logger)
              cmd = bm._build_kvcache_command()

          assert '--oversubscribe' in cmd
          assert '--allow-run-as-root' in cmd

      def test_exec_type_mpi_but_no_hosts(self, base_args, mock_logger):
          """Verify no MPI prefix when exec_type=MPI but no hosts."""
          base_args.exec_type = EXEC_TYPE.MPI
          base_args.hosts = None
          base_args.num_processes = None
          base_args.mpi_bin = 'mpirun'
          base_args.oversubscribe = False
          base_args.allow_run_as_root = False
          base_args.mpi_params = None

          with patch.object(KVCacheBenchmark, '_collect_cluster_information', return_value=None):
              bm = KVCacheBenchmark(base_args, logger=mock_logger)
              cmd = bm._build_kvcache_command()

          # Should not have MPI prefix when no hosts specified
          assert 'mpirun -n' not in cmd
  ```
  </action>
  <verify>
  Run: `pytest tests/unit/test_benchmarks_kvcache.py -v`
  All tests should pass.
  </verify>
  <done>
  - 7 tests pass covering MPI execution scenarios
  - Tests verify MPI prefix is added correctly
  - Tests verify local execution works without MPI
  - Tests verify num_processes defaults correctly
  - Tests verify MPI flags are passed through
  </done>
</task>

</tasks>

<verification>
1. KVCacheBenchmark._build_kvcache_command() generates MPI-wrapped command when:
   - exec_type == EXEC_TYPE.MPI
   - hosts list is provided and non-empty

2. MPI command includes:
   - Correct mpi_bin (mpirun or mpiexec)
   - Host list
   - Process count (-n)
   - Optional flags (--oversubscribe, --allow-run-as-root)

3. Local execution still works when exec_type is not MPI

4. All unit tests pass: `pytest tests/unit/test_benchmarks_kvcache.py -v`
</verification>

<success_criteria>
- MPI prefix is generated correctly for distributed KV cache execution
- Local execution continues to work without regression
- num_processes defaults to len(hosts) when not specified
- Cluster information is collected for distributed runs
- Unit tests verify all MPI execution scenarios
</success_criteria>

<output>
After completion, create `.planning/phases/03-kv-cache-benchmark-integration/03-02-SUMMARY.md`
</output>
