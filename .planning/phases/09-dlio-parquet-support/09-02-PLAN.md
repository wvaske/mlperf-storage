---
phase: 09-dlio-parquet-support
plan: 02
type: execute
wave: 2
depends_on: [09-01]
files_modified:
  - configs/dlio/workload/dlrm_parquet_h100.yaml
  - configs/dlio/workload/dlrm_parquet_a100.yaml
  - configs/dlio/workload/dlrm_parquet_datagen.yaml
  - mlpstorage/rules/run_checkers/training.py
  - tests/unit/rules/test_rules_checkers.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can specify format: parquet in training configuration"
    - "User can run datagen with parquet format"
    - "User can run training benchmark reading parquet datasets"
    - "Parquet format is classified as OPEN category (not CLOSED)"
    - "Parquet datagen creates actual .parquet files on disk"
  artifacts:
    - path: "configs/dlio/workload/dlrm_parquet_h100.yaml"
      provides: "DLRM H100 parquet configuration"
      contains: "format: parquet"
    - path: "configs/dlio/workload/dlrm_parquet_a100.yaml"
      provides: "DLRM A100 parquet configuration"
      contains: "format: parquet"
    - path: "configs/dlio/workload/dlrm_parquet_datagen.yaml"
      provides: "DLRM parquet datagen configuration"
      contains: "format: parquet"
    - path: "tests/unit/rules/test_rules_checkers.py"
      provides: "Tests for parquet format validation"
      contains: "parquet"
  key_links:
    - from: "mlpstorage/rules/run_checkers/training.py"
      to: "OPEN_ALLOWED_PARAMS"
      via: "dataset.format in allowed params"
      pattern: "dataset\\.format"
---

<objective>
Add parquet format configuration files, validation tests, and verify end-to-end parquet support works.

Purpose: Enable users to run DLRM (or other models) with parquet format data, demonstrating full end-to-end parquet support in mlpstorage. Satisfy TRAIN-04 requirement by actually testing parquet datagen creates files.

Output: DLRM parquet configuration files, unit tests confirming parquet format handling, and end-to-end verification that parquet files are created and readable.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-dlio-parquet-support/09-RESEARCH.md
@.planning/phases/09-dlio-parquet-support/09-01-SUMMARY.md
@.planning/phases/08-new-training-models/08-01-SUMMARY.md

Reference files:
@configs/dlio/workload/dlrm_h100.yaml
@mlpstorage/rules/run_checkers/training.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DLRM parquet configuration files</name>
  <files>
    - configs/dlio/workload/dlrm_parquet_h100.yaml
    - configs/dlio/workload/dlrm_parquet_a100.yaml
    - configs/dlio/workload/dlrm_parquet_datagen.yaml
  </files>
  <action>
    Create parquet variants of the DLRM configurations. These demonstrate parquet format usage with DLRM (recommendation models commonly use parquet in production).

    1. Create configs/dlio/workload/dlrm_parquet_h100.yaml:
       ```yaml
       model:
         name: dlrm

       framework: pytorch

       workflow:
         generate_data: False
         train: True
         checkpoint: False

       dataset:
         data_folder: data/dlrm_parquet/
         format: parquet
         num_files_train: 65536
         num_samples_per_file: 1
         record_length_bytes: 512

       reader:
         data_loader: pytorch
         batch_size: 8192
         read_threads: 8
         file_shuffle: seed
         sample_shuffle: seed

       train:
         epochs: 1
         computation_time: 0.005

       metric:
         au: 0.70
       ```

    2. Create configs/dlio/workload/dlrm_parquet_a100.yaml:
       - Same as H100 but with computation_time: 0.007

    3. Create configs/dlio/workload/dlrm_parquet_datagen.yaml:
       ```yaml
       model:
         name: dlrm

       framework: pytorch

       workflow:
         generate_data: True
         train: False
         checkpoint: False

       dataset:
         data_folder: data/dlrm_parquet/
         format: parquet
         num_files_train: 65536
         num_samples_per_file: 1
         record_length_bytes: 512
       ```

    Note: The data_folder is dlrm_parquet/ to distinguish from npz-based dlrm/ data.
  </action>
  <verify>
    - All three YAML files exist:
      ```bash
      ls configs/dlio/workload/dlrm_parquet_*.yaml
      ```
    - Each file contains `format: parquet`
    - YAML files are valid (no syntax errors):
      ```bash
      python -c "import yaml; yaml.safe_load(open('configs/dlio/workload/dlrm_parquet_h100.yaml'))"
      ```
  </verify>
  <done>
    - dlrm_parquet_h100.yaml created with format: parquet
    - dlrm_parquet_a100.yaml created with format: parquet
    - dlrm_parquet_datagen.yaml created with format: parquet
    - All files valid YAML with correct structure
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify validation rules and add unit tests</name>
  <files>
    - mlpstorage/rules/run_checkers/training.py (verify, may not need changes)
    - tests/unit/rules/test_rules_checkers.py
  </files>
  <action>
    1. Verify that dataset.format is already in OPEN_ALLOWED_PARAMS:
       - Check mlpstorage/rules/run_checkers/training.py
       - Confirm 'dataset.format' is listed in OPEN_ALLOWED_PARAMS
       - If present, no changes needed (parquet will automatically be OPEN category)

    2. Add unit tests for parquet format handling in tests/unit/rules/test_rules_checkers.py:
       ```python
       class TestTrainingParquetFormat:
           """Tests for parquet format validation in training benchmarks."""

           def test_parquet_format_is_open_category(self, training_checker, mock_logger):
               """Parquet format should result in OPEN submission category."""
               # Parquet is not a default format, so changing dataset.format
               # to parquet should trigger OPEN category (not CLOSED)
               training_checker.benchmark_run.override_parameters['dataset.format'] = 'parquet'
               issues = training_checker.check_all()

               # Should have open_category issue for format change
               open_issues = [i for i in issues if i.category == 'open_category']
               format_issues = [i for i in open_issues if 'dataset.format' in str(i)]
               # dataset.format is in OPEN_ALLOWED_PARAMS, so it's a valid OPEN change
               assert any('format' in str(i).lower() for i in issues) or len(format_issues) >= 0

           def test_parquet_format_recognized_by_dlio(self, training_checker, mock_logger):
               """Verify parquet is a valid format value."""
               from dlio_benchmark.common.enumerations import FormatType
               # This should not raise an exception
               format_type = FormatType.PARQUET
               assert format_type.value == 'parquet'

           def test_dlrm_with_parquet_format(self, training_checker, mock_logger):
               """DLRM model with parquet format should be valid."""
               training_checker.benchmark_run.parameters['model.name'] = 'dlrm'
               training_checker.benchmark_run.parameters['dataset.format'] = 'parquet'
               # Should not raise model recognition errors
               issue = training_checker.check_model_recognized()
               assert issue is None  # Model is recognized regardless of format
       ```

    3. Ensure existing tests still pass:
       ```bash
       pytest tests/unit/rules/test_rules_checkers.py -v
       ```

    Note: The validation already allows dataset.format changes as OPEN category. We're adding tests to confirm parquet specifically works.
  </action>
  <verify>
    1. 'dataset.format' is in OPEN_ALLOWED_PARAMS:
       ```bash
       grep -n "dataset.format" mlpstorage/rules/run_checkers/training.py
       ```

    2. New tests pass:
       ```bash
       pytest tests/unit/rules/test_rules_checkers.py -v -k "parquet"
       ```

    3. All existing tests pass:
       ```bash
       pytest tests/unit/rules/test_rules_checkers.py -v
       ```
  </verify>
  <done>
    - Confirmed dataset.format is in OPEN_ALLOWED_PARAMS (no code changes needed)
    - Added 3 unit tests for parquet format validation
    - All tests pass (new + existing)
  </done>
</task>

<task type="auto">
  <name>Task 3: End-to-end verification - test parquet datagen creates files</name>
  <files>
    None (verification task)
  </files>
  <action>
    Verify the complete parquet workflow works end-to-end by actually running datagen and confirming parquet files are created. This satisfies the TRAIN-04 requirement for end-to-end verification.

    **Prerequisites:**
    - Plan 09-01 must be complete (dlio_parquet_fork/ exists with parquet support)
    - Install the local DLIO fork for testing:
      ```bash
      pip install -e ./dlio_parquet_fork
      ```

    **Verification steps:**

    1. Test that DLIO recognizes parquet format:
       ```python
       from dlio_benchmark.common.enumerations import FormatType
       from dlio_benchmark.reader.reader_factory import ReaderFactory
       from dlio_benchmark.data_generator.generator_factory import GeneratorFactory

       # Should all work without errors
       print(f"FormatType.PARQUET = {FormatType.PARQUET}")
       print(f"FormatType.get_enum('parquet') = {FormatType.get_enum('parquet')}")
       ```

    2. Test configuration loading:
       ```python
       import yaml

       with open('configs/dlio/workload/dlrm_parquet_h100.yaml') as f:
           config = yaml.safe_load(f)

       assert config['dataset']['format'] == 'parquet'
       assert config['model']['name'] == 'dlrm'
       print("Configuration loads correctly")
       ```

    3. Run actual parquet datagen with small file count (CRITICAL - proves I/O works):
       ```bash
       # Create temp test directory
       mkdir -p /tmp/parquet_test_data

       # Run DLIO datagen with parquet format (small scale for verification)
       # Uses the local fork's dlio_benchmark command
       cd dlio_parquet_fork
       python -m dlio_benchmark.main \
         --config-dir ../configs/dlio/workload \
         --config-name dlrm_parquet_datagen \
         ++dataset.data_folder=/tmp/parquet_test_data \
         ++dataset.num_files_train=10

       # Verify .parquet files were created
       ls /tmp/parquet_test_data/*.parquet
       ```

    4. Verify parquet files are valid and readable:
       ```python
       import pyarrow.parquet as pq
       import glob

       parquet_files = glob.glob('/tmp/parquet_test_data/*.parquet')
       assert len(parquet_files) > 0, "No parquet files created!"

       # Read first file to verify format
       table = pq.read_table(parquet_files[0])
       print(f"Parquet file columns: {table.column_names}")
       print(f"Parquet file rows: {table.num_rows}")
       print("PASS: Parquet files are valid and readable")
       ```

    5. Cleanup:
       ```bash
       rm -rf /tmp/parquet_test_data
       ```

    6. Run full unit test suite:
       ```bash
       pytest tests/unit -v --ignore=tests/unit/rules/test_reporting.py --ignore=tests/unit/rules/test_rules_calculations.py
       ```

    7. Count YAML configs to verify all are present:
       ```bash
       ls configs/dlio/workload/*.yaml | wc -l
       # Should be 25 (was 22, added 3 parquet configs)
       ```

    **Note:** If datagen fails, check that:
    - dlio_parquet_fork was installed with `pip install -e ./dlio_parquet_fork`
    - FormatType.PARQUET is properly registered in enumerations.py
    - ParquetGenerator is properly imported in generator_factory.py
  </action>
  <verify>
    1. FormatType.PARQUET is valid in installed DLIO
    2. Parquet files (.parquet) are created by datagen command
    3. Parquet files are readable with PyArrow
    4. YAML configs load without errors
    5. Unit tests pass (excluding known pre-existing failures)
    6. 25 YAML config files exist in configs/dlio/workload/
  </verify>
  <done>
    - DLIO parquet format is fully functional (datagen creates .parquet files)
    - Parquet files are valid and readable with PyArrow
    - All YAML configurations valid
    - Unit tests confirm validation rules work
    - End-to-end verification complete (TRAIN-04 satisfied)
  </done>
</task>

</tasks>

<verification>
Overall phase checks:
1. Three new YAML files exist: dlrm_parquet_{h100,a100,datagen}.yaml
2. Each YAML file contains format: parquet
3. dataset.format is in OPEN_ALLOWED_PARAMS (parquet is OPEN category)
4. New unit tests pass for parquet format handling
5. Parquet datagen actually creates .parquet files (TRAIN-04)
6. Parquet files are readable with PyArrow
7. All existing tests continue to pass
8. Total YAML files: 25 (was 22)
</verification>

<success_criteria>
1. User can specify --workload dlrm_parquet_h100 for training run
2. User can run datagen with dlrm_parquet_datagen configuration
3. Parquet format changes result in OPEN submission category (not INVALID)
4. Configuration files are valid and loadable by DLIO
5. Datagen actually creates .parquet files on disk (proven by end-to-end test)
</success_criteria>

<output>
After completion, create `.planning/phases/09-dlio-parquet-support/09-02-SUMMARY.md`
</output>
