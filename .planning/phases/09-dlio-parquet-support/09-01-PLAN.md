---
phase: 09-dlio-parquet-support
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - dlio_parquet_fork/dlio_benchmark/common/enumerations.py
  - dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py
  - dlio_parquet_fork/dlio_benchmark/reader/reader_factory.py
  - dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py
  - dlio_parquet_fork/dlio_benchmark/data_generator/generator_factory.py
autonomous: true
user_setup:
  - service: github
    why: "DLIO fork must be pushed to remote for pyproject.toml to reference"
    env_vars: []
    dashboard_config:
      - task: "Fork argonne-lcf/dlio_benchmark to your GitHub account"
        location: "https://github.com/argonne-lcf/dlio_benchmark -> Fork"
      - task: "Push parquet-support branch to your fork"
        location: "git remote add origin <your-fork-url> && git push -u origin parquet-support"
      - task: "Update pyproject.toml with your fork URL"
        location: "pyproject.toml line 23/34: change git URL to your fork"

must_haves:
  truths:
    - "DLIO benchmark accepts format: parquet in configuration"
    - "DLIO can generate synthetic parquet files for training"
    - "DLIO can read parquet files during training benchmark"
  artifacts:
    - path: "dlio_parquet_fork/dlio_benchmark/common/enumerations.py"
      provides: "PARQUET in FormatType enum"
      contains: "PARQUET = 'parquet'"
    - path: "dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py"
      provides: "ParquetReader class"
      exports: ["ParquetReader"]
    - path: "dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py"
      provides: "ParquetGenerator class"
      exports: ["ParquetGenerator"]
  key_links:
    - from: "dlio_parquet_fork/dlio_benchmark/reader/reader_factory.py"
      to: "dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py"
      via: "FormatType.PARQUET case"
      pattern: "FormatType\\.PARQUET"
    - from: "dlio_parquet_fork/dlio_benchmark/data_generator/generator_factory.py"
      to: "dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py"
      via: "FormatType.PARQUET case"
      pattern: "FormatType\\.PARQUET"
---

<objective>
Add Apache Parquet format support to DLIO benchmark by creating a local fork with ParquetReader and ParquetGenerator implementations.

Purpose: Enable training benchmarks to use parquet format for data, particularly useful for recommendation models like DLRM that commonly use Parquet format for training data.

Output: A local DLIO fork (dlio_parquet_fork/) with full parquet support (reader + generator + factory registration). User will push to remote after verification.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-dlio-parquet-support/09-RESEARCH.md

Reference files (in installed DLIO):
- .venv/lib/python3.12/site-packages/dlio_benchmark/common/enumerations.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/reader/csv_reader.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/reader/reader_factory.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/data_generator/csv_generator.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/data_generator/generator_factory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Clone DLIO and add PARQUET to FormatType enum</name>
  <files>
    - dlio_parquet_fork/dlio_benchmark/common/enumerations.py
  </files>
  <action>
    1. Clone DLIO benchmark to local directory for modification:
       ```bash
       git clone --branch mlperf_storage_v2.0 https://github.com/argonne-lcf/dlio_benchmark.git dlio_parquet_fork
       cd dlio_parquet_fork
       git checkout -b parquet-support
       ```

    2. Modify dlio_parquet_fork/dlio_benchmark/common/enumerations.py:
       - Add PARQUET = 'parquet' to FormatType enum (after PNG line ~26)
       - Add get_enum case for PARQUET in FormatType.get_enum() static method (around line 41):
         ```python
         elif _format == "parquet":
             return FormatType.PARQUET
         ```

    3. Modify Compression enum in same file (if SNAPPY not present):
       - Add SNAPPY = 'snappy' to Compression enum
       - This is the default compression for parquet

    Note: All work is in local dlio_parquet_fork/ directory. User will push to remote after verification (see user_setup).
  </action>
  <verify>
    ```bash
    cd dlio_parquet_fork
    python -c "
    import sys
    sys.path.insert(0, '.')
    from dlio_benchmark.common.enumerations import FormatType
    print(f'FormatType.PARQUET = {FormatType.PARQUET}')
    print(f'get_enum parquet = {FormatType.get_enum(\"parquet\")}')
    assert FormatType.PARQUET.value == 'parquet'
    assert FormatType.get_enum('parquet') == FormatType.PARQUET
    print('PASS: PARQUET enum verified')
    "
    ```
  </verify>
  <done>
    - PARQUET is a valid FormatType enum value in dlio_parquet_fork
    - FormatType.get_enum('parquet') works correctly
    - Local git branch parquet-support exists with changes
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ParquetReader and ParquetGenerator classes</name>
  <files>
    - dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py
    - dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py
  </files>
  <action>
    1. Create dlio_parquet_fork/dlio_benchmark/reader/parquet_reader.py following csv_reader.py pattern:
       ```python
       import pyarrow.parquet as pq
       import numpy as np
       from dlio_benchmark.common.constants import MODULE_DATA_READER
       from dlio_benchmark.utils.utility import Profile
       from dlio_benchmark.reader.reader_handler import FormatReader

       dlp = Profile(MODULE_DATA_READER)

       class ParquetReader(FormatReader):
           """Parquet Reader implementation for DLIO."""

           @dlp.log_init
           def __init__(self, dataset_type, thread_index, epoch):
               super().__init__(dataset_type, thread_index)

           @dlp.log
           def open(self, filename):
               super().open(filename)
               table = pq.read_table(filename)
               # Convert Arrow table to numpy array (like CSV reader does)
               return table.to_pandas().to_numpy()

           @dlp.log
           def close(self, filename):
               super().close(filename)

           @dlp.log
           def get_sample(self, filename, sample_index):
               super().get_sample(filename, sample_index)
               image = self.open_file_map[filename][sample_index]
               dlp.update(image_size=image.nbytes)

           def next(self):
               for batch in super().next():
                   yield batch

           @dlp.log
           def read_index(self, image_idx, step):
               return super().read_index(image_idx, step)

           @dlp.log
           def finalize(self):
               return super().finalize()

           def is_index_based(self):
               return True

           def is_iterator_based(self):
               return True
       ```

    2. Create dlio_parquet_fork/dlio_benchmark/data_generator/parquet_generator.py following csv_generator.py pattern:
       ```python
       import numpy as np
       import pyarrow as pa
       import pyarrow.parquet as pq
       from dlio_benchmark.common.enumerations import Compression
       from dlio_benchmark.data_generator.data_generator import DataGenerator
       from dlio_benchmark.utils.utility import progress

       class ParquetGenerator(DataGenerator):
           """Parquet data generator for DLIO."""

           def __init__(self):
               super().__init__()

           def generate(self):
               """Generate synthetic parquet files for training."""
               super().generate()
               np.random.seed(10)
               dim = self.get_dimension(self.total_files_to_generate)

               for i in range(self.my_rank, int(self.total_files_to_generate), self.comm_size):
                   progress(i + 1, self.total_files_to_generate, "Generating Parquet Data")
                   dim1 = dim[2 * i]
                   dim2 = dim[2 * i + 1]

                   # Generate random data as numpy array
                   records = [np.random.randint(255, size=dim1 * dim2, dtype=np.uint8)
                             for _ in range(self.num_samples)]

                   # Convert to Arrow table
                   table = pa.table({'data': [rec.tolist() for rec in records]})

                   out_path_spec = self.storage.get_uri(self._file_list[i])

                   # Map DLIO compression to parquet compression
                   compression = 'snappy'  # default
                   if self.compression == Compression.GZIP:
                       compression = 'gzip'
                   elif self.compression == Compression.NONE:
                       compression = None

                   pq.write_table(table, out_path_spec, compression=compression)

               np.random.seed()
       ```

    IMPORTANT: PyArrow file handles are NOT fork-safe. The reader opens files lazily in open() which is called after worker spawning, so this is safe. Do NOT open files in __init__().
  </action>
  <verify>
    ```bash
    cd dlio_parquet_fork
    python -c "
    import sys
    sys.path.insert(0, '.')
    from dlio_benchmark.reader.parquet_reader import ParquetReader
    from dlio_benchmark.data_generator.parquet_generator import ParquetGenerator
    print('ParquetReader imported successfully')
    print('ParquetGenerator imported successfully')
    print('PASS: Both classes import without error')
    "
    ```
  </verify>
  <done>
    - ParquetReader class extends FormatReader with open/close/get_sample methods
    - ParquetGenerator class extends DataGenerator with generate method
    - Both use PyArrow for parquet I/O operations
    - Both files exist in dlio_parquet_fork/
  </done>
</task>

<task type="auto">
  <name>Task 3: Register parquet format in factories and commit changes</name>
  <files>
    - dlio_parquet_fork/dlio_benchmark/reader/reader_factory.py
    - dlio_parquet_fork/dlio_benchmark/data_generator/generator_factory.py
  </files>
  <action>
    1. Modify dlio_parquet_fork/dlio_benchmark/reader/reader_factory.py:
       - Add case for FormatType.PARQUET after the CSV case (around line 55):
         ```python
         elif type == FormatType.PARQUET:
             if _args.odirect == True:
                 raise Exception("O_DIRECT for %s format is not yet supported." % type)
             else:
                 from dlio_benchmark.reader.parquet_reader import ParquetReader
                 return ParquetReader(dataset_type, thread_index, epoch_number)
         ```

    2. Modify dlio_parquet_fork/dlio_benchmark/data_generator/generator_factory.py:
       - Add case for FormatType.PARQUET after CSV case (around line 38):
         ```python
         elif type == FormatType.PARQUET:
             from dlio_benchmark.data_generator.parquet_generator import ParquetGenerator
             return ParquetGenerator()
         ```

    3. Commit all DLIO changes locally:
       ```bash
       cd dlio_parquet_fork
       git add -A
       git commit -m "feat: add parquet format support

       - Add PARQUET to FormatType enum
       - Create ParquetReader using PyArrow
       - Create ParquetGenerator using PyArrow
       - Register parquet in reader and generator factories
       - Default compression: snappy"
       ```

    NOTE: Do NOT push to remote yet. User will push after end-to-end verification (Plan 09-03).
    NOTE: pyproject.toml update happens AFTER user pushes fork to remote (see user_setup).
  </action>
  <verify>
    ```bash
    cd dlio_parquet_fork
    # Verify factories can instantiate parquet handlers
    python -c "
    import sys
    sys.path.insert(0, '.')
    from dlio_benchmark.common.enumerations import FormatType
    print(f'FormatType.PARQUET = {FormatType.PARQUET}')

    # Check reader_factory has PARQUET case
    with open('dlio_benchmark/reader/reader_factory.py') as f:
        content = f.read()
        assert 'FormatType.PARQUET' in content, 'PARQUET not in reader_factory'
    print('reader_factory.py: PARQUET case present')

    # Check generator_factory has PARQUET case
    with open('dlio_benchmark/data_generator/generator_factory.py') as f:
        content = f.read()
        assert 'FormatType.PARQUET' in content, 'PARQUET not in generator_factory'
    print('generator_factory.py: PARQUET case present')

    print('PASS: Factories updated for parquet support')
    "

    # Verify git commit exists
    git log --oneline -1 | grep -q "parquet" && echo "PASS: Git commit created"
    ```
  </verify>
  <done>
    - ReaderFactory returns ParquetReader for FormatType.PARQUET
    - GeneratorFactory returns ParquetGenerator for FormatType.PARQUET
    - All changes committed locally on parquet-support branch
    - Ready for user to push to remote after end-to-end verification
  </done>
</task>

</tasks>

<verification>
Overall phase checks:
1. FormatType.PARQUET exists and is recognized
2. ParquetReader can be instantiated by ReaderFactory
3. ParquetGenerator can be instantiated by GeneratorFactory
4. All changes committed locally in dlio_parquet_fork/
5. Local DLIO fork is functional (imports work)
</verification>

<success_criteria>
1. DLIO fork (dlio_parquet_fork/) has parquet format support (reader + generator)
2. format: parquet is a valid option in DLIO configuration
3. All code committed locally on parquet-support branch
4. User can push fork and update pyproject.toml when ready (documented in user_setup)
</success_criteria>

<output>
After completion, create `.planning/phases/09-dlio-parquet-support/09-01-SUMMARY.md`
</output>
