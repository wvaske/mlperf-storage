---
phase: 09-dlio-parquet-support
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - dlio_benchmark/common/enumerations.py (in fork)
  - dlio_benchmark/reader/parquet_reader.py (in fork)
  - dlio_benchmark/reader/reader_factory.py (in fork)
  - dlio_benchmark/data_generator/parquet_generator.py (in fork)
  - dlio_benchmark/data_generator/generator_factory.py (in fork)
autonomous: true
user_setup: []

must_haves:
  truths:
    - "DLIO benchmark accepts format: parquet in configuration"
    - "DLIO can generate synthetic parquet files for training"
    - "DLIO can read parquet files during training benchmark"
  artifacts:
    - path: "pyproject.toml"
      provides: "DLIO fork reference for parquet support"
      contains: "dlio-benchmark @ git+https://github.com/mlcommons/dlio_benchmark.git@parquet-support"
    - path: "dlio_benchmark/common/enumerations.py"
      provides: "PARQUET in FormatType enum"
      contains: "PARQUET = 'parquet'"
    - path: "dlio_benchmark/reader/parquet_reader.py"
      provides: "ParquetReader class"
      exports: ["ParquetReader"]
    - path: "dlio_benchmark/data_generator/parquet_generator.py"
      provides: "ParquetGenerator class"
      exports: ["ParquetGenerator"]
  key_links:
    - from: "dlio_benchmark/reader/reader_factory.py"
      to: "dlio_benchmark/reader/parquet_reader.py"
      via: "FormatType.PARQUET case"
      pattern: "FormatType\\.PARQUET"
    - from: "dlio_benchmark/data_generator/generator_factory.py"
      to: "dlio_benchmark/data_generator/parquet_generator.py"
      via: "FormatType.PARQUET case"
      pattern: "FormatType\\.PARQUET"
---

<objective>
Add Apache Parquet format support to DLIO benchmark by creating a fork with ParquetReader and ParquetGenerator implementations.

Purpose: Enable training benchmarks to use parquet format for data, particularly useful for recommendation models like DLRM that commonly use Parquet format for training data.

Output: A DLIO fork with full parquet support (reader + generator + factory registration) and updated pyproject.toml pointing to the fork.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-dlio-parquet-support/09-RESEARCH.md

Reference files (in installed DLIO):
- .venv/lib/python3.12/site-packages/dlio_benchmark/common/enumerations.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/reader/csv_reader.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/reader/reader_factory.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/data_generator/csv_generator.py
- .venv/lib/python3.12/site-packages/dlio_benchmark/data_generator/generator_factory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fork DLIO and add PARQUET to FormatType enum</name>
  <files>
    - Clone from https://github.com/argonne-lcf/dlio_benchmark.git (mlperf_storage_v2.0 branch)
    - dlio_benchmark/common/enumerations.py
  </files>
  <action>
    1. Create a fork of DLIO benchmark for mlperf-storage:
       - Clone argonne-lcf/dlio_benchmark (branch: mlperf_storage_v2.0)
       - Create new branch: parquet-support

    2. Modify dlio_benchmark/common/enumerations.py:
       - Add PARQUET = 'parquet' to FormatType enum (after PNG)
       - Add get_enum case for PARQUET in FormatType.get_enum() static method

    3. Modify dlio_benchmark/common/enumerations.py Compression enum:
       - Add SNAPPY = 'snappy' to Compression enum (if not present)
       - This is the default compression for parquet

    Note: The fork should be created at https://github.com/mlcommons/dlio_benchmark or a similar location where mlperf-storage can reference it. For now, create locally and push to a feature branch that pyproject.toml can reference.
  </action>
  <verify>
    - Python can import the modified enumerations: `from dlio_benchmark.common.enumerations import FormatType; print(FormatType.PARQUET)`
    - FormatType.get_enum('parquet') returns FormatType.PARQUET
  </verify>
  <done>
    - PARQUET is a valid FormatType enum value
    - FormatType.get_enum('parquet') works correctly
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ParquetReader and ParquetGenerator classes</name>
  <files>
    - dlio_benchmark/reader/parquet_reader.py (new)
    - dlio_benchmark/data_generator/parquet_generator.py (new)
  </files>
  <action>
    1. Create dlio_benchmark/reader/parquet_reader.py following csv_reader.py pattern:
       ```python
       import pyarrow.parquet as pq
       import numpy as np
       from dlio_benchmark.common.constants import MODULE_DATA_READER
       from dlio_benchmark.utils.utility import Profile
       from dlio_benchmark.reader.reader_handler import FormatReader

       dlp = Profile(MODULE_DATA_READER)

       class ParquetReader(FormatReader):
           """Parquet Reader implementation for DLIO."""

           @dlp.log_init
           def __init__(self, dataset_type, thread_index, epoch):
               super().__init__(dataset_type, thread_index)

           @dlp.log
           def open(self, filename):
               super().open(filename)
               table = pq.read_table(filename)
               # Convert Arrow table to numpy array (like CSV reader does)
               return table.to_pandas().to_numpy()

           @dlp.log
           def close(self, filename):
               super().close(filename)

           @dlp.log
           def get_sample(self, filename, sample_index):
               super().get_sample(filename, sample_index)
               image = self.open_file_map[filename][sample_index]
               dlp.update(image_size=image.nbytes)

           def next(self):
               for batch in super().next():
                   yield batch

           @dlp.log
           def read_index(self, image_idx, step):
               return super().read_index(image_idx, step)

           @dlp.log
           def finalize(self):
               return super().finalize()

           def is_index_based(self):
               return True

           def is_iterator_based(self):
               return True
       ```

    2. Create dlio_benchmark/data_generator/parquet_generator.py following csv_generator.py pattern:
       ```python
       import numpy as np
       import pyarrow as pa
       import pyarrow.parquet as pq
       from dlio_benchmark.common.enumerations import Compression
       from dlio_benchmark.data_generator.data_generator import DataGenerator
       from dlio_benchmark.utils.utility import progress

       class ParquetGenerator(DataGenerator):
           """Parquet data generator for DLIO."""

           def __init__(self):
               super().__init__()

           def generate(self):
               """Generate synthetic parquet files for training."""
               super().generate()
               np.random.seed(10)
               dim = self.get_dimension(self.total_files_to_generate)

               for i in range(self.my_rank, int(self.total_files_to_generate), self.comm_size):
                   progress(i + 1, self.total_files_to_generate, "Generating Parquet Data")
                   dim1 = dim[2 * i]
                   dim2 = dim[2 * i + 1]

                   # Generate random data as numpy array
                   records = [np.random.randint(255, size=dim1 * dim2, dtype=np.uint8)
                             for _ in range(self.num_samples)]

                   # Convert to Arrow table
                   table = pa.table({'data': [rec.tolist() for rec in records]})

                   out_path_spec = self.storage.get_uri(self._file_list[i])

                   # Map DLIO compression to parquet compression
                   compression = 'snappy'  # default
                   if self.compression == Compression.GZIP:
                       compression = 'gzip'
                   elif self.compression == Compression.NONE:
                       compression = None

                   pq.write_table(table, out_path_spec, compression=compression)

               np.random.seed()
       ```

    IMPORTANT: PyArrow file handles are NOT fork-safe. The reader opens files lazily in open() which is called after worker spawning, so this is safe. Do NOT open files in __init__().
  </action>
  <verify>
    - parquet_reader.py exists and imports successfully
    - parquet_generator.py exists and imports successfully
    - Both classes follow the same structure as csv_reader.py and csv_generator.py
  </verify>
  <done>
    - ParquetReader class extends FormatReader with open/close/get_sample methods
    - ParquetGenerator class extends DataGenerator with generate method
    - Both use PyArrow for parquet I/O operations
  </done>
</task>

<task type="auto">
  <name>Task 3: Register parquet format in factories and update pyproject.toml</name>
  <files>
    - dlio_benchmark/reader/reader_factory.py
    - dlio_benchmark/data_generator/generator_factory.py
    - pyproject.toml
  </files>
  <action>
    1. Modify dlio_benchmark/reader/reader_factory.py:
       - Add case for FormatType.PARQUET after the CSV case:
         ```python
         elif type == FormatType.PARQUET:
             if _args.odirect == True:
                 raise Exception("O_DIRECT for %s format is not yet supported." % type)
             else:
                 from dlio_benchmark.reader.parquet_reader import ParquetReader
                 return ParquetReader(dataset_type, thread_index, epoch_number)
         ```

    2. Modify dlio_benchmark/data_generator/generator_factory.py:
       - Add case for FormatType.PARQUET after CSV case:
         ```python
         elif type == FormatType.PARQUET:
             from dlio_benchmark.data_generator.parquet_generator import ParquetGenerator
             return ParquetGenerator()
         ```

    3. Commit all DLIO changes and push to fork:
       ```bash
       git add -A
       git commit -m "feat: add parquet format support

       - Add PARQUET to FormatType enum
       - Create ParquetReader using PyArrow
       - Create ParquetGenerator using PyArrow
       - Register parquet in reader and generator factories
       - Default compression: snappy"
       git push origin parquet-support
       ```

    4. Update pyproject.toml in mlperf-storage:
       - Change DLIO dependency from:
         `"dlio-benchmark @ git+https://github.com/argonne-lcf/dlio_benchmark.git@mlperf_storage_v2.0"`
       - To (temporary fork reference - update URL when upstream PR merged):
         `"dlio-benchmark @ git+https://github.com/mlcommons/dlio_benchmark.git@parquet-support"`

       Note: The exact fork URL depends on where the fork is hosted. Coordinate with project maintainers on the appropriate location.

    5. Reinstall DLIO with parquet support:
       ```bash
       pip install -e ".[full]"
       ```
  </action>
  <verify>
    1. DLIO can be imported with parquet support:
       ```python
       from dlio_benchmark.common.enumerations import FormatType
       from dlio_benchmark.reader.reader_factory import ReaderFactory
       from dlio_benchmark.data_generator.generator_factory import GeneratorFactory
       print(FormatType.PARQUET)  # Should print: FormatType.PARQUET
       ```

    2. Unit tests pass: `pytest tests/unit -v -k "not reporting"`

    3. pyproject.toml references the parquet-support branch
  </verify>
  <done>
    - ReaderFactory returns ParquetReader for FormatType.PARQUET
    - GeneratorFactory returns ParquetGenerator for FormatType.PARQUET
    - pyproject.toml updated to reference DLIO fork with parquet support
    - DLIO reinstalled with parquet support
  </done>
</task>

</tasks>

<verification>
Overall phase checks:
1. FormatType.PARQUET exists and is recognized
2. ParquetReader can be instantiated by ReaderFactory
3. ParquetGenerator can be instantiated by GeneratorFactory
4. pyproject.toml points to DLIO fork with parquet-support branch
5. All existing unit tests pass
</verification>

<success_criteria>
1. DLIO benchmark has parquet format support (reader + generator)
2. format: parquet is a valid option in DLIO configuration
3. mlpstorage installs DLIO with parquet support via pyproject.toml
4. Existing functionality remains intact (no regression)
</success_criteria>

<output>
After completion, create `.planning/phases/09-dlio-parquet-support/09-01-SUMMARY.md`
</output>
