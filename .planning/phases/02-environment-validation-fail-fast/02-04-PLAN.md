---
phase: 02-environment-validation-fail-fast
plan: 04
type: execute
wave: 3
depends_on: ["02-02", "02-03"]
files_modified:
  - mlpstorage/validation_helpers.py
  - tests/unit/test_validation_helpers.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "All validation runs before benchmark execution starts"
    - "Multiple errors are collected and reported together"
    - "Error output includes actionable suggestions for each issue"
    - "Validation covers: MPI, DLIO, SSH, paths, config"
  artifacts:
    - path: "mlpstorage/validation_helpers.py"
      provides: "Comprehensive pre-run validation"
      exports: ["validate_benchmark_environment"]
  key_links:
    - from: "mlpstorage/validation_helpers.py"
      to: "mlpstorage/dependency_check.py"
      via: "import for dependency checks"
      pattern: "from mlpstorage.dependency_check import"
    - from: "mlpstorage/validation_helpers.py"
      to: "mlpstorage/environment"
      via: "import for SSH validation"
      pattern: "from mlpstorage.environment import"
---

<objective>
Create comprehensive fail-fast environment validation that runs before benchmark execution.

Purpose: Users see ALL environment issues at once with actionable suggestions, rather than fixing one issue, re-running, and finding another.

Output: `validate_benchmark_environment()` function that validates everything upfront.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-environment-validation-fail-fast/02-RESEARCH.md
@mlpstorage/validation_helpers.py
@mlpstorage/dependency_check.py
@mlpstorage/errors.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create comprehensive environment validator</name>
  <files>mlpstorage/validation_helpers.py</files>
  <action>
Add `validate_benchmark_environment()` function to validation_helpers.py:

1. Add imports at top:
   ```python
   from mlpstorage.dependency_check import check_mpi_with_hints, check_dlio_with_hints, check_ssh_available
   from mlpstorage.environment import detect_os, validate_ssh_connectivity, ValidationIssue
   ```

2. Create `validate_benchmark_environment(args, logger=None, skip_remote_checks: bool = False) -> None`:
   - Initialize empty list for collecting issues
   - Detect OS once at start for all checks

   - Check MPI if needed (distributed run with multiple hosts):
     - Call check_mpi_with_hints wrapped in try/except
     - On DependencyError, append to issues list (don't raise immediately)

   - Check DLIO if training/checkpointing benchmark:
     - Call check_dlio_with_hints wrapped in try/except
     - On DependencyError, append to issues list

   - Check SSH connectivity if distributed and not skip_remote_checks:
     - Call validate_ssh_connectivity for all non-localhost hosts
     - For each failure, create ValidationIssue and append to list
     - Include OS-specific ssh install hint if ssh binary not found

   - Include existing path validation (call _validate_paths)
   - Include existing required params validation (call _validate_required_params)

   - After ALL checks complete:
     - If issues list is not empty:
       - Log all issues with logger (if provided)
       - Format summary message using format_error('ENVIRONMENT_VALIDATION_SUMMARY', ...)
       - Raise the first error (DependencyError, MPIError, or ConfigurationError as appropriate)
     - If no issues, log success message if logger provided

3. Add helper `_requires_mpi(args) -> bool`:
   - Return True if args.hosts exists and has more than one host

4. Add helper `_is_distributed_run(args) -> bool`:
   - Return True if args.hosts has any non-localhost hosts

The key pattern is: collect ALL issues first, then report all together, then raise.
  </action>
  <verify>
```python
from mlpstorage.validation_helpers import validate_benchmark_environment
from argparse import Namespace
# Test with minimal args (should pass if no distributed run)
args = Namespace(program='training', command='run', model='unet3d', data_dir='/tmp', results_dir='/tmp')
# This should not raise if deps are available
try:
    validate_benchmark_environment(args)
    print("Validation passed")
except Exception as e:
    print(f"Validation failed: {e}")
```
  </verify>
  <done>
validate_benchmark_environment exists and collects all validation issues before reporting. When multiple issues exist, all are logged before raising the first error.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests for comprehensive validator</name>
  <files>tests/unit/test_validation_helpers.py</files>
  <action>
Create tests/unit/test_validation_helpers.py (or extend if exists):

1. TestValidateBenchmarkEnvironment class:

   - test_passes_when_all_deps_available:
     - Mock shutil.which to return paths for all deps
     - Create args with program='training', model='unet3d', data_dir exists
     - Should not raise

   - test_collects_multiple_errors:
     - Mock shutil.which to return None for mpirun and dlio_benchmark
     - Create args requiring both
     - Should raise, and logger should have logged both errors

   - test_checks_mpi_for_distributed_runs:
     - Create args with hosts=['host1', 'host2']
     - Mock shutil.which to return None for mpirun
     - Should raise DependencyError about MPI

   - test_skips_mpi_for_single_host:
     - Create args with hosts=['localhost']
     - Mock shutil.which to return None for mpirun
     - Should NOT check for MPI (single host doesn't need it)

   - test_checks_dlio_for_training:
     - Create args with program='training'
     - Mock shutil.which to return None for dlio_benchmark
     - Should raise DependencyError about DLIO

   - test_checks_dlio_for_checkpointing:
     - Create args with program='checkpointing'
     - Mock shutil.which to return None for dlio_benchmark
     - Should raise DependencyError about DLIO

   - test_skips_dlio_for_kvcache:
     - Create args with program='kvcache'
     - Mock shutil.which to return None for dlio_benchmark
     - Should NOT raise about DLIO

   - test_checks_ssh_for_remote_hosts:
     - Create args with hosts=['remote-host']
     - Mock validate_ssh_connectivity to return failure
     - Should raise MPIError about host unreachable

   - test_skip_remote_checks_flag:
     - Create args with hosts=['remote-host']
     - Call with skip_remote_checks=True
     - Should NOT call validate_ssh_connectivity

Use Namespace from argparse to create mock args objects.
  </action>
  <verify>pytest tests/unit/test_validation_helpers.py -v</verify>
  <done>
All validation_helpers tests pass. Tests verify that multiple errors are collected, appropriate checks run for different benchmark types, and skip flags work correctly.
  </done>
</task>

</tasks>

<verification>
```bash
# Run validation helper tests
pytest tests/unit/test_validation_helpers.py -v

# Run all related tests
pytest tests/unit/test_validation_helpers.py tests/unit/test_dependency_check.py tests/unit/test_environment.py -v

# Quick integration test
python -c "
from mlpstorage.validation_helpers import validate_benchmark_environment
from argparse import Namespace
args = Namespace(program='vectordb', command='run', results_dir='/tmp')
try:
    validate_benchmark_environment(args)
    print('Validation passed for vectordb')
except Exception as e:
    print(f'Validation issue: {type(e).__name__}: {str(e)[:100]}')
"
```
</verification>

<success_criteria>
1. validate_benchmark_environment function exists in validation_helpers.py
2. Function collects ALL issues before raising (doesn't fail on first issue)
3. Function checks MPI only for distributed runs
4. Function checks DLIO only for training/checkpointing
5. Function checks SSH connectivity for remote hosts
6. All tests pass
7. Error messages include actionable suggestions
</success_criteria>

<output>
After completion, create `.planning/phases/02-environment-validation-fail-fast/02-04-SUMMARY.md`
</output>
