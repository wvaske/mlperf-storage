---
phase: 02-environment-validation-fail-fast
plan: 05
type: execute
wave: 4
depends_on: ["02-04"]
files_modified:
  - mlpstorage/main.py
  - mlpstorage/benchmarks/base.py
  - tests/unit/test_benchmarks_base.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Validation runs before benchmark class is instantiated"
    - "Validation runs before cluster info collection"
    - "All benchmark types call validation"
    - "Validation errors are caught and displayed cleanly in main()"
  artifacts:
    - path: "mlpstorage/main.py"
      provides: "Integration point for fail-fast validation"
      contains: "validate_benchmark_environment"
  key_links:
    - from: "mlpstorage/main.py"
      to: "mlpstorage/validation_helpers.py"
      via: "import and call"
      pattern: "validate_benchmark_environment"
---

<objective>
Integrate fail-fast validation into the benchmark execution path.

Purpose: Ensure ALL environment validation happens before any benchmark work begins - before cluster info collection, before data loading, before any execution.

Output: Validation integrated into main.py and benchmark base class.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@mlpstorage/main.py
@mlpstorage/benchmarks/base.py
@mlpstorage/validation_helpers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate validation into main.py</name>
  <files>mlpstorage/main.py</files>
  <action>
Add fail-fast validation to the benchmark execution path in main.py:

1. Add import at top:
   ```python
   from mlpstorage.validation_helpers import validate_benchmark_environment
   ```

2. In `run_benchmark()` function, add validation BEFORE benchmark class instantiation:
   - After lockfile validation (if requested)
   - BEFORE `benchmark_class = program_switch_dict.get(args.program)`
   - Call: `validate_benchmark_environment(args, logger=logger)`
   - This ensures validation runs before any benchmark work

3. The existing exception handling in `main()` already handles DependencyError, MPIError, ConfigurationError - verify these are caught and displayed properly.

4. Add a skip flag check: if `--skip-validation` is passed (add to universal args if not present), skip the validation call. This is useful for debugging.

5. **IMPORTANT: Relationship between validate_pre_run and validate_benchmark_environment:**

   The codebase has an existing `validate_pre_run()` function in validation_helpers.py.
   Plan 02-04 creates a new `validate_benchmark_environment()` function.

   **Design decision: validate_benchmark_environment REPLACES validate_pre_run as the main entry point.**

   In main.py:
   - ONLY call `validate_benchmark_environment()` - do NOT also call `validate_pre_run()`
   - `validate_benchmark_environment()` internally calls the same helper functions (`_validate_paths`, `_validate_required_params`, `_validate_hosts`, `_validate_dependencies`) plus the NEW OS-aware checks
   - Remove any existing calls to `validate_pre_run()` in run_benchmark() if present
   - `validate_pre_run()` remains in validation_helpers.py for backward compatibility but is effectively deprecated
   - Add a deprecation comment to `validate_pre_run()`: "Deprecated: Use validate_benchmark_environment() instead"

Key point: validation must happen EARLY, before:
- Benchmark class instantiation
- Cluster info collection
- Result directory creation
- Any other benchmark setup
  </action>
  <verify>
```bash
# Test that validation is called (will fail if MPI not installed on non-distributed run)
python -c "
from mlpstorage.main import run_benchmark
from argparse import Namespace
# Create minimal args - should trigger validation
args = Namespace(
    program='training',
    command='run',
    model='unet3d',
    data_dir='/tmp',
    results_dir='/tmp/results',
    what_if=True,
    debug=False,
    verbose=False,
    stream_log_level='INFO',
    verify_lockfile=None,
    loops=1,
    hosts=None
)
print('Testing validation integration...')
"
```
  </verify>
  <done>
validate_benchmark_environment is called in run_benchmark before benchmark instantiation. This is the ONLY validation entry point (replaces validate_pre_run). validate_pre_run is marked as deprecated. Validation errors are caught by existing exception handlers in main().
  </done>
</task>

<task type="auto">
  <name>Task 2: Add validation to benchmark base class for additional checks</name>
  <files>mlpstorage/benchmarks/base.py</files>
  <action>
While main.py handles early validation, add a validation hook to the base class for benchmark-specific validation:

1. Add import if needed:
   ```python
   from mlpstorage.validation_helpers import validate_benchmark_environment
   ```

2. Add `_validate_environment()` method to Benchmark class:
   ```python
   def _validate_environment(self) -> None:
       """Validate environment before benchmark execution.

       Called early in run() to catch configuration issues before
       any work is done. Subclasses can override to add benchmark-
       specific validation.

       Note: Primary environment validation is done in main.py via
       validate_benchmark_environment() BEFORE benchmark instantiation.
       This hook is for benchmark-specific validation that requires
       the benchmark instance to exist.

       Raises:
           DependencyError: If required dependencies are missing.
           ConfigurationError: If configuration is invalid.
       """
       # Environment validation is primarily done in main.py before
       # benchmark instantiation. This hook allows subclasses to add
       # benchmark-specific validation if needed.
       pass
   ```

3. In the `run()` method, call `self._validate_environment()` at the very start, before `self._run()`:
   ```python
   def run(self) -> int:
       """Execute the benchmark and track runtime."""
       self._validate_environment()  # Add this line
       start_time = time.time()
       result = self._run()
       self.runtime = time.time() - start_time
       return result
   ```

This provides a hook for subclasses to add benchmark-specific validation while keeping the main validation in main.py.
  </action>
  <verify>
```python
from mlpstorage.benchmarks.base import Benchmark
# Verify _validate_environment method exists
assert hasattr(Benchmark, '_validate_environment')
print("_validate_environment method exists on Benchmark base class")
```
  </verify>
  <done>
Benchmark base class has _validate_environment hook that is called at start of run(). Subclasses can override for benchmark-specific validation. Docstring clarifies that main validation is in main.py.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add/update tests for integration</name>
  <files>tests/unit/test_benchmarks_base.py</files>
  <action>
Add tests to verify the validation integration:

1. TestBenchmarkValidation class (add to existing test file):

   - test_validate_environment_called_on_run:
     - Create a mock benchmark subclass
     - Mock _validate_environment to track if it was called
     - Call run()
     - Verify _validate_environment was called before _run

   - test_validate_environment_can_be_overridden:
     - Create subclass that overrides _validate_environment
     - Verify custom validation is called

   - test_validation_error_prevents_run:
     - Create subclass where _validate_environment raises DependencyError
     - Call run()
     - Verify _run was NOT called
     - Verify DependencyError propagates

Use existing test patterns from test_benchmarks_base.py. Use mock fixtures from tests/fixtures/.
  </action>
  <verify>pytest tests/unit/test_benchmarks_base.py -v -k "validation or Validation"</verify>
  <done>
Tests verify that _validate_environment is called before _run, can be overridden by subclasses, and validation errors prevent benchmark execution.
  </done>
</task>

</tasks>

<verification>
```bash
# Run all related tests
pytest tests/unit/test_benchmarks_base.py tests/unit/test_validation_helpers.py -v

# Integration test - run a command that should trigger validation
# This should either pass validation or show clear error message
python -m mlpstorage training configview --model unet3d --accelerator h100 --help 2>&1 | head -20

# Test that validation errors are formatted nicely
python -c "
from mlpstorage.errors import DependencyError
from mlpstorage.main import main
import sys
# Capture what happens with a bad config
print('Validation integration check complete')
"

# Verify validate_pre_run is marked deprecated
grep -n "Deprecated" mlpstorage/validation_helpers.py | head -5
```
</verification>

<success_criteria>
1. validate_benchmark_environment is called in main.py before benchmark instantiation
2. validate_benchmark_environment is the ONLY entry point (not also calling validate_pre_run)
3. validate_pre_run marked as deprecated in validation_helpers.py
4. Benchmark base class has _validate_environment hook
5. _validate_environment is called at start of run()
6. Validation errors are caught and displayed with suggestions
7. All tests pass
8. Validation happens before cluster info collection
</success_criteria>

<output>
After completion, create `.planning/phases/02-environment-validation-fail-fast/02-05-SUMMARY.md`
</output>
